<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic
  PUBLIC "-//OASIS//DTD DITA Topic//EN" "http://docs.oasis-open.org/dita/v1.1/OS/dtd/topic.dtd" ><topic xml:lang="en-us" id="topic4104">
<title>HPE Helion OpenStack(R) Carrier Grade (Beta): Running Traffic Through the Test Paths</title>
<prolog>
<metadata>
<othermeta name="layout" content="default"/>
<othermeta name="product-version" content="HPE Helion OpenStack(R)"/>
<othermeta name="product-version" content="HPE Helion OpenStack(R) 1.1"/>
<othermeta name="role" content="All"/>
<othermeta name="role" content="Paul F"/>
<othermeta name="product-version1" content="HPE Helion OpenStack(R)"/>
<othermeta name="product-version2" content="HPE Helion OpenStack(R) 1.1"/>
</metadata>
</prolog>
<body>
<p>
<!--UNDER REVISION-->
 <!--./CarrierGrade/DeploymentScenarios/carrier-grade-deployment-running-traffic.md-->
 <!--permalink: /helion/openstack/carrier/deploy/running-traffic/--></p>
<p>You can exercise the HPE Helion OpenStack(R) Carrier Grade components by running test traffic through the test path for the <xref href="../../CarrierGrade/DeploymentScenarios/carrier-grade-deployment-bridging-overview.dita" >Bridging Scenario Overview</xref> or <xref href="../../CarrierGrade/DeploymentScenarios/carrier-grade-deployment-routing-overview.dita" >Routing Scenario Overview</xref>.</p>
<!-- ===================== horizontal rule ===================== -->
<p>
<b>Note:</b> This feature applies only to servers in the VNF Region.</p>
<!-- ===================== horizontal rule ===================== -->
<section id="bridging-scenario-connections"> <title>Bridging Scenario Connections</title>
<p>To connect test equipment for a Bridging Scenario, you require the following:</p>
<ul>
<li>
<p>Access to an edge Layer 2 switch connected to the physical network on which the provider networks <codeph>provider-net-a</codeph> and <codeph>provider-net-b</codeph> are implemented. The switch must be configured for connection to the tenant networks <codeph>tenant1-net</codeph> and <codeph>tenant2-net</codeph>, realized on the provider networks.</p>

<p>For switch connectivity details, consult the admin user.</p>
</li>
<li>
<p>The VLAN IDs for the tenant networks on the provider network. You can obtain this information from the admin user.</p>
</li>
</ul>
<p>Connect a traffic generator and analyzer ports to the appropriate ports on the Layer 2 switch, using the appropriate VLANs.</p>
<p>Test performance is influenced substantially by the following factors:</p>
<ul>
<li>
<p>the type of virtual switch deployed in each tenant (Linux-native bridging or DPDK-accelerated)</p>
</li>
<li>
<p>the type of virtual interface used to attach to the tenant networks (virtio or AVP)</p>
</li>
</ul>
<p>The possible combinations are ranked from highest to lowest performance as follows:</p>
<ol>
<li>
<p>DPDK-accelerated switch with AVP network interfaces (best performance)</p>
</li>
<li>
<p>Linux-native bridging with AVP network interfaces</p>
</li>
<li>
<p>Linux-native bridging with virtio network interfaces (poorest performance)</p>
</li>
</ol>
</section>
<section id="routing-scenario-connections"> <title>Routing Scenario Connections</title>
<p>To connect test equipment for a Routing Scenario, you require the following:</p>
<ul>
<li>
<p>A connection to the physical network on which the provider networks provider-net-a and provider-net-b are implemented. The connection can be direct or through a Layer 3 edge router.</p>
</li>
<li>
<p>The IP subnets assigned to the tenant networks <codeph>tenant1-net</codeph> and <codeph>tenant2-net</codeph>.</p>
</li>
</ul>
<p>Connect a traffic generator and analyzer ports to the physical network,configured with IP addresses on the appropriate subnets.</p>
<p>Test performance is influenced by the use of virtio network interfaces or AVP network interfaces, with AVP providing better performance.</p>
</section>
<section id="test-recommendations"> <title>Test Recommendations</title>
<p>For realistic performance figures, follow these recommendations:</p>
<ul>
<li>Use bidirectional test traffic. This gives the virtual switches in the test path, both Linux-native and DPDK-accelerated, as well as all other physical switches involved in the connection, the opportunity to populate the MAC address-to-port tables for both directions, minimizing the amount of flooded traffic.</li>
</ul>
<p>One-way traffic is not typical of a real-world deployment, in which
there is always at least a small amount of traffic in each direction. One-way traffic results in CPU-intensive activity as packets are sent to all destination ports instead of one port. In addition, HPE Helion OpenStack(R) Carrier Grade server implements flooded-packet throttling to protect VMs, limiting the rate to 100Mb/s per virtual switch CPU. The effect is to reduce the overall
switching throughput substantially.</p>
<ul>
<li>
<p>Use at least 20 different IP-5-tuple traffic flows. The AVS switch on the compute node host and the switches of the example guest images implement software-based multi-core switching that balances traffic across multiple cores based on IP-5-tuple flows. Fewer IP-5-tuple flows translate into sub-optimal flow balancing decisions, which in turn translate into poor usage of the available cores.</p>
</li>
<li>
<p>Consider the effect of packet size on performance. At the platform level, larger packets require less packet processing overhead. At the application level, larger packets may or may not require more processing overhead, depending on the nature of the application.</p>
</li>
</ul>
<p>
  <xref type="section" href="#topic4104/top"> Return to Top </xref>
</p>
<!-- ===================== horizontal rule ===================== -->
</section>
</body>
</topic>
