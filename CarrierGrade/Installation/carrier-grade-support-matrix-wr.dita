<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic
  PUBLIC "-//OASIS//DTD DITA Topic//EN" "http://docs.oasis-open.org/dita/v1.1/OS/dtd/topic.dtd" ><topic xml:lang="en-us" id="topic8886">
<title>HPE Helion OpenStack(R) Carrier Grade (Beta):Support Matrix for the Wind RiverÂ(R) Linux Region</title>
<titlealts>
<searchtitle>HPE Helion OpenStack(R) Carrier Grade: Support Matrix</searchtitle>
</titlealts>
<prolog>
<metadata>
<othermeta name="layout" content="default"/>
<othermeta name="product-version" content="HPE Helion OpenStack(R) 1.1"/>
<othermeta name="role" content="Storage Administrator"/>
<othermeta name="role" content="Storage Architect"/>
<othermeta name="role" content="Michael B,"/>
<othermeta name="product-version1" content="HPE Helion OpenStack(R) 1.1"/>
</metadata>
</prolog>
<body>
<p>
<!--UNDER REVISION-->
 <!--./CarrierGrade/Installation/carrier-grade-support-matrix-wr.md-->
 <!--permalink: /helion/openstack/carrier/support-matrix/wr/--></p>
<p> <xref href="../../CarrierGrade/Installation/carrier-grade-support-matrix.dita" >â–² HPE Helion OpenStack(R) Carrier Grade (Beta): Support Matrix</xref>  </p>
<!-- Taken from Titanium Server Software Installation Guide, 15.x -->
<p>To ensure the performance and stability of the systems running Wind River Linux in the HPE Helion OpenStack(R) Carrier Grade environment, it is very important to meet the requirements and conform to the following recommendations.</p>
<section id="hard"> <title>Hardware Requirements</title>
<p>For successful software installation and operation, the server hardware platform must meet specific requirements.</p>
<p>The servers running Wind River Linux consist of a set of hosts connected to internal and external networks:</p>
<ul>
<li>Two controller nodes are required.</li>
</ul>
<p>The server runs on all x86-64 processors that support Intel Virtualization Technology (VT) VT-x/VT-d extensions. However, the kernel has been optimized specifically for the Intel Xeon-core family of processors.</p>
</section>
<section id="system-bios-requirements"> <title>System / BIOS Requirements</title>
<p>The BIOS on each host must support PXE booting for initial installation of the server software.</p>
</section>
<section id="memory-requirements"> <title>Memory Requirements</title>
<p>The following are the minimum RAM resources suggested for the hosts.</p>
<table>
<tgroup cols="2">
<colspec colname="col1"/>
<colspec colname="col2"/>
<thead>
<row>
    <entry>Host Type</entry>
    <entry>RAM Size</entry>
  </row>
</thead>
<tbody>
<row>
    <entry>Controller</entry>
    <entry>32 GB</entry>
  </row>
<row>
    <entry>Compute</entry>
    <entry>32 GB</entry>
  </row>
</tbody>
</tgroup>
</table>
<p>The actual memory requirement for production environments depends on the expected load. In particular, for compute nodes, the memory requirement depends on the expected number of virtual machines and the types of application running on them.</p>
</section>
<section id="storage-requirements"> <title>Storage Requirements</title>
<p>The storage requirements for the Titanium Server depend on the system scale.</p>
<p>All hosts require one system disk where the Titanium Server system software is installed. System disks are automatically partitioned by the server installer program.</p>
<p>Controller nodes require an additional physical disk for volume storage. Storage nodes require an additional physical disk for OSD storage.</p>
<p>The following minimum hard drive capacities are suggested for the hosts.</p>
<table>
<tgroup cols="">
<thead>
<row>
    <entry>Host</entry>
    <entry>Type</entry>
    <entry>Drive Capacity</entry>
  </row>
</thead>
<tbody>
<row>
    <entry>Controller</entry>
    <entry>Primary disk</entry>
    <entry>500 GB. A two-disk RAID-1 is suggested.</entry>
  </row>
<row>
    <entry>Controller</entry>
    <entry>Secondary disk</entry>
    <entry>500 GB. Solid-state drive recommended.
A two-disk RAID is suggested. </entry>
  </row>
<row>
    <entry>Compute</entry>
    <entry>Primary disk</entry>
    <entry>120 GB

</entry>
  </row>
</tbody>
</tgroup>
</table>
<p>
<b>NOTE:</b> The storage configurations for controller-0 and controller-1 must be identical.</p>
<p>On the controller nodes, the disk space is used to accommodate a variety of content, including Titanium Server and guest images, the OpenStack(R) configuration database, Cinder volumes, and Ceilometer CSV files.</p>
<p>For controller nodes, the suggested storage hardware includes the following:</p>
<ul>
<li>solid state drives (SSD) to improve overall performance</li>
<li>hardware RAID array for transparent failover and fallback operations</li>
</ul>
<p>The server uses distributed replicated block device (DRDB) technology to automatically synchronize the hard drives across the two controller nodes.</p>
</section>
<section id="ethernet-interfaces"> <title>Ethernet Interfaces</title>
<p>All hosts in the server server connect to at least the internal management network using an Ethernet interface. The ports used for this connection must support network booting and must be configured to be used as the primary booting device for normal operations.</p>
<p>Typically this means that they must be on-board ports, since in most BIOS/UEFI implementations only on-board ports can be configured for network booting. You can use ports on a 10 GB NIC instead, if these ports fulfill these requirements.</p>
<p>The following table illustrates the number and type of Ethernet ports required in two different installation scenarios. It assumes that the ports used to connect to the internal management network are on-board 1 GB ports.</p>
<p>
<b>NOTE:</b> The following table assumes that each interface is connected to a single network. An Ethernet interface can be shared by more than one network.</p>
<table>
<tgroup cols="">
<thead>
<row>
    <entry>Personality</entry>
    <entry>Basic Scenario</entry>
    <entry>LAG Scenario</entry>
  </row>
</thead>
<tbody>
<row>
    <entry>
Controller Node</entry>
    <entry>* One 1G on-board interface (Internal management network)
<!--A BR tag was used here in the original source.-->* One 1G interface (OAM)
<!--A BR tag was used here in the original source.-->* One optional 1G or 10G interface (Infrastructure network)</entry>
    <entry>
* Two 1G on-board interfaces (Internal
management network)
<!--A BR tag was used here in the original source.-->* Two 1G interfaces (OAM)
<!--A BR tag was used here in the original source.-->* Two optional 1G or 10G interfaces (Infrastructure network)
NOTE: The controller-0 and controller-1 port configurations
must be identical.</entry>
    <entry>
</entry>
  </row>
<row>
    <entry>Compute Node</entry>
    <entry>* One 1G on-board interface (Internal management network)
<!--A BR tag was used here in the original source.-->* One 1G (Intel i350) or 10G (Intel 82599) interface per additional Provider Network</entry>
    <entry>
* Two 1G on-board interfaces (Internal management network)
<!--A BR tag was used here in the original source.-->* Two 1G (Intel i350) or 10G (Intel 82599) interfaces per additional Provider Network</entry>
  </row>
</tbody>
</tgroup>
</table>
<p>In the basic scenario, a single Ethernet port is used to attach the host to each of the networks. In the LAG scenario, two Ethernet ports are used for each connection.</p>
</section>
<section id="board-management-modules"> <title>Board Management Modules</title>
<p>For out-of-band reset and power-on/power-off capabilities, HP360 or HP380 servers equipped with HPE iLO (Integrated Lights Out) board management modules are required. Each module must be connected using port-based VLAN to a switch that has access to the internal management network.</p>
</section>
<section id="usb-interface"> <title>USB Interface</title>
<p>For the controller, a USB interface is required for backup and restore operations, and for software installation if a DVD is not available.</p>
</section>
<section id="net"> <title>Network Requirements</title>
<p>The networking environment of the Titanium Server incorporates up to five types of network:</p>
<ul>
<li>the internal management network</li>
<li>the OAM network</li>
<li>one or more provider networks</li>
<li>an optional infrastructure network</li>
<li>an optional board management network. </li>
</ul>
<p>Operational requirements for each network are described in the following sections.</p>
</section>
<section id="internal-management-network"> <title>Internal Management Network</title>
<p>The internal management network must be implemented as a single, dedicated, Layer 2 broadcast domain for the exclusive use of each Titanium Server cluster. Sharing of this network by more than one Titanium Server cluster is not a supported configuration.</p>
<p>During the Titanium Server software installation process, several network services such as BOOTP, DHCP, and PXE, are expected to run over the internal management network. These services are used to bring up the different hosts to an operational state. Therefore, it is mandatory that this network be operational and available in advance, to ensure a successful installation.</p>
<p>On each host, the internal management network can be implemented using a 1Gb or 10 Gb Ethernet port. In either
case, requirements for this port are:</p>
<ul>
<li>must be capable of PXE-booting</li>
<li>can be used by the motherboard as a primary boot device</li>
</ul>
</section>
<section id="infrastructure-network"> <title>Infrastructure Network</title>
<p>This is an optional network.</p>
<p>As with the internal management network, the infrastructure network must be implemented as a single, dedicated, Layer 2 broadcast domain for the exclusive use of each Titanium Server cluster.</p>
<p>Sharing of this network by more than one Titanium Server cluster is not a supported configuration.</p>
<p>The infrastructure network can be implemented as a 1Gb or 10 Gb Ethernet network. In its absence, all infrastructure traffic is carried over the internal management network.</p>
</section>
<section id="oam-network"> <title>OAM Network</title>
<p>You should ensure that the following services are available on the OAM Network:</p>
<ul>
<li>
<p>DNS Service - Needed to facilitate the name resolution of servers reachable on the OAM Network.</p>

<p>The Titanium Server can operate without a configured DNS service. However, a DNS service should be in place to ensure that links to external references in the current and future versions of the web administration interface work as expected.</p>
</li>
<li>
<p>NTP Service - The Network Time Protocol (NTP) can be optionally used by the Titanium Server controller nodes to synchronize their local clocks with a reliable external time reference. However, it is strongly suggested that this service be available, among other things, to ensure that system-wide log reports present a unified view of the day-to-day
operations.</p>
</li>
</ul>
<p>The Titanium Server compute nodes always use the controller nodes as the de-facto time server for the entire Titanium Server cluster.</p>
</section>
<section id="provider-network"> <title>Provider Network</title>
<p>There are no specific requirements for network services to be available on the provider network. However, you must ensure that all network services required by the guests running in the compute nodes are available. For configuration purposes, the compute nodes themselves are entirely served by the services provided by the controller nodes over the internal management network.</p>
</section>
<section id="board-management-network"> <title>Board Management Network</title>
<p>The board management network is implemented on the internal L2 switch using a dedicated VLAN. Refer to Reference Logical Architecture on page 12 for further information on the internal L2 switch, and to The Board Management Network on page 38 for network planning details.</p>
<p>
  <xref type="section" href="#topic8886/top"> Return to Top </xref>
</p>
<!-- ===================== horizontal rule ===================== -->
</section>
</body>
</topic>
