<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic
  PUBLIC "-//OASIS//DTD DITA Topic//EN" "http://docs.oasis-open.org/dita/v1.1/OS/dtd/topic.dtd" >
<topic xml:lang="en-us" id="topic10581">
  <title>HP Helion <tm tmtype="reg">OpenStack</tm> Carrier Grade 1.1: Installing the KVM
    Deployment</title>
  <prolog>
    <metadata>
      <othermeta name="layout" content="default"/>
      <othermeta name="product-version" content="HP Helion Openstack Carreir Grade 1.1"/>
      <othermeta name="role" content="Storage Administrator"/>
      <othermeta name="role" content="Storage Architect"/>
      <othermeta name="role" content="Michael B"/>
      <othermeta name="product-version1" content="HP Helion Openstack Carreir Grade 1.1"/>
    </metadata>
  </prolog>
  <body>
    <p>
      <!--UNDER REVISION-->
      <!--./CarrierGrade/Installation/carrier-grade-install-pb-hlm-vm.md-->
      <!--permalink: /helion/openstack/carrier/install/pb/hlm-vm/--></p>
    <p>The first phase of the HP Helion Openstack Carrier Grade installation involves creating a
      virtual machine for the Helion Lifecycle Management (HLM) and then deploying the HP Helion
      Openstack cloud.</p>

    <section id="prepare">
      <title>Prepare the system for deployment</title>
      <p>Make sure you have downloaded and extracted all of the required installation packages on
        the HLM host </p>
      <p>Use the following steps to prepare the server on which the HLM VM will be deployed (the HLM
        host):</p>
      <ol>

        <li>Edit the <codeph>/root/infra-ansible-playbooks/group_vars/all</codeph> file for your
          environment. For information on each variable, refer to the comments in the file with each
          variable. Make sure the <codeph>hlm_kvm_host</codeph> value is configured properly.</li>
        <li id="hosts">Check the hosts file <codeph>/root/infra-ansible-playbooks/hosts</codeph>
          file to enter the IP address of the vibr0 interface in the <codeph>hlm_kvm_host</codeph>
          field, as shown in the following example. </li>
      </ol>
      <codeblock>  [hlm_kvm_host]
  192.168.122.1 </codeblock>
    </section>
    <section id="deploy-the-hlm-and-vsd-vm">
      <title>Deploy the HLM Virtual Machine</title>
      <p>Use the following steps to deploy the HLM VM on the HLM Host using Ansible playbooks.</p>
      <ol>
        <li>Execute the following command:<codeblock>ansible-playbook -i hosts setup_hlm_onBM.yml</codeblock>
          <!--The command will do the following:<ul><li>Copy both installation files (tar balls) to the HLM host, decrypt, and extract the files.</li><li>Execute the <codeph>updatepackages</codeph> command. </li><li>Execute the <codeph>prepareenv</codeph> command.</li><li>Execute the <codeph>Init cobbler</codeph> command.</li><li>Execute the <codeph>Importiso</codeph> command.<p/></li></ul>--><ul>
            <li>You will see similar message when the playbook is run successful:
              hlmcompleteionsnaphot</li>
          </ul></li>
        <li>Locate the IP address for the HLM VM in the
            <codeph>/root/infra-ansible-playbooks/group_vars/all</codeph> file of the HLM host under
            <codeph>hlm_clmstaticip</codeph> field. You will use this IP address to SSH into the HLM
          VM during the installation process.</li>
      </ol>
    </section>
    <section id="configure-a-json-file-for-installation">
      <title>Configure a JSON file for installation</title>
      <p>The HP Helion OpenStack deployment requires a JSON file. Use the following steps to install
        and edit the file.</p>
      <ol>
        <li>Login to HLM VM.
            <codeblock>ssh &lt;HLM_VM_IP>
where: HLM_VM_IP is the CLM IP of the HLM VM. </codeblock><p>Use
            the default credentials:
            <codeblock>User Name: cghelion
    Password: cghelion</codeblock></p><p><b>Important:</b>
            After logging in with the default password, make sure you change the password for the
              <codeph>cghelion</codeph> user. </p></li>


        <li>Execute the following command to run a script that disables the <codeph>root</codeph>
          user. For security purposes, we strongly recommend disabling root.
            <codeblock>sudo /root/cg-hlm/dev-tools/disable-root.sh</codeblock><p>This command will
            disable the <codeph>root</codeph> user and performs the following tasks:</p><ul>
            <li>removes password for root from the /etc/shadow file</li>
            <li>removes any and all keys in /root/.ssh/authorized_keys file</li>
            <li>changes PermitRootLogin in /etc/ssh/sshd_config file from "yes" back to default of
              "without-password"</li>
            <li>restarts the ssh service so the above config change takes effect immediately</li>
          </ul><p>Once this command complete, the <codeph>cghelion</codeph> user should be used to
            access the HLM VM and any actions that require root access should be done using
              <codeph>sudo</codeph>.</p></li>
        <li>Execute the following command to switch to the root
          user:<codeblock>sudo su -</codeblock></li>

        <li>On the HLM VM, change to the home directory.<codeblock>cd ~</codeblock></li>
        <li>Provision and configure your HP Helion OpenStack
            VM.<codeblock>hnewcloud  &lt;cloudname&gt; denver</codeblock><p>Where:</p><ul>
            <li><codeph>&lt;cloudname&gt;</codeph> is the name of the cloud to create</li>
            <li><codeph>denver</codeph> is the name of the template to use. The installation kit
              includes the <codeph>denver</codeph> template.</li>
          </ul><p>The command creates the <codeph>&lt;cloudname&gt;</codeph> directory, which will
            contain a JSON template file <codeph>node-provision.json</codeph>. This template
            supplies input values to the <codeph>hprovision</codeph> script, later in the
            installation.</p></li>
        <li>Edit <codeph>node-provision.json</codeph> file to change only the following
            fields:<table id="table_ycw_qrn_xs">
            <tgroup cols="2">
              <colspec colname="col1" colsep="1" rowsep="1"/>
              <colspec colname="col2" colsep="1" rowsep="1"/>
              <thead>
                <row>
                  <entry colsep="1" rowsep="1">Field</entry>
                  <entry colsep="1" rowsep="1">Description</entry>
                </row>
              </thead>
              <tbody>
                <row>
                  <entry>Pxe-mac-address</entry>
                  <entry>MAC address of the interface you want to PXE boot onto. This is not same as
                    iLO MAC address.</entry>
                </row>
                <row>
                  <entry>pm_ip</entry>
                  <entry>Power management IP (iLO ip)</entry>
                </row>
                <row>
                  <entry>pm_user</entry>
                  <entry>Power management user (iLO username)</entry>
                </row>
                <row>
                  <entry>pm_pass</entry>
                  <entry>Power management password (iLO password)</entry>
                </row>
                <row>
                  <entry>failure_zone, vendor, model, os_partition_size, data_partition_size</entry>
                  <entry>Enter the same value as for these fields an in the
                      <codeph>nodes.json</codeph> file used during cloud deployment</entry>
                </row>
              </tbody>
            </tgroup>
          </table><p>To see a sample <codeph>node-provision.json</codeph> file, see<xref
              href="carrier-grade-install-pb-hlm-vm-json.dita#topic4797">Sample JSON File for the
              HLM Virtual Machine Installation</xref>.</p></li>
        <li>Modify the <codeph>environment.json</codeph> file to configure the VLANs and network
          addresses as appropriate for your environment. Set the following for the CLM, CAN, and BLS
          network:<codeblock>"cidr": 
"start-address": </codeblock> The three controller nodes
          should have CLM, CAN, EXT, BLS on eth0 and TUL on eth1. <!--Hiding for RC0 
              <p>The two compute nodes should have CLM, EXT, BLS on eth0 and TUL on eth1.</p>
           -->
          <p>
            <b>Example:</b>
          </p><pre>{</pre><pre>    "product": {</pre><pre>        "version": 1</pre><pre>    },</pre><pre> </pre><pre>    "node-type": [</pre><pre>        {</pre><pre>            "name": "CCN",</pre><pre>            "interface-map": [</pre><pre>                {</pre><pre>                    "name": "INTF0",</pre><pre>                    "ethernet-port-map": {</pre><pre>                        "interface-ports": [ "eth0" ]</pre><pre>                    },</pre><pre>                    "logical-network-map": [</pre><pre>                        {</pre><pre>                            "name": "CLM",</pre><pre>                            "type": "vlan",</pre><pre>                            "segment-id": "502",</pre><pre>                            "network-address": {</pre><pre>                                "cidr": "10.50.2.0/24",</pre><pre>                                "start-address": "10.50.2.10",</pre><pre>                                "gateway": "10.50.2.1"</pre><pre>                            }</pre><pre>                        },</pre><pre>                        {</pre><pre>                            "name": "CAN",</pre><pre>                            "type": "vlan",</pre><pre>                            "segment-id": "504",</pre><pre>                            "network-address": {</pre><pre>                                "cidr": "10.50.4.0/24",</pre><pre>                                "start-address": "10.50.4.10"</pre><pre>                            }</pre><pre>                        },</pre><pre>                        {</pre><pre>                            "name": "BLS",</pre><pre>                            "type": "vlan",</pre><pre>                            "segment-id": "76",</pre><pre>                            "network-address": {</pre><pre>                                "cidr": "172.16.76.0/19",</pre><pre>                                "start-address": "172.16.76.10"</pre><pre>                            }</pre><pre>                        }</pre><pre>                    ]</pre><pre>                }</pre><pre>            ]</pre><pre>        }</pre><pre>    ]</pre><pre>}</pre><p>
            <b>NOTE:</b> The Helion Configuration Processor assigns the first address of the CLM
            address range to itself for serving python and debian repositories. Make sure that you
            set the first IP address of the CLM range for the eth2 (CLM) address of the HLM
            node.</p></li>
        <li>Modify the <codeph>definition.json</codeph> file: <ol id="ol_t3x_btn_xs">
            <li>Set the number of compute systems to 2.
              <codeblock>"count": 2, //number of computes in the resource pool</codeblock></li>
            <li>Update the <codeph>ansible-vars</codeph> section with all the information based on
              your setup.</li>
            <li>Make sure you have two NTP entries in the <codeph>upstream_ntp_servers</codeph>
              fields in the <codeph>definition.json</codeph> file as seen in the following example.
              If you have only one NTP server in your environment, specify the same NTP server
              twice. Example:
              <codeblock>{
    "product": {
      "version": 1
      },
    
    "cloud": {
      "name": "b44tb4",
      "nickname": "b44tb4",
      "server-config": "nodes.json",
      "environment": "environment.json",
      "network-config": ".hos/lnet-control-data.json"
    },
    
    "failure-zones": [
      {
        "name": "fz1"
      },
      {
        "name": "fz2"
      },
      {
        "name": "fz3"
      }
    ],
    
    "control-planes": [
      {
        "file": ".hos/ccp-1x3-ss.json",
        "resource-nodes": []
      }
    ],
    
    "ansible-vars": {
      "dns_address": "10.1.2.44",
      "dns_domain_name": "helion.cg",
      "ldap_url": "10.1.2.44",
      "ldap_username": "admin",
      "ldap_password": "admin",
      "ldap_domain": "dc=helioncg,dc=local",
      "ldap_ou": "CGTestUsers",
      "ldap_nova_password": "nova",
      "ldap_nova_user": "nova",
      "ldap_neutron_password": "neutron",
      "ldap_neutron_user": "neutron",
      "ldap_cinder_password": "cinder",
      "ldap_cinder_user": "cinder",
      "ldap_glance_password": "glance",
      "ldap_glance_user": "glance",
      "ldap_enabled": 1,
      "upstream_ntp_servers": [
        "10.1.2.44",
        "16.110.135.123",
        "2.debian.pool.ntp.org"
      ],
      "ssl_cert_file": "ca.crt",
      "ssl_key_file": "cakey.pem",
      "ssl_passphrase": "cghelion"
     },
    
    "wr-vars": {
      "database_storage": 50,
      "backup_storage": 300,
      "image_storage": 250,
      "region_name": "RegionOne",
      "logical_interface": [
        {
          "lag_interface": "N",
          "interface_mtu": 1500,
          "interface_ports": [
              "eth0"
          ],
           "network": [
              {
                  "ip_start_address": "10.50.2.51",
                  "ip_end_address": "10.50.2.99",
                  "name": "CLM"
              },
              {
                  "ip_start_address": "172.16.76.150",
                  "ip_end_address": "172.16.76.199",
                  "name": "BLS"
               },
               {
                  "ip_start_address": "10.50.4.51",
                  "ip_end_address": "10.50.4.99",
                  "gateway": "10.50.4.1",
                  "name": "CAN"
                }
            ]
        }
    ],
    "pxeboot_cidr": "10.50.1.0/24",
    "license_file_name": "license.lic"
    }
 }
</codeblock></li>
          </ol></li>
      </ol>
    </section>
    <section id="pxe-boot">
      <title>Configure PXE boot</title>
      <p>After you edit the <codeph>node-provision.json</codeph> file, you must enable one-time PXE
        boot on the servers set the correct boot order. Execute the following on the HLM VM:</p>
      <ol>
        <li>Copy the <codeph>ilopxebootonce.py</codeph> from the
            <systemoutput>/root/cg-hlm/dev-tools/ilopxebootonce.py</systemoutput> to the directory
          where you have the <codeph>node-provision.json</codeph> file.</li>
        <li>Execute the following script:
          <codeblock>python ilopxebootonce.py node-provision.json</codeblock></li>
      </ol>
      <p>After the script is run, the <codeph>Current One-Time Boot Option</codeph> is set to
          <codeph>Network Device 1</codeph> on all the servers listed in
          <codeph>node-provision.json</codeph> file.</p>
    </section>
    <section id="create-a-new-cloud-template-and-bring-the-cloud-nodes-up">
      <title>Provision the cloud nodes</title>
      <ol>
        <li>Use the following script to start the provisioning of the HP Helion OpenStack cloud:
            <codeblock>hprovision &lt;cloudname&gt; </codeblock><p>Where:</p><ul>
            <li><codeph>&lt;cloudname&gt;</codeph> is the name of the cloud to create</li>
          </ul><p>This script will PXE boot the nodes specified in
              <codeph>node-provision.json</codeph> file. The script also tracks the PXE boot
            completion process and will create the <codeph>nodes.json</codeph> file in the
            directory. </p></li>
        <li>Make sure the nodes are booted up.<p>a. Change to the <codeph>&lt;cloudname&gt;</codeph>
            directory:</p><codeblock>cd ~/&lt;cloudname&gt;</codeblock><p>b. Once the baremetal
            nodes are provisioned, make sure the <codeph>nodes.json</codeph> file is generated and
            that you can establish a password-less SSH connection to these nodes from HLM
          VM.</p></li>

        <li>Based on your environment, you can use either HP StoreServ (3PAR) or HP StoreVirtual VSA
          block storage Use the following steps to integrate a fully-installed block storage into HP
          Helion OpenStack Carrier Grade: <ol>
            <li>Change to the <codeph>cinder/blocks</codeph> directory:
              <codeblock>cd ~/&lt;cloudname>/services/cinder/blocks</codeblock> Where &lt;cloudname>
              is the name you assigned to the cloud.</li>
            <li>Then:<p><b>For HP StoreServ (3PAR) storage </b>: Edit the
                  <codeph>cinder_conf</codeph> file to configure the 3PAR settings. For
                  example:</p><codeblock>[DEFAULT]
enabled_backends=hp3par,hplefthand

[hp3par]
volume_backend_name=&lt;mybackendname1>
hp3par_api_url=https://&lt;management_ip>:8080/api/v1
hp3par_username=&lt;username>
hp3par_password=&lt;password>
hp3par_cpg=&lt;cpg>
san_ip=&lt;san_ip>
san_login=&lt;username>
san_password=&lt;password>
hp3par_iscsi_ips=&lt;iscsi_target_ips seperated by ,>
volume_driver=cinder.volume.drivers.san.hp.hp_3par_iscsi.HP3PARISCSIDriver
hp3par_debug=False
hp3par_iscsi_chap_enabled=false
hp3par_snapshot_retention=48
hp3par_snapshot_expiration=72</codeblock><p><b>For
                  HP StoreVirtual VSA storage</b> Edit the <codeph>cinder_conf</codeph> file to
                configure the VSA settings. For example:
                <codeblock>[DEFAULT]
enabled_backends=hp3par,hplefthand
hplefthand_username = &lt;username>
hplefthand_password = &lt;password>
hplefthand_clustername = &lt;Cluster Name>
hplefthand_api_url = https://&lt;Iscsi Virtual IP address>/lhos </codeblock></p></li>
          </ol></li>
        <li>After editing the JSON files, validate each JSON file to make sure there are no
          syntactical errors using the tool of your preference. For example, using the Python
          json.tool: <codeblock>python -m json.tool &lt;filename>.json</codeblock></li>
        <li>Once you have correctly edited all the json Cloud Model files, run the HP Helion
          OpenStack Configuration Processor:
            <codeblock>hcfgproc -d definition.json</codeblock><p>The HP Helion OpenStack
            Configuration Processor is a script, called <codeph>hcfgproc</codeph>, that is
            incorporated into the installation environment. </p></li>
        <li>Review the CloudDiagram, <codeph>hosts.hf</codeph>, and
            <codeph>net/interfaces.d/eth.cfg</codeph> files to make sure the network settings are
          correct.</li>
        <li>Initialize network interfaces on all the cloud nodes using the following command:
            <codeblock><codeph>hnetinit &lt;cloudname&gt;</codeph></codeblock><p>You can run this
            command from any directory.</p><p>After this command completes, all cloud nodes and CLM
            network interfaces should be set correctly.</p></li>

        <li>Use the following command to deploy the cloud:
            <codeblock><codeph>hdeploy &lt;cloudname&gt;</codeph></codeblock><p>Once cloud
            deployment is successfully complete, there will be 3 controller nodes in the non-KVM
            region.</p></li>
      </ol>
    </section>
    <section>        
      <title>  Apply the Horizon interface patch:</title> 
      <ol><li>Locate the <codeph>hcg_1.1.0_horizon_patch1.tar</codeph> file you downloaded to the HLM host and
        extract the files.</li>
        <li>From the HLM VM, SSH into any controller node using the default credentials: <p>
          <codeblock>User Name: cghelion
            Password: cghelion</codeblock>
        </p></li>
        <li>Execute the following command to switch to the root
          user:<codeblock>sudo su -</codeblock></li> 
        <li>Copy the <codeph>user.py</codeph> file to first controller node in the
          <codeph>/opt/stack/venvs/horizon/lib/python2.7/site-packages/openstack_auth/</codeph>
          directory.</li>
        <li>Use the following command to restart the Horizon service on that node:
          <codeblock>sudo service apache2 restart</codeblock></li>
        <li>Enter <codeph>exit</codeph> to leave the root shell.</li>
        <li>Log into each of the other two controller nodes in the non-KVM region and repeat
          above steps to copy the <codeph>user.py</codeph> to each controller .</li> 
      </ol>
    </section>
    
    
    <section id="next-step">
      <title>Next Step</title>
      <p>
        <xref href="carrier-grade-install-kvm-cloud-GA.dita">Deploying the KVM Region</xref>
      </p>
    </section>
  </body>
</topic>
