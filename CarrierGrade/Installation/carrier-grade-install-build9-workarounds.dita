<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic
  PUBLIC "-//OASIS//DTD DITA Topic//EN" "http://docs.oasis-open.org/dita/v1.1/OS/dtd/topic.dtd" >
<topic xml:lang="en-us" id="topic10581">
  <title>HP Helion <tm tmtype="reg">OpenStack</tm> Carrier Grade 2.0 Alpha: Build 9 Workarounds</title>
  <prolog>
    <metadata>
      <othermeta name="layout" content="default"/>
      <othermeta name="product-version" content="HP Helion Openstack Carreir Grade 1.1"/>
      <othermeta name="role" content="Storage Administrator"/>
      <othermeta name="role" content="Storage Architect"/>
      <othermeta name="role" content="Michael B"/>
      <othermeta name="product-version1" content="HP Helion Openstack Carreir Grade 1.1"/>
    </metadata>
  </prolog>
  <body>
    <section>
      <title>Workarounds to be followed when hdeploy with Build#9 fails</title>
      <p>If the <codeph>hdeploy</codeph> script in build 9 fails, review the topics in this section
        for troubleshooting.</p>
    </section>
    <section>
      <title>Apache2 Failure During hdeploy</title>
    <p>If the <codeph>hdeploy</codeph> script fails to restart the Apache v2 web server, the problem
        can be with the configuration of the Attis role. </p>
    <p>You can fix this problem by manually editing the following files, removing a specific file on
      each controller, and then restarting the <codeph>hdeploy</codeph> script with a
        <codeph>-t</codeph> option.</p> 
    <ol>
        <li>Log into the HLM VM.</li>
        <li>Edit the <codeph>roles/ATTIS/templates/etc/apache2/sites-available/attis.conf</codeph>
          file to replace the following text: <ol>
            <li>Replace:
                <codeblock>Listen {{ ATTIS.consumes_KEY_API.vips.admin[0].host }}:9898 </codeblock><p>with
                the following:
              </p><codeblock>Listen {{ host.vars.my_network_name }}:9898</codeblock></li>
            <li>Replace:
                <codeblock>&lt;VirtualHost {{ ATTIS.consumes_KEY_API.vips.admin[0].host }}:9898&gt; </codeblock><p>with:
              </p><codeblock>&lt;VirtualHost {{ host.vars.my_network_name }}:9898&gt;</codeblock></li>
          </ol></li><li>In the <codeph>roles/ATTIS/tasks/start.yml</codeph> file, comment out each of the following
          lines:
          <codeblock># Start Attis service
    
#- name: Start Attis service
# service: name=attis-server state=started</codeblock></li>
          <li>Log onto each of the three (3) controllers in the non-KVM region and remove the
            <codeph>rm /etc/apache2/sites-available/attis.conf</codeph> file.</li>
    <li>Execute the <codeph>hdeploy</codeph> script on the HLM VM using the <codeph>-t</codeph>
          option: <codeblock>hdeploy –t &lt;cloudname&gt;</codeblock></li>
        </ol>
      <p>Return to <xref href="carrier-grade-install-kvm-esx-GA.dita#topic10581/hdeploy">the KVM +
          ESX installation</xref>.</p>
    </section>
    <section><title>VRG Failure During hdeploy</title>
    
    <p>If the DCN installation packages fail to install, do the following:</p>
      <ol>
        <li>Log onto the HLM VM.</li>
        <li>Comment out following lines from the <codeph>&lt;clouddir path to
          ansible&gt;\ansible\roles\DCN-VRG\tasks\install.yml</codeph>:
<codeblock>
#- name: Install packages
# apt: name={{ item }} force=yes
# with_items:
# - qemu-kvm
# - libvirt-bin
# - libjson-perl
# - python-twisted-core
# - vlan
# - module-init-tools
# - uuid-runtime
# - libprotobuf-c1
# - python-setproctitle
# - nuage-python-openvswitch
# - nuage-openvswitch-common
# - nuage-openvswitch-switch
</codeblock></li>
    <li>Execute the <codeph>hdeploy</codeph> script on the HLM VM using the <codeph>-t</codeph>
          option: <codeblock>hdeploy –t &lt;cloudname&gt;</codeblock></li></ol>
      <p>Return to <xref href="carrier-grade-install-kvm-esx-GA.dita#topic10581/hdeploy">the KVM + ESX installation</xref>.</p>
    </section>
    <section><title>nova-compute is down after cloud deploy</title> As a workaround, comment the
      line <codeph>log_config_append = /etc/nova/logging.conf</codeph> in
        <codeph>/etc/nova/nova.conf </codeph>on compute node. Restart <codeph>nova-compute</codeph>
      service on the compute-proxy node and all nova servcies on controllers. </section>
    <section><title>icinga2 status appears to be in down state on one of the controller node
      </title><p>Shut down the Icinga2 service on controllers M2 and M3 On Controller M1 do the following:. </p>
      <p>Edit the
          <codeph>/etc/icinga2/zones.d/&lt;cloudname>-CCP/services/&lt;cloudname>-CCP-T1-services.conf
        </codeph> file to remove following blocks:</p>
      <codeblock>apply Service "elk-sch-healthy" {
import "generic-service"
check_command = "elk-sch-healthy"
vars.apply_to_services = [ "ELK-SCH" ]
assign where ("BUILD9-CCP-T2" in host.groups || "BUILD9-CCP-T2-VIP" in host.groups) &amp;&amp; "ELK-SCH" in host.vars.services
}
apply Service "elk-kib-healthy" {
import "generic-service"
check_command = "elk-kib-healthy"
vars.apply_to_services = [ "ELK-KIB" ]
assign where ("BUILD9-CCP-T2" in host.groups || "BUILD9-CCP-T2-VIP" in host.groups) &amp;&amp; "ELK-KIB" in host.vars.services
}
apply Service "attis-healthy" {
import "generic-service"
check_command = "attis-healthy"
vars.apply_to_services = [ "ATTIS" ]
assign where ("BUILD9-CCP-T2" in host.groups || "BUILD9-CCP-T2-VIP" in host.groups) &amp;&amp; "ATTIS" in host.vars.services
}
Restart Icinga2 service ==> service icinga2 restart.
Access Icinga-Web
http://&lt;CLM-VIP>/icinga-web
  Credentials: root / password        </codeblock></section>
    <section><title>stackrc should have right variable names with unset command</title>
      <p>Remove all the values highlighted below and source stackrc again</p>
      <codeblock>export OS_PASSWORD=0nOIyeQTy
export OS_USERNAME=admin
export OS_TENANT_NAME=admin
export OS_AUTH_URL=https://10.200.191.120:5000/v2.0
export OS_CACERT=/etc/stunnel/ssl/hlm/certs/ca.crt
export OS_REGION_NAME=memphis
unset OS_PROJECT_DOMAIN_NAME=Default
unset OS_PROJECT_NAME=admin
unset OS_USER_DOMAIN_ID=default
unset OS_IDENTITY_API_VERSION=3
unset OS_AUTH_VERSION=3</codeblock>
    </section>
    <section><title>hdeploy failure during swift role configuration</title>
      
      <p>If you experience errors during swift role configurations, as shown in the following error messages, perform the following
        perform the following tasks to recover the RabbitMQ Cluster</p> 
      
      <codeblock>2015-09-24 11:13:37,315 p=6946 u=root |  TASK: [Ensure swift node ring path exists] ************************************
2015-09-24 11:13:37,483 p=6946 u=root |  changed: [B15-CCP-T2-M1-NETCLM]
2015-09-24 11:13:37,483 p=6946 u=root |  TASK: [Create rings] **********************************************************
2015-09-24 11:13:38,128 p=6946 u=root |  changed: [B15-CCP-T2-M1-NETCLM] => (item={'key': 'object.builder', 'value': {'min_part': 1, 'replica': 3, 'part_power': 10}})
2015-09-24 11:13:38,129 p=6946 u=root |  TASK: [Register disks] ********************************************************
2015-09-24 11:13:38,199 p=6946 u=root |  ok: [B15-CCP-T2-M1-NETCLM]
2015-09-24 11:13:38,204 p=6946 u=root |  ok: [B15-CCP-T2-M3-NETCLM]
2015-09-24 11:13:38,216 p=6946 u=root |  TASK: [Write disks to config file] ********************************************
2015-09-24 11:13:38,263 p=6946 u=root |  fatal: [B15-CCP-T2-M1-NETCLM] => {'msg': "AnsibleUndefinedVariable: One or more undefined variables: 'dict object' has no attribute 'swift_local_disks'", 'failed': True}
2015-09-24 11:13:38,263 p=6946 u=root |  fatal: [B15-CCP-T2-M1-NETCLM] => {'msg': "AnsibleUndefinedVariable: One or more undefined variables: 'dict object' has no attribute 'swift_local_disks'", 'failed': True}
2015-09-24 11:13:38,263 p=6946 u=root |  FATAL: all hosts have already failed -- aborting
2015-09-24 11:13:38,264 p=6946 u=root |  PLAY RECAP ********************************************************************
2015-09-24 11:13:38,264 p=6946 u=root |             to retry, use: --limit @/root/deploy.retry

2015-09-24 11:13:38,264 p=6946 u=root |  B15-CCP-T1-M1-NETCLM       : ok=77   changed=67   unreachable=0    failed=0
2015-09-24 11:13:38,264 p=6946 u=root |  B15-CCP-T2-M1-NETCLM       : ok=181  changed=160  unreachable=0    failed=1
2015-09-24 11:13:38,265 p=6946 u=root |  B15-CCP-T2-M2-NETCLM       : ok=67   changed=58   unreachable=0    failed=1
2015-09-24 11:13:38,265 p=6946 u=root |  B15-CCP-T2-M3-NETCLM       : ok=180  changed=161  unreachable=0    failed=1      </codeblock>
      
      <p>There may be a specific set of steps to follow for a particular RabbitMQ cluster issues,
        but the general procedure for resetting an entire RabbitMQ cluster is as follows:</p>
      <ol>
      <li>On each controller node, perform the following commands: <ol>
            <li>Use the following command to stop the RabbitMQ application, leaving the Erlang node
              running.: <codeblock>sudo rabbitmqctl stop_app</codeblock></li>
            <li>Use the following command to remove the node from any cluster it belongs to, remove
              all data from the management database, such as configured users and vhosts, and delete
              all persistent messages:  <codeblock>sudo rabbitmqctl reset</codeblock></li>
            <li>Use the following command to restart the RabbitMQ application::
              <codeblock>sudo rabbitmqctl start_app</codeblock></li>
          </ol></li>
      <li>On controller0, use the following command to unconditionally start the node:
          <codeblock>sudo rabbitmqctl force_boot</codeblock></li>
        <li><xref href="carrier-grade-install-kvm-esx-GA.dita#topic10581/hdeploy">Restart the
            hdeploy process</xref>.<p>
            <codeblock>hdeploy &lt;cloudname&gt;</codeblock>
          </p></li></ol>
    </section>
    </body>
</topic>
