<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic
  PUBLIC "-//OASIS//DTD DITA Topic//EN" "http://docs.oasis-open.org/dita/v1.1/OS/dtd/topic.dtd" >
<topic xml:lang="en-us" id="topic10581">
  <title>HP Helion <tm tmtype="reg">OpenStack</tm> Carrier Grade 2.0 Alpha: Build 9 Workarounds</title>
  <prolog>
    <metadata>
      <othermeta name="layout" content="default"/>
      <othermeta name="product-version" content="HP Helion Openstack Carreir Grade 1.1"/>
      <othermeta name="role" content="Storage Administrator"/>
      <othermeta name="role" content="Storage Architect"/>
      <othermeta name="role" content="Michael B"/>
      <othermeta name="product-version1" content="HP Helion Openstack Carreir Grade 1.1"/>
    </metadata>
  </prolog>
  <body>
    <section>
      <title>Workarounds to be followed when hdeploy with Build#9 fails</title>
      <p>If the <codeph>hdeploy</codeph> script in build 9 fails, review the topics in this section
        for troubleshooting.</p>
      <p>
        <ul id="ul_a5g_3k1_mt">
          <li><xref href="#topic10581/apache" format="dita"/></li>
          <li><xref href="#topic10581/vrg" format="dita"/></li>
          <li><xref href="#topic10581/nova" format="dita"/></li>
          <li><xref href="#topic10581/stackrc" format="dita"/></li>
          <li><xref href="#topic10581/rabbitmq" format="dita"/></li>
        </ul>
      </p>
    </section>
    <section id="apache">
      <title>Apache2 Failure During hdeploy</title>
    <p>If the <codeph>hdeploy</codeph> script fails to restart the Apache v2 web server, the problem
        can be with the configuration of the Attis role. </p>
    <p>You can fix this problem by manually editing the following files, removing a specific file on
      each controller, and then restarting the <codeph>hdeploy</codeph> script with a
        <codeph>-t</codeph> option.</p> 
    <ol>
        <li>Log into the HLM VM.</li>
        <li>Edit the <codeph>roles/ATTIS/templates/etc/apache2/sites-available/attis.conf</codeph>
          file to replace the following text: <ol>
            <li>Replace:
                <codeblock>Listen {{ ATTIS.consumes_KEY_API.vips.admin[0].host }}:9898 </codeblock><p>with
                the following:
              </p><codeblock>Listen {{ host.vars.my_network_name }}:9898</codeblock></li>
            <li>Replace:
                <codeblock>&lt;VirtualHost {{ ATTIS.consumes_KEY_API.vips.admin[0].host }}:9898&gt; </codeblock><p>with:
              </p><codeblock>&lt;VirtualHost {{ host.vars.my_network_name }}:9898&gt;</codeblock></li>
          </ol></li><li>In the <codeph>roles/ATTIS/tasks/start.yml</codeph> file, comment out each of the following
          lines:
          <codeblock># Start Attis service
    
#- name: Start Attis service
# service: name=attis-server state=started</codeblock></li>
          <li>Log onto each of the three (3) controllers in the non-KVM region and remove the
            <codeph>rm /etc/apache2/sites-available/attis.conf</codeph> file.</li>
    <li>Execute the <codeph>hdeploy</codeph> script on the HLM VM using the <codeph>-t</codeph>
          option: <codeblock>hdeploy –t &lt;cloudname&gt;</codeblock></li>
        </ol>
      <p>Return to <xref href="carrier-grade-install-kvm-esx-GA.dita#topic10581/hdeploy">the KVM +
          ESX installation</xref>.</p>
    </section>
    <section id="vrg"><title>VRG Failure During hdeploy</title>
    
    <p>If the DCN installation packages fail to install, do the following:</p>
      <ol>
        <li>Log onto the HLM VM.</li>
        <li>Comment out following lines from the <codeph>&lt;clouddir path to
          ansible&gt;\ansible\roles\DCN-VRG\tasks\install.yml</codeph>:
<codeblock>
#- name: Install packages
# apt: name={{ item }} force=yes
# with_items:
# - qemu-kvm
# - libvirt-bin
# - libjson-perl
# - python-twisted-core
# - vlan
# - module-init-tools
# - uuid-runtime
# - libprotobuf-c1
# - python-setproctitle
# - nuage-python-openvswitch
# - nuage-openvswitch-common
# - nuage-openvswitch-switch
</codeblock></li>
    <li>Execute the <codeph>hdeploy</codeph> script on the HLM VM using the <codeph>-t</codeph>
          option: <codeblock>hdeploy –t &lt;cloudname&gt;</codeblock></li></ol>
      <p>Return to <xref href="carrier-grade-install-kvm-esx-GA.dita#topic10581/hdeploy">the KVM + ESX installation</xref>.</p>
    </section>
    <section id="nova"><title>nova-compute is down after cloud deploy</title> As a workaround, comment the
      line <codeph>log_config_append = /etc/nova/logging.conf</codeph> in
        <codeph>/etc/nova/nova.conf </codeph>on compute node. Restart <codeph>nova-compute</codeph>
      service on the compute-proxy node and all nova servcies on controllers. </section>
    <section><title>icinga2 status appears to be in down state on one of the controller node
      </title><p>Shut down the Icinga2 service on controllers M2 and M3 On Controller M1 do the following:. </p>
      <p>Edit the
          <codeph>/etc/icinga2/zones.d/&lt;cloudname>-CCP/services/&lt;cloudname>-CCP-T1-services.conf
        </codeph> file to remove following blocks:</p>
      <codeblock>apply Service "elk-sch-healthy" {
import "generic-service"
check_command = "elk-sch-healthy"
vars.apply_to_services = [ "ELK-SCH" ]
assign where ("BUILD9-CCP-T2" in host.groups || "BUILD9-CCP-T2-VIP" in host.groups) &amp;&amp; "ELK-SCH" in host.vars.services
}
apply Service "elk-kib-healthy" {
import "generic-service"
check_command = "elk-kib-healthy"
vars.apply_to_services = [ "ELK-KIB" ]
assign where ("BUILD9-CCP-T2" in host.groups || "BUILD9-CCP-T2-VIP" in host.groups) &amp;&amp; "ELK-KIB" in host.vars.services
}
apply Service "attis-healthy" {
import "generic-service"
check_command = "attis-healthy"
vars.apply_to_services = [ "ATTIS" ]
assign where ("BUILD9-CCP-T2" in host.groups || "BUILD9-CCP-T2-VIP" in host.groups) &amp;&amp; "ATTIS" in host.vars.services
}
Restart Icinga2 service ==> service icinga2 restart.
Access Icinga-Web
http://&lt;CLM-VIP>/icinga-web
  Credentials: root / password        </codeblock></section>
    <section id="stackrc"><title>stackrc should have right variable names with unset command</title>
      <p>Remove all the values highlighted below and source stackrc again</p>
      <codeblock>export OS_PASSWORD=0nOIyeQTy
export OS_USERNAME=admin
export OS_TENANT_NAME=admin
export OS_AUTH_URL=https://10.200.191.120:5000/v2.0
export OS_CACERT=/etc/stunnel/ssl/hlm/certs/ca.crt
export OS_REGION_NAME=memphis
unset OS_PROJECT_DOMAIN_NAME=Default
unset OS_PROJECT_NAME=admin
unset OS_USER_DOMAIN_ID=default
unset OS_IDENTITY_API_VERSION=3
unset OS_AUTH_VERSION=3</codeblock>
    </section>
    <section id="rabbitmq"><title>hdeploy failure during swift role configuration</title>
      
      <p>If you experience errors during swift role configurations, as shown in the following error messages, perform the following
        perform the following tasks to recover the RabbitMQ Cluster</p> 
      
      <codeblock>2015-09-24 11:10:57,275 p=6946 u=root | TASK: [Enable RMQ management plugin] ******************************************
2015-09-24 11:10:58,563 p=6946 u=root | failed: [B15-CCP-T2-M2-NETCLM] =>
{"changed": true, "cmd": ["rabbitmq-plugins", "enable", "rabbitmq_management"], "delta": "0:00:01.050269", "end": "2015-09-24 11:10:21.939417", "rc": 1, "start": "2015-09-24 11:10:20.889148"}

2015-09-24 11:10:58,564 p=6946 u=root | stderr: ^M
Crash dump was written to: erl_crash.dump^M
Kernel pid terminated (application_controller) ({application_start_failure,kernel,{#shutdown,{failed_to_start_child,net_sup,{shutdown,{failed_to_start_child,net_kernel,{'EXIT',nodistribution#}}}},{k
2015-09-24 11:10:58,564 p=6946 u=root | stdout: {error_logger,
{#2015,9,24},{11,10,21#},"Protocol: ~tp: register/listen error: ~tp~n",["inet_tcp",econnrefused]}^M
{error_logger,{#2015,9,24}       
,
{11,10,21#},crash_report,[[{initial_call,{net_kernel,init,['Argument__1']}},{pid,&lt;0.21.0>},{registered_name,[]},{error_info,{exit,{error,badarg},
[{gen_server,init_it,6,[{file,"gen_server.erl"},{line,322}]},{proc_lib,init_p_do_apply,3,[{file,"proc_lib.erl"},
{line,237}]}]}},{ancestors,[net_sup,kernel_sup,&lt;0.10.0>]},{messages,[]},{links,[#Port&lt;0.93>,&lt;0.18.0>]},{dictionary,[{longnames,false}]},{trap_exit,true},{status,running},{heap_size,610},{stack_size,27},{reductions,823}],[]]}^M
{error_logger,{#2015,9,24},{11,10,21#}
          
,supervisor_report,[{supervisor,{local,net_sup}},
{errorContext,start_error},{reason,{'EXIT',nodistribution}},{offender,[{pid,undefined},{name,net_kernel},{mfargs,{net_kernel,start_link,[['rabbitmq-plugins23209',shortnames]]}},{restart_type,permanent},{shutdown,2000},{child_type,worker}]}]}^M
{error_logger,{#2015,9,24},{11,10,21#},supervisor_report,[{supervisor,{local,kernel_sup}},{errorContext,start_error}

,{reason,{shutdown,{failed_to_start_child,net_kernel,{'EXIT',nodistribution}}}},{offender,[
{pid,undefined} </codeblock>
      
      <p>There may be a specific set of steps to follow for a particular RabbitMQ cluster issues,
        but the general procedure for resetting an entire RabbitMQ cluster is as follows:<ol
          id="ol_p5w_5q1_mt">
          <li>Log into the HLM VM.</li>
          <li>Change to the <codeph>ansible</codeph> directory:<p>
              <codeblock>cd ~/&lt;cloud_name&gt;/clouds/&lt;cloud_name&gt;/001/stage/ansible</codeblock>
            </p>Where <codeph>&lt;cloud_name&gt;</codeph> is the name of your cloud.</li>
          <li>Execute the following commands:<p>
              <codeblock>ansible &lt;tier_name&gt; -i hosts -su cghelion -m apt -a "name=rabbitmq-server state=absent"
ansible &lt;tier_name&gt; -i hosts -su cghelion -a "apt-get -y autoremove"
ansible &lt;tier_name&gt; -i hosts -su cghelion -a "pkill -u rabbitmq"
ansible &lt;tier_name&gt; -i hosts -su cghelion -a "rm -r /var/lib/rabbitmq" 
echo "FND-RMQ" &gt; /tmp/deploy.role</codeblock>
            </p> Where <codeph>&lt;tier_name&gt;</codeph> is the control plane being deployed, for
            example: INFRA-CCP-T1.<p>For example: host=INFRA-CCP-T1-M1-NETCLM is composed
            of:</p><codeblock>tier=INFRA-CCP-T1 
host=M1
network=CLM            </codeblock></li>
          <li>Execute the following command to set the hdeploy to restart from the failing RMQ step.<p>
              <codeblock>echo "FND-RMO" &gt; /tmp/deploy.role</codeblock>
            </p></li>
          <li><xref href="carrier-grade-install-kvm-esx-GA.dita#topic10581/hdeploy">Restart the
              hdeploy process</xref>. <codeblock>hdeploy -t &lt;cloudname&gt;</codeblock></li>
        </ol></p>
      
    </section>
    </body>
</topic>
