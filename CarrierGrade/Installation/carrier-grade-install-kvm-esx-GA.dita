<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic
  PUBLIC "-//OASIS//DTD DITA Topic//EN" "http://docs.oasis-open.org/dita/v1.1/OS/dtd/topic.dtd" ><topic xml:lang="en-us" id="topic10581">
<title>HP Helion <tm tmtype="reg">OpenStack</tm> Carrier Grade 1.1: Installing the KVM + ESX
    Deployment</title>
<prolog>
<metadata>
<othermeta name="layout" content="default"/>
</metadata>
</prolog>
<body>
  <p>The first phase of the HP Helion Openstack Carrier Grade installation involves creating a virtual machine for the Helion Lifecycle Management (HLM) and then deploying the HP Helion Openstack cloud.</p>
    <p>The installation uses Ansible playbooks, which are files that contain which contains the
      steps that should be run.</p>

<section id="prepare"> <title>Prepare the system for deployment</title>
<p>Use the following steps to prepare the server on which the HLM VM will be deployed (the HLM host):</p>
<ol>

  <li>Edit the <codeph>/root/infra-ansible-playbooks/group_vars/all</codeph> file for your
          environment. For information on each variable, refer to the comments in the file with each
          variable. Make sure the <codeph>hlm_kvm_host</codeph> value is configured properly.
    <p>The <codeph>group_vars/all</codeph> file should appear similar to the following for a KVM + ESX deployment:</p>
  <codeblock>
    ############################################# Variables for HLM  #################################################################
    #These variables are relevant in both All Virtual and BareMetal scenarios.
    
    #Set this to 1 if you will not deploy a dcn cloud via this HLM node and will deploy only ovs cloud
    ovs_cloud_only:  0
    
    #Set this to 'bm' if cloud is being deployed over baremetal.
    #Set this to 'av' if the cloud is all virtual
    cloud_type: 'bm'
    
    #These are hlm related variables that must be changed according to your Baremetal Env
    hlm_login_id:       root
    hlm_password:       cghelion
    
    #The following variables are for CLM network IP details for HLM
    hlm_clmstaticip:    10.20.3.100
    hlm_clmnetmask:     255.255.255.0
    hlm_clmgateway:     10.20.3.1
    
    #The variables starting with cobbler_ are inputs that are usually given to initcobbler.sh. Set accordingly.
    cobbler_pxestartip: 10.20.1.100
    cobbler_pxeendip:   10.20.1.200
    cobbler_pxestaticip: 10.20.1.51
    cobbler_pxenetmask: 255.255.255.0
    
    #Set the location of your images that will be used by libvirt
    imagelocation: /var/lib/libvirt/images
    
    #Set the location of your infra-ansible-playbooks
    ansible_dir: ~/infra-ansible-playbooks
    
    ##################################################################################################################################
    
    ############################################# Variables for DCN ##################################################################
    #Set the location of dcn bits on KVM
    #There must be 4 debs, 2 tar.gz files, 1 vsc qcow2
    #You must copy your VSD qcow2 to the imagelocation on the KVM if you want to provision it on the same KVM as the HLM
    dcn_dir: ~/cg/dcn
    ##################################################################################################################################
    
    ############################################# Variables for VSD ##################################################################
    #Ignore these variables if creating an OVS cloud. This section is relevant only for DCN cloud - in both BM and All Virtual cases
    #These variables are used for VSD Configuration
    #If you have already configured a VSD and ignore the following variables
    
    dns_domain_name: dcn-one.helion.cg
    dns_address: 10.1.2.44
    vsd_address: 10.20.5.21
    vsd_gateway: 10.20.5.1
    vsd_netmask: 255.255.255.0
    vsd_name: vsd1
    vsdimagename: VSD-3.0.0_HP_r3.0_36
    upstream_ntp_servers:
    - 10.1.2.44
    - 16.110.135.123
    ###################################################################################################################################
  </codeblock></li>
  <li id="hosts">Check the hosts file <codeph>/root/infra-ansible-playbooks/hosts</codeph> file to
    enter the IP address of the vibr0 interface in the <codeph>hlm_kvm_host</codeph> field, as shown in the following example. Make sure the DCM network
          details are correct. Also, verify the CLM IP address in this file.</li>
</ol>
<codeblock>  [hlm_kvm_host]
  192.168.122.1 </codeblock>
</section>
<section id="deploy-the-hlm-and-vsd-vm">
      <title>Deploy the HLM Virtual Machine</title>
      <p>Use the following steps to deploy the HLM VM on the HLM Host using Ansible playbooks.</p>
      <ol>
        <li>Make sure the Ansible playbook file is not in executable mode.</li>
        <li>Execute the following
          command:<codeblock>ansible-playbook -i hosts setup_hlm_onBM.yml</codeblock> The command
          will do the following:<ul>
            <li>Copy both installation files (tar balls) to the HLM host, decrypt, and extract the
              files.</li>
            <li>Execute the <codeph>updatepackages</codeph> command.</li>
            <li>Execute the <codeph>prepareenv</codeph> command.</li>
            <li>Execute the <codeph>Init cobbler</codeph> command.</li>
            <li>Execute the <codeph>Importiso</codeph> command.<p>You will see similar message when
                the playbook is run successful:</p></li>
          </ul>
          <image href="../../media/CGH-ansible-playbook-run.png" id="image_bfp_pjm_vs">
            <alt>Sample Ansible Playbook run output</alt>
          </image></li>
        <li>Locate the IP address for the HLM VM in the
            <codeph>/root/infra-ansible-playbooks/group_vars/all</codeph> file of the HLM host under
          <codeph>hlm_clmstaticip</codeph> field.</li> 
      </ol>
      <p>For KVM + ESX deployments, proceed with <xref href="#topic10581/vsd" format="dita"/>.</p>    
<p>For KVM deployments, skip the following sections and proceed to <xref
          href="#topic10581/configure-a-json-file-for-installation" format="dita"/>.</p>

</section>
  
<section id="vsd"> <title>Configuring the VSD node, creating user and applying License</title>
  <p>During the <xref href="#topic10581/deploy-the-hlm-and-vsd-vm" format="dita"/> process, the
        Ansible playbook run creates a virtual machine is created to host the VSD node of DCN.</p>
<p>You need to log into the VSD node (using SSH), then create a default user and apply the required license.</p>
  
<p>After logging into the VDS node, execute the following command to launch a console window:</p>
<codeblock>virsh console vsd</codeblock>
<p>You should see the status as below from VSD VM.</p>
<p>
  <image href="../../media/CGH-install-virsh-vsd.png"/>
</p>
</section>

  <!--<section id="vsd-performance-workaround"> <title>VSD Performance workaround</title>
<p>By default VSD has only 8G memory. For better performance behavior, you can update the VSD memory to 16G.</p>
<ol>
<li>From HLM host, execute the following command to power down the VSD node:
          VM:<codeblock>virsh destroy vsd</codeblock></li>
<li>Execute the following command to edit the memory setting in the VSD XML
          file:<codeph>virsh edit vsd 

Current value 
    &lt;memory unit='KiB'&gt;8388608&lt;/memory&gt;
    &lt;currentMemory unit='KiB'&gt;8388608&lt;/currentMemory&gt;

Change to 
    &lt;memory unit='KiB'&gt;16777216&lt;/memory&gt;
    &lt;currentMemory unit='KiB'&gt;16777216&lt;/currentMemory&gt;
</codeph></li>
<li>Save the value<codeph>virsh save vsd
</codeph></li>
<li>Use the following command to start the
            VM:<codeph>virsh start vsd
</codeph><p>It can take 15 minutes or
            more to sync with NTP and to get all the other VSD services up.</p></li>
</ol>
</section>-->
<section id="launch-vsd-dashboard"> <title>Launch VSD Dashboard</title>
<p>On the HLM host:</p>
<ol>
<li>Using an browser such as Chrome or FireFox, navigate to the DCM IP of VSD.</li>
<li>In the log in page, enter the default
          credentials:<codeblock>User Name: Csproot 
Password: csproot 
Org: csp 
VSD Server : auto 
</codeblock></li>
</ol>
      <p/>
      <p>Follow the instructions in the pdf to get the license - <b>Get confirmation from
        Nayana</b></p>
      <p><xref
          href="https://wiki.hpcloud.net/display/HCG/HP+Helion+OpenStack+Carrier+Grade+%28NFVi%29+Home#HPHelionOpenStackCarrierGrade%28NFVi%29Home-HPHelionOpenStackCarrierGradeDCN/Nuage"
          format="html" scope="external"
          >https://wiki.hpcloud.net/display/HCG/HP+Helion+OpenStack+Carrier+Grade+%28NFVi%29+Home#HPHelionOpenStackCarrierGrade%28NFVi%29Home-HPHelionOpenStackCarrierGradeDCN/Nuage</xref></p>
</section>
<section id="applying-the-license"> <title>Applying the License</title>
<p>To install the required DCN license:</p>
<ol>
<li>From VSD Dashboard, click the <b>Open VSP Configuration</b> tab on the top right corner of the
          dashboard.</li>
<li>Click the <b>Licenses</b> tab and click <b>+</b>.</li>
<li>Copy and paste the license that you have receive into the screen that displays.</li>
</ol>
</section>
<section id="create-user-for-plugin-login"> <title>Create User for Plugin Login</title>
<p>You must create a user and add it to CMS Group.</p>
<ol>
<li>From VSD Dashboard, click the <b>Open VSP Configuration</b> tab on the top right corner of the
          dashboard.</li>
<li>Click the <b>CSP Users</b> tab and click <b>+</b>.</li>
<li>Create a user named <codeph>OSadmin</codeph> with the password <codeph>OSadmin</codeph>.</li>
<li>Add the user to the <codeph>CMS Group</codeph>.</li>
</ol>
</section>
  <section><title>Install the DVswitch</title></section>
  
  <section><title>Install the VRSVapp</title></section>
  <section id="configure-a-json-file-for-installation"> <title>Deploy the HLM VM</title>
<p>Use the following steps to deploy the HLM VM on the HLM host:</p>
<ol>
<li>Login to HLM VM.
            <codeblock>ssh &lt;HLM_VM_IP>
where: HLM_VM_IP is the IP of the HLM VM. </codeblock><p>Use
            the default credentials:
          <codeblock>User Name: root
Password: cghelion</codeblock></p></li>
    
<li>On the HLM VM, change to the home directory.<codeblock>cd ~</codeblock></li>
<li>Provision and configure your HP Helion OpenStack
            VM.<codeblock>hnewcloud  &lt;cloudname&gt; &lt;memphis&gt;</codeblock><p>Where:</p><ul>
            <li><codeph>&lt;cloudname&gt;</codeph> is the name of the cloud to create</li>
            <li><codeph>&lt;memphis&gt;</codeph> is the name of the template to use. The
              installation kit includes the fully-tested <codeph>memphis</codeph> template.</li>
          </ul><p>The command creates the <codeph>&lt;cloudname&gt;</codeph> directory, which will
            contain several JSON template files. </p></li></ol>
  </section>
  <section id="configure-node-provision-json">
      <title>Configure the JSON Node Provision file</title>
      <p>Edit the <codeph>node-provision.json</codeph> file. This template supplies input values to
        the <codeph>hprovision</codeph> script, later in the installation. </p>
        
      <p> Edit <codeph>node-provision.json</codeph> file based on following guidelines:</p> 
        <table>
        <tgroup cols="2">
          <colspec colname="col1" colsep="1" rowsep="1"/>
          <colspec colname="col2" colsep="1" rowsep="1"/>
          <thead>
            <row>
              <entry colsep="1" rowsep="1">Field</entry>
              <entry colsep="1" rowsep="1">Baremetal</entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry>name</entry>
              <entry>Name of the system you want to add</entry>
            </row>
            <row>
              <entry>Pxe-mac-address</entry>
              <entry>MAC address of the interface you want to PXE boot onto. This is not same as iLO
                MAC address,</entry>
            </row>
            <row>
              <entry>Pxe-interface</entry>
              <entry>The name of the interface on which PXE boot should occur. For example:
                  <codeph>eth0</codeph></entry>
            </row>
            <row>
              <entry>pm_type</entry>
              <entry>ipmilan </entry>
            </row>
            <row>
              <entry>pm_ip</entry>
              <entry>Power management IP (iLO ip)</entry>
            </row>
            <row>
              <entry>pm_user</entry>
              <entry>Power management user (iLO username)</entry>
            </row>
            <row>
              <entry>pm_pass</entry>
              <entry>Power management password (iLO password)</entry>
            </row>
            <row>
              <entry>node_group</entry>
              <entry>Enter the same value as <codeph>node-type</codeph> in the
                  <codeph>nodes.json</codeph> file used during cloud deployment. For example:
                `CCN-001-001`.</entry>
            </row>
            <row>
              <entry>failure_zone, vendor, model, os_partition_size, data_partition_size</entry>
              <entry>Enter the same value as for these fields an in the <codeph>nodes.json</codeph>
                file used during cloud deployment</entry>
            </row>
          </tbody>
        </tgroup>
      </table><p>To see a sample <codeph>node-provision.json</codeph> file, see <xref
        href="carrier-grade-install-pb-kvm-node-json.dita">Sample node-provision.json File for Installing the KVM + ESX Topology</xref>.</p>
    </section>
 <p> <b>Configure PXE boot</b> </p>
<p>After you edit the <codeph>node-provision.json</codeph> file, you must enable one-time PXE boot
        on the servers set the correct boot order. Execute the following on the HLM VM:</p>
<ol>
<li>Use the following command to install the <codeph>python-hpilo</codeph> module on HLM
  VM:<codeblock>pip install python-hpilo</codeblock><p>
            <codeph>python-hpilo</codeph> is a python library and command-line tool for
          iLO.</p></li>
<li>Copy the <codeph>ilopxebootonce.py</codeph> from the
            <systemoutput>/root/cg-hlm/dev-tools/ilopxebootonce.py</systemoutput> to the directory
          where you have the <codeph>node-provision.json</codeph> file.</li>
<li>Execute the following script:
  <codeblock>python ilopxebootonce.py node-provision.json</codeblock></li>
</ol>
<p>After the script is run, the <codeph>Current One-Time Boot Option</codeph> is set to <codeph>Network Device 1</codeph> on all the servers listed in <codeph>node-provision.json</codeph> file.</p>
    <!--<section id="configure-def-json"><title>Configure the JSON Definition file</title><p>Edit the <codeph>definition.json</codeph> file on the HLM VM.</p><p>To see a sample <codeph>definition.json</codeph> file, see <xref href="carrier-grade-install-pb-kvm-def-json.dita">Sample definition.json File for Installing the KVM + ESX Topology</xref>..</p></section><section id="configure-env-json"><title>Configure the JSON Environment file</title><p>Edit the <codeph>environment-provision.json</codeph> file on the HLM VM.</p><p>To see a sample <codeph>environment-provision.json</codeph> file, see see <xref href="carrier-grade-install-pb-kvm-env-json.dita">Sample environment.json File for Installing the KVM + ESX Topology</xref>..</p></section>-->
  
  <section id="configure-ansible-json">
    <title>Configure the JSON Ansible file</title>
    <p>Edit the <codeph>ansible.json</codeph> file on the HLM VM.</p>
    <p>To see a sample <codeph>ansible.json</codeph> file, see see <xref
      href="carrier-grade-install-pb-kvm-ansible-json.dita">Sample ansible.json File for Installing the KVM + ESX Topology</xref>..</p>
  </section>
  
  <section id="configure-esx-json">
    <title>Configure the JSON ESX file</title>
    <p>Edit the <codeph>esx.json</codeph> file on the HLM VM.</p>
    <p>To see a sample <codeph>esx.json</codeph> file, see see <xref
      href="carrier-grade-install-pb-kvm-esx-json.dita">Sample esx.json File for Installing the KVM + ESX Topology</xref>..</p>
  </section>
  
  <section id="configure-ldap-json">
    <title>Configure the JSON LDAP file</title>
    <p>Edit the <codeph>ldap.json</codeph> file on the HLM VM.</p>
    <p>To see a sample <codeph>ldap.json</codeph> file, see see <xref
      href="carrier-grade-install-pb-kvm-ldap-json.dita">Sample ldap.json File for Installing the KVM + ESX Topology</xref>..</p>
  </section>
  
  <section id="configure-wr-json">
    <title>Configure the JSON KVM file</title>
    <p>Edit the <codeph>wr.json</codeph> file on the HLM VM.</p>
    <p>To see a sample <codeph>wr.json</codeph> file, see see <xref
      href="carrier-grade-install-pb-kvm-wr-json.dita">Sample wr.json File for Installing the KVM + ESX Topology</xref>..</p>
  </section>
  
  <section><title>Configure the ESX Compute Proxy</title>
    <p>The HP Helion OpenStack Carrier Grade vCenter ESX compute proxy (compute proxy) is a driver
      that enables the Compute service to communicate with a VMware vCenter server managing one or
      more ESX hosts. The HP Helion OpenStack Compute Service (Nova) requires this driver to
      interface with VMware ESX hypervisor APIs.</p>
    <p>For instructions to configure the ESX compute proxy, see <xref
          href="carrier-grade-install-esx-proxy.dita#topic10581">Deploy the ESX COmpute Proxy</xref></p>
  </section>
  
<section id="create-a-new-cloud-template-and-bring-the-cloud-nodes-up"> <title>Create new cloud template and bring the cloud nodes up</title>
<ol>
<li>Use the following script to start the provisioning of the HP Helion OpenStack
            cloud:<p>hprovision <codeph>&lt;cloudname&gt;</codeph>
          </p><p>Where:</p><ul>
            <li><codeph>&lt;cloudname&gt;</codeph> is the name of the cloud to create</li>
          </ul><p>This script will PXE boot the nodes specified in
              <codeph>node-provision.json</codeph> file. The script also tracks the PXE boot
            completion process and will create the <codeph>nodes.json</codeph> file in the
            directory. </p><p>You can log in to the iLO of each of  the nodes to monitor the boot
            process.</p></li>
        <li>Make sure the nodes are booted up using iLO.</li>
  <li>Update the <codeph>node-provision.json</codeph> file used in the <xref type="section"
            href="#topic10581/pxe-boot">previous step</xref>.<p>a. Change to the
              <codeph>&lt;cloudname&gt;</codeph>
            directory:</p><codeblock>cd ~/&lt;cloudname&gt;</codeblock><p>b. Once the baremetal
            nodes are provisioned, make sure the <codeph>nodes.json</codeph> file is generated. The
              <codeph>nodes.json</codeph> file will have entries of 3 controllers, 2 DCN Hosts, 2
            VRS-G and the compute proxy nodes.</p></li>
        <li>Verify that the ESX Compute Provy VM is installed and active.
         <ol>
           <li>Ping proxy node from HLM VM with the static CLM IP in the <codeph>esx.json</codeph> file'</li>
           <li>SSH to compute proxy node from the HLM VM using the static CLM IP.</li>
         </ol></li>
        <li>If you need cinder vmdk support, ????????</li>
<li>Modify the <codeph>environment.json</codeph> file to configure the VLANs and network addresses
          as appropriate for your environment. Set the following for the CLM, CAN, and BLS
          network:<codeblock>"cidr": 
"start-address": </codeblock> The three controller nodes
          should have CLM, CAN, EXT, BLS on eth0 and TUL on eth1. <!--Hiding for RC0 
              <p>The two compute nodes should have CLM, EXT, BLS on eth0 and TUL on eth1.</p>
           -->
          <p>To see a sample <codeph>environment-provision.json</codeph> file, see see <xref
              href="carrier-grade-install-pb-kvm-env-json.dita">Sample environment.json File for
              Installing the KVM + ESX Topology</xref>..</p></li>
<li>Modify the <codeph>definition.json</codeph> file: <ol>
            <li>Set the number of compute systems to 2.
              <codeblock>"count": 2, //number of computes in the resource pool</codeblock></li>
            <li>Update the <codeph>ansible-vars</codeph> section with all the information based on
              your setup.</li>
            <li>Make sure you have two NTP entries in the <codeph>upstream_ntp_servers</codeph>
              fields in the <codeph>definition.json</codeph> file as seen in the following example.
              If you have only one NTP server in your environment, specify the same NTP server
                twice.<p>To see a sample <codeph>definition.json</codeph> file, see <xref
                  href="carrier-grade-install-pb-kvm-def-json.dita">Sample definition.json File for
                  Installing the KVM + ESX Topology</xref>..</p></li>
          </ol></li>
  
  <li>Use the following steps to modify to the <codeph>cinder/blocks</codeph> directory for your cloud: 
  <ul>
    <li>Change to the <codeph>cinder/blocks</codeph> directory:
      <codeblock>cd ~/&lt;cloudname>/services/cinder/blocks</codeblock>
      Where &lt;cloudname> is the name you assigned to the cloud.</li>
    <li>Copy the <codeph>cinder_conf_default.hp3parSample</codeph> file to the
                <codeph>cinder_conf_default</codeph> file and edit the file to configure to 3PAR
              settings. For example:
              <codeblock>hp3par_api_url=https://&lt;hp3par_ip>:8080/api/v1
hp3par_username=&lt;hp3par_user>
hp3par_password=&lt;hp3par_user_password
hp3par_cpg=bronze
san_ip=&lt;san_ip>
san_login=&lt;san_user>
san_password=&lt;san_password>
hp3par_iscsi_ips=&lt;iscsi_ip1>,&lt;iscsi_ip2>,&lt;iscsi_ip3>,&lt;iscsi_ip4>
volume_driver=cinder.volume.drivers.san.hp.hp_3par_iscsi.HP3PARISCSIDriver
hp3par_debug=False
hp3par_iscsi_chap_enabled=false</codeblock></li></ul></li>            
<li>Once you have correctly edited all the json Cloud Model files, run the HP Helion OpenStack
          Configuration Processor: <codeblock>hcfgproc -d definition.json</codeblock><p>The
              <codeph>hcfgproc</codeph> script gets installed in <codeph>/usr/local/bin</codeph> by
            the <codeph>prepare-env</codeph> script. The script generates a <codeph>clouds/</codeph>
            directory within the directory. </p></li>
<li>Review the CloudDiagram, <codeph>hosts.hf</codeph>, and
            <codeph>net/interfaces.d/eth.cfg</codeph> files to make sure the network settings are
          correct.</li>
<li>Initialize network interfaces on all the cloud nodes using the following command:
            <codeblock><codeph>hnetinit &lt;cloudname&gt;</codeph></codeblock><p>You can run this
            command from any directory.</p><p>After this command completes, all cloud nodes and CLM
            network interfaces should be set correctly.</p></li>

<li>Use the following command to deploy the cloud:
            <codeblock><codeph>hdeploy &lt;cloudname&gt;</codeph></codeblock><p>Once cloud
            deployment is successfully complete, there are 3 controller nodes, 2 DCN Nodes, 2 VRS-G
            nodes and 1 compute proxy.</p></li>
</ol>
<p>You can verify that the non-KVM region cloud was
            installed properly by <xref
              href="../AdminGuideNew/Dashboard/carrier-grade.dashboard.launch.dita#topic1160"
              >launching the Helion dashboard</xref> in a standard browser. </p>
  <p>Use the CAN VIP available in either location:</p>
  <ul>	
    <li>open the <codeph>~clouds/&lt;cloud name>/001/stage/net/hosts.hf</codeph> and search for “VIP”</li>
    <li>open the /root/stackrc file on any controller node</li>
  </ul>





</section>
<section id="next-step"> <title>Next Step</title>
<p>
        <xref href="carrier-grade-install-kvm-cloud-GA.dita">Deploying the KVM Region</xref>
      </p>
</section>
</body>
</topic>
