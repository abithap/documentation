<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic
  PUBLIC "-//OASIS//DTD DITA Topic//EN" "http://docs.oasis-open.org/dita/v1.1/OS/dtd/topic.dtd" ><topic xml:lang="en-us" id="topic10581">
<title>HP Helion <tm tmtype="reg">OpenStack</tm> Carrier Grade 1.1: Installing the KVM + ESX
    Deployment</title>
<prolog>
<metadata>
<othermeta name="layout" content="default"/>
</metadata>
</prolog>
<body>
  <p>The first phase of the HP Helion Openstack Carrier Grade installation involves creating a
      virtual machine for the Helion Lifecycle Management (HLM), deploying the HP Helion OpenStack
      cloud, installing DCN components, and installing the ESX compute proxy.</p>
    <p>The installation uses Ansible playbooks, which are files that contain which contains scripts
      that execute required installation processes.</p>

<section id="prepare"> <title>Prepare the system for deployment</title>
<p>Use the following steps to prepare the server on which the HLM VM will be deployed (the HLM host):</p>
<ol>
        <li>Log into the HLM host.</li>

  <li>Edit the <codeph>/root/infra-ansible-playbooks/group_vars/all</codeph> file for your
          environment. For information on each variable, refer to the comments in the file with each
          variable. Make sure the <codeph>hlm_kvm_host</codeph> value is configured properly.
    <p>The <codeph>group_vars/all</codeph> file should appear similar to the following for a KVM + ESX deployment:</p>
  <codeblock>
    ############################################# Variables for HLM  #################################################################
    #These variables are relevant in both All Virtual and BareMetal scenarios.
    
    #Set this to 1 if you will not deploy a dcn cloud via this HLM node and will deploy only ovs cloud
    ovs_cloud_only:  0
    
    #Set this to 'bm' if cloud is being deployed over baremetal.
    #Set this to 'av' if the cloud is all virtual
    cloud_type: 'bm'
    
    #These are hlm related variables that must be changed according to your Baremetal Env
    hlm_login_id:       root
    hlm_password:       cghelion
    
    #The following variables are for CLM network IP details for HLM
    hlm_clmstaticip:    10.20.3.100
    hlm_clmnetmask:     255.255.255.0
    hlm_clmgateway:     10.20.3.1
    
    #The variables starting with cobbler_ are inputs that are usually given to initcobbler.sh. Set accordingly.
    cobbler_pxestartip: 10.20.1.100
    cobbler_pxeendip:   10.20.1.200
    cobbler_pxestaticip: 10.20.1.51
    cobbler_pxenetmask: 255.255.255.0
    
    #Set the location of your images that will be used by libvirt
    imagelocation: /var/lib/libvirt/images
    
    #Set the location of your infra-ansible-playbooks
    ansible_dir: ~/infra-ansible-playbooks
    
    ##################################################################################################################################
    
    ############################################# Variables for DCN ##################################################################
    #Set the location of dcn bits on KVM
    #There must be 4 debs, 2 tar.gz files, 1 vsc qcow2
    #You must copy your VSD qcow2 to the imagelocation on the KVM if you want to provision it on the same KVM as the HLM
    dcn_dir: ~/cg/dcn
    ##################################################################################################################################
    
    ############################################# Variables for VSD ##################################################################
    #Ignore these variables if creating an OVS cloud. This section is relevant only for DCN cloud - in both BM and All Virtual cases
    #These variables are used for VSD Configuration
    #If you have already configured a VSD and ignore the following variables
    
    dns_domain_name: dcn-one.helion.cg
    dns_address: 10.1.2.44
    vsd_address: 10.20.5.21
    vsd_gateway: 10.20.5.1
    vsd_netmask: 255.255.255.0
    vsd_name: vsd1
    vsdimagename: VSD-3.0.0_HP_r3.0_36
    upstream_ntp_servers:
    - 10.1.2.44
    - 16.110.135.123
    ###################################################################################################################################
  </codeblock></li>
  <li id="hosts">Check the hosts file <codeph>/root/infra-ansible-playbooks/hosts</codeph> file to
    enter the IP address of the vibr0 interface in the <codeph>hlm_kvm_host</codeph> field, as shown in the following example. Make sure the DCM network
          details are correct. Also, verify the CLM IP address in this file.</li>
</ol>
<codeblock>  [hlm_kvm_host]
  192.168.122.1 </codeblock>
</section>
<section id="deploy-the-hlm-and-vsd-vm">
      <title>Deploy the HLM Virtual Machine</title>
      <p>Use the following steps to deploy the HLM VM on the HLM Host using Ansible playbooks.</p>
      <ol>
        <li>Make sure the Ansible playbook file is not in executable mode.</li>
        <li>Execute the following
          command:<codeblock>ansible-playbook -i hosts setup_hlm_onBM.yml</codeblock> The command
          will do the following:<ul>
            <li>Copy both installation files (tar balls) to the HLM host, decrypt, and extract the
              files.</li>
            <li>Execute the <codeph>updatepackages</codeph> command.</li>
            <li>Execute the <codeph>prepareenv</codeph> command.</li>
            <li>Execute the <codeph>Init cobbler</codeph> command.</li>
            <li>Execute the <codeph>Importiso</codeph> command.<p>You will see similar message when
                the playbook is run successful:</p></li>
          </ul>
          <image href="../../media/CGH-ansible-playbook-run.png" id="image_bfp_pjm_vs">
            <alt>Sample Ansible Playbook run output</alt>
          </image></li>
        <li>Locate the IP address for the HLM VM in the
            <codeph>/root/infra-ansible-playbooks/group_vars/all</codeph> file of the HLM host under
          <codeph>hlm_clmstaticip</codeph> field.</li> 
      </ol>

</section>
  
<section id="vsd"> <title>Configuring the VSD node, creating user and applying License</title>
  <p>During the <xref href="#topic10581/deploy-the-hlm-and-vsd-vm" format="dita"/> process, the
        Ansible playbook run creates a virtual machine is created to host the VSD node of DCN. You
        need to create a default user and apply the required license</p>
<p>Before you start, make sure the VSD node was installed by logging into the VSD node using
        SSH.</p>
  
<p>After logging into the VSD node, execute the following command to launch a console window:</p>
<codeblock>virsh console vsd</codeblock>
<p>You should see the status as below from VSD VM.</p>
<p>
  <image href="../../media/CGH-install-virsh-vsd.png"/>
</p>
</section>

  <!--<section id="vsd-performance-workaround"> <title>VSD Performance workaround</title>
<p>By default VSD has only 8G memory. For better performance behavior, you can update the VSD memory to 16G.</p>
<ol>
<li>From HLM host, execute the following command to power down the VSD node:
          VM:<codeblock>virsh destroy vsd</codeblock></li>
<li>Execute the following command to edit the memory setting in the VSD XML
          file:<codeph>virsh edit vsd 

Current value 
    &lt;memory unit='KiB'&gt;8388608&lt;/memory&gt;
    &lt;currentMemory unit='KiB'&gt;8388608&lt;/currentMemory&gt;

Change to 
    &lt;memory unit='KiB'&gt;16777216&lt;/memory&gt;
    &lt;currentMemory unit='KiB'&gt;16777216&lt;/currentMemory&gt;
</codeph></li>
<li>Save the value<codeph>virsh save vsd
</codeph></li>
<li>Use the following command to start the
            VM:<codeph>virsh start vsd
</codeph><p>It can take 15 minutes or
            more to sync with NTP and to get all the other VSD services up.</p></li>
</ol>
</section>-->
<section id="launch-vsd-dashboard">
      <title>Apply the license</title>
      <p>On the HLM host:</p>
      <ol>
        <li>Using an browser such as Chrome or FireFox, navigate to the DCM IP of VSD.</li>
        <li>In the log in page, enter the default
          credentials:<codeblock>User Name: Csproot 
Password: csproot 
Org: csp 
VSD Server : auto 
</codeblock></li>
        <li>From VSD Dashboard, click the <b>Open VSP Configuration</b> tab on the top right corner of the
          dashboard.</li>
        <li>Click the <b>Licenses</b> tab and click <b>+</b>.</li>
        <li>Copy and paste your DCN license key into the screen that displays.</li>

      </ol>
      <!--Follow the instructions in the pdf to get the license - <b>Get confirmation from Nayana</b><p><xref href="https://wiki.hpcloud.net/display/HCG/HP+Helion+OpenStack+Carrier+Grade+%28NFVi%29+Home#HPHelionOpenStackCarrierGrade%28NFVi%29Home-HPHelionOpenStackCarrierGradeDCN/Nuage" format="html" scope="external">https://wiki.hpcloud.net/display/HCG/HP+Helion+OpenStack+Carrier+Grade+%28NFVi%29+Home#HPHelionOpenStackCarrierGrade%28NFVi%29Home-HPHelionOpenStackCarrierGradeDCN/Nuage</xref></p>-->
    </section>
<section id="create-user-for-plugin-login"> <title>Create User for Plugin Login</title>
<p>You must create a user and add it to CMS Group.</p>
<ol>
<li>From VSD Dashboard, click the <b>Open VSP Configuration</b> tab on the top right corner of the
          dashboard.</li>
<li>Click the <b>CSP Users</b> tab and click <b>+</b>.</li>
<li>Create a user named <codeph>OSadmin</codeph> with the password <codeph>OSadmin</codeph>.</li>
<li>Add the user to the <codeph>CMS Group</codeph>.</li>
</ol>
</section>
  <section><title>Install the DVswitch</title></section>
  
  <section><title>Install the VRSVapp</title></section>
  <section id="configure-a-json-file-for-installation"> <title>Deploy the HP Helion OpenStack VM</title>
<p>Use the following steps  on the HLM host to deploy the HP Helion OpenStack VM:</p>
<ol>
<li>Login to HLM VM.
            <codeblock>ssh &lt;HLM_VM_IP>
where: HLM_VM_IP is the IP of the HLM VM. </codeblock><p>Use
            the default credentials:
          <codeblock>User Name: root
Password: cghelion</codeblock></p></li>
    
<li>On the HLM VM, change to the home directory.<codeblock>cd ~</codeblock></li>
<li>Provision and configure your HP Helion OpenStack
            VM.<codeblock>hnewcloud  &lt;cloudname&gt; &lt;memphis&gt;</codeblock><p>Where:</p><ul>
            <li><codeph>&lt;cloudname&gt;</codeph> is the name of the cloud to create</li>
            <li><codeph>&lt;memphis&gt;</codeph> is the name of the template to use. The
              installation kit includes the fully-tested <codeph>memphis</codeph> template.</li>
          </ul><p>The command creates the <codeph>&lt;cloudname&gt;</codeph> directory, which will
            contain several JSON template files. </p></li></ol>
  </section>
  <section id="configure-node-provision-json">
      <title>Configure the JSON Node Provision file</title>
      <p>Edit the <codeph>node-provision.json</codeph> file. This template supplies input values to
        the <codeph>hprovision</codeph> script, later in the installation. </p>
        
      <p> Edit <codeph>node-provision.json</codeph> file based on following guidelines:</p> 
        <table>
        <tgroup cols="2">
          <colspec colname="col1" colsep="1" rowsep="1"/>
          <colspec colname="col2" colsep="1" rowsep="1"/>
          <thead>
            <row>
              <entry colsep="1" rowsep="1">Field</entry>
              <entry colsep="1" rowsep="1">Baremetal</entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry>name</entry>
              <entry>Name of the system you want to add</entry>
            </row>
            <row>
              <entry>Pxe-mac-address</entry>
              <entry>MAC address of the interface you want to PXE boot onto. This is not same as iLO
                MAC address,</entry>
            </row>
            <row>
              <entry>Pxe-interface</entry>
              <entry>The name of the interface on which PXE boot should occur. For example:
                  <codeph>eth0</codeph></entry>
            </row>
            <row>
              <entry>pm_type</entry>
              <entry>ipmilan </entry>
            </row>
            <row>
              <entry>pm_ip</entry>
              <entry>Power management IP (iLO ip)</entry>
            </row>
            <row>
              <entry>pm_user</entry>
              <entry>Power management user (iLO username)</entry>
            </row>
            <row>
              <entry>pm_pass</entry>
              <entry>Power management password (iLO password)</entry>
            </row>
            <row>
              <entry>node_group</entry>
              <entry>Enter the same value as <codeph>node-type</codeph> in the
                  <codeph>nodes.json</codeph> file used during cloud deployment. For example:
                `CCN-001-001`.</entry>
            </row>
            <row>
              <entry>failure_zone, vendor, model, os_partition_size, data_partition_size</entry>
              <entry>Enter the same value as for these fields an in the <codeph>nodes.json</codeph>
                file used during cloud deployment</entry>
            </row>
          </tbody>
        </tgroup>
      </table><p>To see a sample <codeph>node-provision.json</codeph> file, see <xref
        href="carrier-grade-install-pb-kvm-node-json.dita">Sample node-provision.json File for Installing the KVM + ESX Topology</xref>.</p>
    </section>
 <p> <b>Configure PXE boot</b> </p>
<p>After you edit the <codeph>node-provision.json</codeph> file, you must enable one-time PXE boot
        on the servers set the correct boot order. Execute the following on the HLM VM:</p>
<ol>
<li>Use the following command to install the <codeph>python-hpilo</codeph> module on HLM
  VM:<codeblock>pip install python-hpilo</codeblock><p>
            <codeph>python-hpilo</codeph> is a python library and command-line tool for
          iLO.</p></li>
<li>Copy the <codeph>ilopxebootonce.py</codeph> from the
            <systemoutput>/root/cg-hlm/dev-tools/ilopxebootonce.py</systemoutput> to the directory
          where you have the <codeph>node-provision.json</codeph> file.</li>
<li>Execute the following script:
  <codeblock>python ilopxebootonce.py node-provision.json</codeblock></li>
</ol>
<p>After the script is run, the <codeph>Current One-Time Boot Option</codeph> is set to <codeph>Network Device 1</codeph> on all the servers listed in <codeph>node-provision.json</codeph> file.</p>
    <section id="configure-def-json"><title>Configure the JSON Definition file</title>
      <p>Edit the <codeph>definition.json</codeph> file on the HLM VM:: <ol>
        <li>Set the number of compute systems to 2.
          <codeblock>"count": 2, //number of computes in the resource pool</codeblock></li>
        <li>Update the <codeph>ansible-vars</codeph> section with all the information based on
          your setup.</li>
        <li>Make sure you have two NTP entries in the <codeph>upstream_ntp_servers</codeph>
          fields in the <codeph>definition.json</codeph> file. If you have only one NTP server
          in your environment, specify the same NTP server twice.<p>To see a sample
            <codeph>definition.json</codeph> file, see <xref
              href="carrier-grade-install-pb-kvm-def-json.dita">Sample definition.json File for
              Installing the KVM + ESX Topology</xref>.</p></li>
      </ol></p>
      <p>To see a sample <codeph>definition.json</codeph> file, see <xref href="carrier-grade-install-pb-kvm-def-json.dita">Sample 
        definition.json File for Installing the KVM + ESX Topology</xref>..</p></section>
  
  <section id="configure-env-json"><title>Configure the JSON Environment file</title>
    <p>Edit the <codeph>environment.json</codeph> file on the HLM VM to configure the VLANs and network
      addresses as appropriate for your environment. Set the following for the CLM, CAN, and BLS
      network:
      <codeblock>"cidr": 
        "start-address": </codeblock> The three controller nodes
      should have CLM, CAN, EXT, BLS on eth0 and TUL on eth1.</p>
    <p>To see a 
      sample <codeph>environment.json</codeph> file, 
      see  <xref href="carrier-grade-install-pb-kvm-env-json.dita">Sample environment.json File for 
        Installing the KVM + ESX Topology</xref>..</p></section>
  
  <section id="configure-ansible-json">
    <title>Configure the JSON Ansible file</title>
    <p>Edit the <codeph>ansible.json</codeph> file on the HLM VM.</p>
    <p>To see a sample <codeph>ansible.json</codeph> file,  see <xref
          href="carrier-grade-install-pb-kvm-ansible-json.dita">Sample ansible.json File for
          Installing the KVM + ESX Topology</xref>.</p>
  </section>
  
  <section id="configure-esx-json">
    <title>Configure the JSON ESX file</title>
    <p>Edit the <codeph>esx.json</codeph> file on the HLM VM. This file is called by the script that
        installs the HP Helion OpenStack cloud, later in the installation. </p>
    <p>To see a sample <codeph>esx.json</codeph> file, see <xref
          href="carrier-grade-install-pb-kvm-esx-json.dita">Sample esx.json File for Installing the
          KVM + ESX Topology</xref>.</p>
  </section>
  
  <section id="configure-ldap-json">
    <title>Configure the JSON LDAP file</title>
    <p>Edit the <codeph>ldap.json</codeph> file on the HLM VM.</p>
    <p>To see a sample <codeph>ldap.json</codeph> file, see
        <xref
      href="carrier-grade-install-pb-kvm-ldap-json.dita">Sample ldap.json File for Installing the KVM + ESX Topology</xref>.

      </p>
  </section>
  
  <section id="configure-wr-json">
    <title>Configure the JSON KVM file</title>
    <p>Edit the <codeph>wr.json</codeph> file on the HLM VM.</p>
    <p>To see a sample <codeph>wr.json</codeph> file, see
<xref
      href="carrier-grade-install-pb-kvm-wr-json.dita">Sample wr.json File for Installing the KVM + ESX Topology</xref>.

      </p>
  
  </section>
      <section><title>Configure the ESX Compute Proxy</title>
    <p>The HP Helion OpenStack Carrier Grade vCenter ESX compute proxy (compute proxy) is a driver
      that enables the Compute service to communicate with a VMware vCenter server managing one or
      more ESX hosts. The HP Helion OpenStack Compute Service (Nova) requires this driver to
      interface with VMware ESX hypervisor APIs.</p>
    <p>For instructions to configure the ESX compute proxy, see <xref
          href="carrier-grade-install-esx-proxy.dita#topic10581">Deploy the ESX Compute
        Proxy</xref></p>
  </section>
  
<section id="create-a-new-cloud-template-and-bring-the-cloud-nodes-up">
      <title>Create new cloud template and bring the cloud nodes up</title>
      <ol>
        <li>On the HLM host, use the following script to start the provisioning of the HP Helion
          OpenStack cloud:
            <codeblock>hprovision &lt;cloudname&gt;          </codeblock><p>Where:</p><ul>
            <li><codeph>&lt;cloudname&gt;</codeph> is the name of the cloud to create</li>
          </ul><p>This script will PXE boot the nodes specified in
              <codeph>node-provision.json</codeph> file. The script also tracks the PXE boot
            completion process and will create the <codeph>nodes.json</codeph> file in the
            directory. </p><p>You can log in to the iLO of each of the nodes to monitor the boot
            process.</p></li>
        <li>Make sure the nodes are booted up using iLO.</li>
        <li>Update the <codeph>node-provision.json</codeph> file used in the <xref type="section"
            href="#topic10581/pxe-boot">previous step</xref>.<p>a. Change to the
              <codeph>&lt;cloudname&gt;</codeph>
            directory:</p><codeblock>cd ~/&lt;cloudname&gt;</codeblock><p>b. Once the baremetal
            nodes are provisioned, make sure the <codeph>nodes.json</codeph> file is generated. The
              <codeph>nodes.json</codeph> file will have entries of 3 controllers, 2 DCN Hosts, 2
            VRS-G and the compute proxy nodes.</p></li>
        <li>Verify that the ESX Compute Provy VM is installed and active. <ol>
            <li>Ping proxy node from HLM VM with the static CLM IP in the <codeph>esx.json</codeph>
              file.</li>
            <li>SSH to the compute proxy node from the HLM VM using the static CLM IP.</li>
          </ol></li>
        <li>If you need cinder vmdk support, execute the following steps:
          <ol>
            <li>Go to /root/&lt;cloud name>/services/cinder/blocks folder, copy <codeph>cinder_conf.vmdksample</codeph> to 
              <codeph>cinder_conf</codeph> 
            in the same folder and update the entries based on your infrastructure.
            <codeblock>
            [DEFAULT]
            enabled_backends=vmdk
            
            [vmdk]
            volume_backend_name = &lt;mybackendname3>
            vmware_host_ip = &lt;hostip>
            vmware_host_username = &lt;username>
            vmware_host_password = &lt;password>
            vmware_volume_folder = &lt;volumes_folder>
                      vmware_image_transfer_timeout_secs = 7200
                      vmware_task_poll_interval = 0.5
                      vmware_max_objects_retrieval = 100
                      volume_driver = cinder.volume.drivers.vmware.vmdk.VMwareVcVmdkDriver
            </codeblock></li>
          </ol>        
        </li>

<!--        <li>Use the following steps to modify to the <codeph>cinder/blocks</codeph> directory for
          your cloud: <ul>
            <li>Change to the <codeph>cinder/blocks</codeph> directory:
              <codeblock>cd ~/&lt;cloudname>/services/cinder/blocks</codeblock> Where &lt;cloudname>
              is the name you assigned to the cloud.</li>
            <li>Copy the <codeph>cinder_conf_default.hp3parSample</codeph> file to the
                <codeph>cinder_conf_default</codeph> file and edit the file to configure to 3PAR
              settings. For example:
              <codeblock>hp3par_api_url=https://&lt;hp3par_ip>:8080/api/v1
hp3par_username=&lt;hp3par_user>
hp3par_password=&lt;hp3par_user_password
hp3par_cpg=bronze
san_ip=&lt;san_ip>
san_login=&lt;san_user>
san_password=&lt;san_password>
hp3par_iscsi_ips=&lt;iscsi_ip1>,&lt;iscsi_ip2>,&lt;iscsi_ip3>,&lt;iscsi_ip4>
volume_driver=cinder.volume.drivers.san.hp.hp_3par_iscsi.HP3PARISCSIDriver
hp3par_debug=False
hp3par_iscsi_chap_enabled=false</codeblock></li>
          </ul></li>
-->        
        <li>Once you have correctly edited all the JSON files, run the HP Helion OpenStack
          Configuration Processor: <codeblock>hcfgproc -d definition.json</codeblock><p>The
              <codeph>hcfgproc</codeph> script gets installed in <codeph>/usr/local/bin</codeph> by
            the <codeph>prepare-env</codeph> script. The script generates a <codeph>clouds/</codeph>
            directory within the directory. </p></li>
        <li>Review the CloudDiagram, <codeph>hosts.hf</codeph>, and
            <codeph>net/interfaces.d/eth.cfg</codeph> files to make sure the network settings are
          correct.</li>
        <li>Initialize network interfaces on all the cloud nodes using the following command:
            <codeblock><codeph>hnetinit &lt;cloudname&gt;</codeph></codeblock><p>You can run this
            command from any directory.</p><p>After this command completes, all cloud nodes and CLM
            network interfaces should be set correctly.</p></li>
        <li>Use the following command to deploy the cloud:
            <codeblock><codeph>hdeploy &lt;cloudname&gt;</codeph></codeblock><p>Once cloud
            deployment is successfully complete, there are 3 controller nodes, 2 DCN Nodes, 2 VRS-G
            nodes and 1 compute proxy.</p></li>
      </ol>
      <p>You can verify that the non-KVM region cloud was installed properly by <xref
          href="../AdminGuideNew/Dashboard/carrier-grade.dashboard.launch.dita#topic1160">launching
          the Helion dashboard</xref> in a standard browser using the CAN VIP available in either
        location:</p>
      <ul>
        <li>open the <codeph>~clouds/&lt;cloud name>/001/stage/net/hosts.hf</codeph> and search for
          “VIP”</li>
        <li>open the /root/stackrc file on any controller node</li>
      </ul>
    </section>
<section><title>Verify the VSC VMs</title>
  <p>Use the following steps to verify that the VSC VMs were installed and are operational</p>
  <ol>
    <li>SSH to your VSC VM from HLM Host using the DCM IP. The default username and password:
            <codeph>admin/admin</codeph></li>
  <li>Execute the following command:
    <codeblock>admin display-config</codeblock></li>
    <li>Execute the followng commands to verify the VMs are active:
          <codeblock>show vswitch-controller vsd
show vswitch-controller xmpp-server
ping router "management" &lt;vsd IP or domain name> </codeblock>
        </li>
  </ol>
</section>
  <section><title>Verify the VRS-G Node</title> The installation creates a VRS-G node as part of the
      cloud deployment. Use the following command to verify that the VRS-G is active: <p>
        <codeblock>show vswitch-controller vswitches </codeblock>
      </p><p>The output for the command should appear similar to the following image:</p>
      <image href="../../media/CGH-install-verify-vrsg.png" id="image_x3t_jvp_vs"/>
      <p>Verify that the domain name and DMS server are listed in the
          <codeph>/etc/resolv.conf</codeph> file on all the cloud nodes </p><codeblock>cat etc/resolv.conf          </codeblock>
      <image href="../../media/CGH-install-resolv-conf.jpg" id="image_idc_5wp_vs">
        <alt>Verify VRS-G</alt>
      </image>
    </section>
  <section><title>Create external network and subnet</title>
    <ol>
      <li>Obtain the static IP route for the external network/FIP EXT Uplink Subnet"
      <codeblock>
        #
        interface Vlan-interface 1553
        description NFV TestBed - EXT UPLINK SUBNET
        ip address 10.200.53.1 255.255.255.192
        #
        .
        .
        .
        IP Packet Frame Type: PKTFMT_ETHNT_2,  Hardware Address: b8af-67bd-f516
        .
        .
        .
        Destination/Mask    Proto  Pre  Cost         NextHop         Interface
        
        10.200.53.0/26      Direct 0    0            10.200.53.1     Vlan1553
        10.200.53.1/32      Direct 0    0            127.0.0.1       InLoop0
        10.200.53.64/26     Static 60   0            10.200.53.2     Vlan1553
        10.200.53.128/26    Static 60   0            10.200.53.2     Vlan1553
        10.200.53.192/26    Static 60   0            10.200.53.2     Vlan1553
      </codeblock></li>  
      <li>Execute the following command to create the external netwoork:
      <codeblock>neutron net-create ext-net --router:external</codeblock></li>
      <li>Execute the following command to create a subnet for the external netwoork:
            <codeblock>neutron subnet-create ext-net &lt;subnet-ip> --name &lt;name></codeblock><p>where:
            &lt;subnet-ip> is the IP address range to assign and &lt;name> is a descriptive
            name.</p><p>For
          example:</p><codeblock>neutron subnet-create ext-net 10.200.53.64/26 --name extsub1</codeblock></li>
      <li>Use the following command to create a virtual router: <codeblock>neutron router-create &lt;name></codeblock>
          <p>where: &lt;name> is a descriptive name.</p></li></ol>
  </section>
  <section><title>Onboard the Gateway</title>
    
    <ol><li>Launch the VSD dashboard.</li>
      <li>Click the <b>Gateways</b> tab</li>
      <li>Click the arrow next to the new gateway to onboard that gateway</li>
      <li>Click <b>Organizations</b> to add permissions to the Gateway.</li>
    </ol>
  <p>Note: To create a redundant grouping of the VRS-G, refer to the DCN documentation.</p>
</section>
  <section><title>Retrieving the FIP Name from VSD for external Network</title>
    <ol><li>Launch the VSD.</li>
      <li>Go to <b>VSD Dashboard->Open Data Center Configuration->Shared Network tab</b>
        </li>
        <li>For the Floating IP subnet, click the <b>Edit</b> button to get the FIP name. <p>This is
            the same name you assiigned when you created the subnet.</p></li>
      <li> Substitute this value in the FIP_NAME="3d2b2090-2044-4580-92ce-ab5a6e4a9184" 
        [example]  in the curl scrip that will be run on HLM KVM Host.</li>
      </ol>
  </section>
    <section><title>Add VLAN to VRSG network device </title>
      <ol><li>Based on your VLAN ID, run the below command on VRS-G node1
        
        <p> For example: <codeph>nuage-vlan-config mod eth0 Access 1500-2000</codeph></p></li>
        
        <li>Add the VLAN ID [EXT Net] on the VSD dashboard for eth0</li>
      </ol>
  </section>
    <section><title>Run the curl script</title></section>
    <ol>
      <li>Install jq on the server from where you are running the curl script</li>
      <li>Modify the curl script for your environment:
        <codeblock>cat vrsg_script_cloud2.sh</codeblock>
        <p>Under Parameters, edit:</p>
      <ul>
        <li>VLAN</li>
        <li>VSD_IP. Add DCM VSD IP</li>
        <li>FIP_NAME. Enter a unique name show in VSD created for external network floating ip pool</li>
        <li>GW_NAME. Enter the TUL ip of the VRSG node created on VSD / VSC</li>
        <li>PORT_NAME. Enter the physical NIC; use don VRSG node for trunk/ tagged external trafic</li>
        <li>UPLINK_SUBNET</li>
        <li>UPLINK_MASK</li>
        <li>VRSG_IP </li>
        <li>UPLINK_GW</li>
        <li>UPLINK_GW_MAC</li>
      </ul></li>
      <li>Execute the curl script
        <codeblock>bash +x vrsg_script_cloud2.sh</codeblock></li>
      <li>Use the VSD dashboard to confirm that uplink port appears in from monitoring tab under VRS-G node.</li>
    </ol>
  <section><title>Upload a Glance VMDK Image</title>
    <ol>
      <li>Launch the Horizon dashboard.</li> 
      <li>Go to <b>Admin -> Images</b> tab and upload the VMDK image.</li> 
      <li>Select vmdk format from drop down menu.</li>
      <li>Click <b>Create Image</b>.</li>
      <li>For the new image, click <b>Actions -> Edit</b> to update the metadata for the image.</li> 
      <li> Add <codeph>vmware_adaptertype</codeph> and <codeph>vmware_disktype</codeph> properties .</li>
    </ol>
    <p>Once the images are uploaded, you are ready to launch the instance with different flavors.</p>
    <p>Once the VM is launched, the progress can be seen from recent tasks in ESX cluster.</p>
      
      
    </section>



  <section id="next-step"> <title>Next Step</title>
<p>
        <xref href="carrier-grade-install-kvm-cloud-GA.dita">Deploying the KVM Region</xref>
      </p>
</section>
</body>
</topic>
