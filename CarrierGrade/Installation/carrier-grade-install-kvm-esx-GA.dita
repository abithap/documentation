<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic
  PUBLIC "-//OASIS//DTD DITA Topic//EN" "http://docs.oasis-open.org/dita/v1.1/OS/dtd/topic.dtd" ><topic xml:lang="en-us" id="topic10581">
<title>HP Helion <tm tmtype="reg">OpenStack</tm> Carrier Grade 1.1: Installing the KVM + ESX<tm
      tmtype="reg"/> Deployment</title>
<prolog>
  <metadata>
    <othermeta name="layout" content="default"/>
    <othermeta name="product-version" content="HP Helion Openstack Carreir Grade 1.1"/>
    <othermeta name="role" content="Storage Administrator"/>
    <othermeta name="role" content="Storage Architect"/>
    <othermeta name="role" content="Michael B"/>
    <othermeta name="product-version1" content="HP Helion Openstack Carreir Grade 1.1"/>
  </metadata>
</prolog>
<body>
  <p>The first phase of the HP Helion Openstack Carrier Grade installation involves creating a
      virtual machine for the Helion Lifecycle Management (HLM), deploying the HP Helion OpenStack
      cloud, installing DCN components, and installing the VMware ESX <tm tmtype="reg"/>compute
      proxy.</p>
    <p>The installation uses Ansible playbooks, which are files that contain scripts that execute
      required installation processes.</p>
<section id="prereqs">
  <p>Before starting the HP Helion OpenStack Carrier Grade installation, the VMware vSphere application and 
    HP Distributed Cloud Networking components must be fully installed and functioning:</p>
<ul>
<li>ESXi requirements: 
  <ul><li>	vSphere Cluster running ESXi 5.5u2 with vMotion enabled.</li> 
    <li>1 Virtual Machine Port Group with all Cloud Vlans associated to the Port Group.</li></ul></li>
  
  <li>VRS requirements <p>VRSvApp must be installed. Please Refer to the DCN VSP 3.0 R7 installation
            Guide for VRSvApp installation.</p></li>
</ul>  
  
</section>
<section id="prepare"> <title>Prepare the system for deployment</title>
<p>Use the following steps to prepare the server on which the HLM VM will be deployed (the HLM host):</p>
<ol>
        <li>Log into the HLM host using the default credentials</li>

  <li>Edit the <codeph>/root/infra-ansible-playbooks/group_vars/all</codeph> file for your
          environment. For information on each variable, refer to the comments in the file with each
          variable. <p>The <codeph>group_vars/all</codeph> file should appear similar to the
            following for a KVM + ESX
          deployment:</p><codeblock>
    ############################################# Variables for HLM  #################################################################
    #These variables are relevant in both All Virtual and BareMetal scenarios.
    
    #Set this to 1 if you will not deploy a dcn cloud via this HLM node and will deploy only ovs cloud
    ovs_cloud_only:  0
    
    #Set this to 'bm' if cloud is being deployed over baremetal.
    #Set this to 'av' if the cloud is all virtual
    cloud_type: 'bm'
    
    #These are hlm related variables that must be changed according to your Baremetal Env
    hlm_login_id:       root
    hlm_password:       cghelion
    
    #The following variables are for CLM network IP details for HLM
    hlm_clmstaticip:    10.20.3.100
    hlm_clmnetmask:     255.255.255.0
    hlm_clmgateway:     10.20.3.1
    
    #The variables starting with cobbler_ are inputs that are usually given to initcobbler.sh. Set accordingly.
    cobbler_pxestartip: 10.20.1.100
    cobbler_pxeendip:   10.20.1.200
    cobbler_pxestaticip: 10.20.1.51
    cobbler_pxenetmask: 255.255.255.0
    
    #Set the location of your images that will be used by libvirt
    imagelocation: /var/lib/libvirt/images
    
    #Set the location of your infra-ansible-playbooks
    ansible_dir: ~/infra-ansible-playbooks</codeblock></li>
  <li id="hosts">Check the hosts file <codeph>/root/infra-ansible-playbooks/hosts</codeph> file to
          enter the IP address of the vibr0 interface in the <codeph>hlm_kvm_host</codeph> field, as
          shown in the following example. </li>
</ol>
<codeblock>  [hlm_kvm_host]
  192.168.122.1 </codeblock>
</section>
<section id="deploy-the-hlm-and-vsd-vm">
      <title>Deploy the HLM Virtual Machine</title>
      <p>Use the following steps on the HLM host to deploy the HLM VM on the HLM Host using Ansible
        playbooks.</p>
      <ol>
        <li>Execute the following
          command:<codeblock>ansible-playbook -i hosts setup_hlm_onBM.yml</codeblock><!-- The command will do the following:<ul><li>Copy both installation files (tar balls) to the HLM host, decrypt, and extract the files.</li><li>Execute the <codeph>updatepackages</codeph> command.</li><li>Execute the <codeph>prepareenv</codeph> command.</li><li>Execute the <codeph>Init cobbler</codeph> command.</li><li>Execute the <codeph>Importiso</codeph> command.<p>You will see a message similar to the following image when the playbook is run successful:</p></li></ul>-->
            hlmcompleteionsnaphot<image href="../../media/CGH-ansible-playbook-run.png"
            id="image_bfp_pjm_vs">
            <alt>Sample Ansible Playbook run output</alt>
          </image></li>
        <li>Locate the IP address for the HLM VM in the
            <codeph>/root/infra-ansible-playbooks/group_vars/all</codeph> file of the HLM host under
            <codeph>hlm_clmstaticip</codeph> field. You will use this IP address to SSH into the HLM
          VM during the installation process.</li> 
      </ol>

</section>
  
<section id="vsd"> <title>Applying the VSD license</title>
  <p>During the <xref href="#topic10581/deploy-the-hlm-and-vsd-vm" format="dita"/> process, the
        Ansible playbook run creates a virtual machine to host the VSD node of DCN. You need to and
        apply the required license to use VSD.</p>
<p>Before you start, make sure the VSD node was installed by logging into the VSD VM using SSH and
        running the following command:</p>
<codeblock>service vsd status</codeblock>
<p>You should see the status as below from VSD VM.</p>
<p>
        <image href="../../media/CGH-install-virsh-vsd.png" width="400"/></p>
</section>

  <!--<section id="vsd-performance-workaround"> <title>VSD Performance workaround</title>
<p>By default VSD has only 8G memory. For better performance behavior, you can update the VSD memory to 16G.</p>
<ol>
<li>From HLM host, execute the following command to power down the VSD node:
          VM:<codeblock>virsh destroy vsd</codeblock></li>
<li>Execute the following command to edit the memory setting in the VSD XML
          file:<codeph>virsh edit vsd 

Current value 
    &lt;memory unit='KiB'&gt;8388608&lt;/memory&gt;
    &lt;currentMemory unit='KiB'&gt;8388608&lt;/currentMemory&gt;

Change to 
    &lt;memory unit='KiB'&gt;16777216&lt;/memory&gt;
    &lt;currentMemory unit='KiB'&gt;16777216&lt;/currentMemory&gt;
</codeph></li>
<li>Save the value<codeph>virsh save vsd
</codeph></li>
<li>Use the following command to start the
            VM:<codeph>virsh start vsd
</codeph><p>It can take 15 minutes or
            more to sync with NTP and to get all the other VSD services up.</p></li>
</ol>
</section>-->
<section id="launch-vsd-dashboard">
      <title>Apply the license</title>
      <p>On the HLM host:</p>
      <ol>
        <li>Using a browser, browse to the link in the confirmation email you received when you
          purchased DCN and obtain the license.</li>
        <li>After you have obtained the license, use a browser to navigate to the VSD dashboard
          using the VSD URI.</li>
        <li>In the login page, enter the default
          credentials:<codeblock>User Name: Csproot 
Password: csproot 
Org: csp 
VSD Server : auto 
</codeblock></li>
        <li>From VSD Dashboard, click the <b>Open VSP Configuration</b> tab on the top right corner of the
          dashboard.</li>
        <li>Click the <b>Licenses</b> tab and click <b>+</b>.</li>
        <li>Copy and paste your DCN license key into the dialog box.</li>

      </ol>
      <!--Follow the instructions in the pdf to get the license - <b>Get confirmation from Nayana</b><p><xref href="https://wiki.hpcloud.net/display/HCG/HP+Helion+OpenStack+Carrier+Grade+%28NFVi%29+Home#HPHelionOpenStackCarrierGrade%28NFVi%29Home-HPHelionOpenStackCarrierGradeDCN/Nuage" format="html" scope="external">https://wiki.hpcloud.net/display/HCG/HP+Helion+OpenStack+Carrier+Grade+%28NFVi%29+Home#HPHelionOpenStackCarrierGrade%28NFVi%29Home-HPHelionOpenStackCarrierGradeDCN/Nuage</xref></p>-->
    </section>
<section id="create-user-for-plugin-login"> <title>Create User for Plugin Login</title>
<p>You must create a user and add it to CMS Group.</p>
<ol>
<li>From VSD Dashboard, click the <b>Open VSP Configuration</b> tab on the top right corner of the
          dashboard.</li>
<li>Click the <b>CSP Users</b> tab and click <b>+</b>.</li>
<li>Create a user named <codeph>OSadmin</codeph> with the password <codeph>OSadmin</codeph>. The
          name and password are case-sensitive.  Do not copy and paste from here into the
          field.</li>
<li>Add the user to the <codeph>CMS Group</codeph>.</li>
</ol>
</section>
  <section><title>Install the DVswitch</title></section>
    <section>
          <p>Please Refer to the DCN VSP 3.0 R7 installation Guide for DVswitch installation.</p>
    </section>
  
  <section><title>Install the VRSVapp</title>
    <p>Please Refer to the DCN VSP 3.0 R7 installation Guide for VRSvApp installation.</p>
  </section>
  <section id="configure-a-json-file-for-installation"> <title>Deploy HP Helion OpenStack</title>
<p>Use the following steps on the HLM host to deploy the HP Helion OpenStack:</p>
<ol>
<li>Login to HLM VM.
            <codeblock>ssh &lt;HLM_VM_IP>
where: HLM_VM_IP is the CLM IP of the HLM VM. </codeblock><p>Use
            the default credentials: </p><p>
            <!--when to change?-->
            <codeblock>User Name: root
Password: cghelion</codeblock>
          </p></li>
    
<li>On the HLM VM, change to the home directory.<codeblock>cd ~</codeblock></li>
<li>Provision and configure your HP Helion
            OpenStack.<codeblock>hnewcloud  &lt;cloudname&gt; memphis</codeblock><p>Where:</p><ul>
            <li><codeph>&lt;cloudname&gt;</codeph> is the name of the cloud to create.</li>
            <li><codeph>memphis</codeph> is the name of the template to use. The installation kit
              includes a temlate called <codeph>memphis</codeph> designed to install HP Helion
              OpenStack.</li>
          </ul><p>The command creates the <codeph>&lt;cloudname&gt;</codeph> directory, which
            contains several JSON template files. </p></li></ol>
  </section>
  <section id="configure-node-provision-json">
      <title>Configure the node-provision JSON file</title>
      <p>Edit the <codeph>node-provision.json</codeph> file in the <codeph>&lt;cloudname></codeph>
        directory of the HLM VM. This template supplies input values to the
          <codeph>hprovision</codeph> script, later in the installation. </p>
        
      <p> Edit <codeph>node-provision.json</codeph> file to change only the following fields:</p> 
        <table>
        <tgroup cols="2">
          <colspec colname="col1" colsep="1" rowsep="1"/>
          <colspec colname="col2" colsep="1" rowsep="1"/>
          <thead>
            <row>
              <entry colsep="1" rowsep="1">Field</entry>
              <entry colsep="1" rowsep="1">Description</entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry>Pxe-mac-address</entry>
              <entry>MAC address of the interface you want to PXE boot onto. This is not same as iLO
                MAC address.</entry>
            </row>
            <row>
              <entry>pm_ip</entry>
              <entry>Power management IP (iLO ip)</entry>
            </row>
            <row>
              <entry>pm_user</entry>
              <entry>Power management user (iLO username)</entry>
            </row>
            <row>
              <entry>pm_pass</entry>
              <entry>Power management password (iLO password)</entry>
            </row>
            <row>
              <entry>failure_zone, vendor, model, os_partition_size, data_partition_size</entry>
              <entry>Enter the same value as for these fields an in the <codeph>nodes.json</codeph>
                file used during cloud deployment</entry>
            </row>
          </tbody>
        </tgroup>
      </table>
      <p>Also, remove the server assigned for second VRSG.</p>
    <codeblock>,
{
      "name": "vrg2",
      "pxe_mac_address": "d0:bf:9c:c0:ba:38",
      "pxe_interface": "auto",
      "pm_type": "ipmilan",
      "pm_ip": "10.1.3.56",
      "pm_user": "cghelion",
      "pm_pass": "cghelion#",
      "failure_zone": "fz2",
      "node_group": <b>"VRG-001-001",</b>
      "vendor": "HP",
      "model": "DL680",
      "os_partition_size": "20",
      "data_partition_size": "80"
      }    </codeblock>
    
  
    <p>To see a sample <codeph>node-provision.json</codeph> file, see <xref
        href="carrier-grade-install-pb-kvm-node-json.dita">Sample node-provision.json File for Installing the KVM + ESX Topology</xref>.</p>
    </section>
 <p> <b>Configure PXE boot</b> </p>
<p>After you edit the <codeph>node-provision.json</codeph> file, you must enable one-time PXE boot
        on the servers set the correct boot order. Execute the following on the HLM VM:</p>
<ol>
<li>Copy the <codeph>ilopxebootonce.py</codeph> from the
          <systemoutput>/root/cg-hlm/dev-tools/ilopxebootonce.py</systemoutput> to the
          <codeph>&lt;cloudname></codeph> directory where you have the
          <codeph>node-provision.json</codeph> file.</li>
<li>Execute the following script:
  <codeblock>python ilopxebootonce.py node-provision.json</codeblock></li>
</ol>
<p>After the script is run, the <codeph>Current One-Time Boot Option</codeph> is set to <codeph>Network Device 1</codeph> on all the servers listed in <codeph>node-provision.json</codeph> file.</p>
    <section id="configure-def-json"><title>Configure the definition.json file</title>
      <p>Edit the <codeph>definition.json</codeph> file in the <codeph>&lt;cloudname></codeph>
        directory of the HLM VM to define the number of ESX compute proxies required in your
        environment. You need one proxy per vCenter; set this value to the number of vCenters you
        have: <ol>
          <li>Set the number of compute systems to 1.
            <codeblock>"count": 1, //number of computes in the resource pool</codeblock></li>
        </ol></p></section>
  
  <section id="configure-env-json"><title>Configure the environment.json file</title>
    <p>Edit the <codeph>environment.json</codeph> file in the <codeph>&lt;cloudname></codeph>
        directory of the HLM VM to configure the VLANs and network addresses as appropriate for your
        environment. Set the following for the CLM (management), CAN (api), and BLS (blockstore)
        network.</p>
      <p>For each network
        provide:</p>
        <codeblock>{
"name": "management",
"type": "vlan",
"segment-id": "1551",
"network-address": {
  "cidr": "10.200.51.0/24",
  "start-address": "10.200.51.100",
  "gateway": "10.200.51.1"
  }
},        </codeblock>
          <p>To see a sample <codeph>environment.json</codeph> file, see <xref
          href="carrier-grade-install-pb-kvm-env-json.dita">Sample environment.json File for
          Installing the KVM + ESX Topology</xref>.</p></section>
  
  <section id="configure-ansible-json">
    <title>Configure the ansible.json file</title>
    <p>Edit the <codeph>ansible.json</codeph> file in the <codeph>&lt;cloudname>/vars</codeph> directory of the HLM
        VM.</p>
    <p>To see a sample <codeph>ansible.json</codeph> file,  see <xref
          href="carrier-grade-install-pb-kvm-ansible-json.dita">Sample ansible.json File for
          Installing the KVM + ESX Topology</xref>.</p>
  </section>
  
  <section id="configure-esx-json">
    <title>Configure the esx.json file</title>
    <p>Edit the <codeph>esx.json</codeph> file on the HLM VM. This file is called by the script that
        installs the HP Helion OpenStack cloud, later in the installation. </p>
    <p>To see a sample <codeph>esx.json</codeph> file, see <xref
          href="carrier-grade-install-pb-kvm-esx-json.dita">Sample esx.json File for Installing the
          KVM + ESX Topology</xref>.</p>
  </section>
  
  <section id="configure-ldap-json">
    <title>Configure the ldap.json file</title>
    <p>Edit the <codeph>ldap.json</codeph> file in the <codeph>&lt;cloudname>/vars</codeph>
        directory of the HLM VM.</p>
    <p>To see a sample <codeph>ldap.json</codeph> file, see <xref
          href="carrier-grade-install-pb-kvm-ldap-json.dita">Sample ldap.json File for Installing
          the KVM + ESX Topology</xref>. </p>
  </section>
  
  <section id="configure-wr-json">
    <title>Configure the wr.json file</title>
    <p>Edit the <codeph>wr.json</codeph> file on the HLM VM.</p>
    <p>To see a sample <codeph>wr.json</codeph> file, see <xref
          href="carrier-grade-install-pb-kvm-wr-json.dita">Sample wr.json File for Installing the
          KVM + ESX Topology</xref>. </p>
  
  </section>
  <section><title>Configure the dcn.json file</title>
    <p>Edit the <codeph>dcn.json</codeph> file in the
          <codeph>/opt/share/hlm/1/site/services/dcn.json</codeph> directory of the HLM VM to remove
          <b>both</b> instances of the following code:
        <codeblock>
      {
          "description": "Ingress 47 GRE",
          "firewall": {
            "direction": "in",
            "protocol": "gre"
            },
           "port": 47,
            "scope": "private"
      },
    </codeblock></p>
    </section>
      <section><title>Configure the ESX Compute Proxy</title>
    <p>The HP Helion OpenStack Carrier Grade vCenter ESX compute proxy (compute proxy) is a driver
      that enables the Compute service to communicate with a VMware vCenter server managing one or
      more ESX hosts. The HP Helion OpenStack Compute Service (Nova) requires this driver to
      interface with VMware ESX hypervisor APIs.</p>
    <p>For instructions to configure the ESX compute proxy, see <xref
          href="carrier-grade-install-esx-proxy.dita#topic10581">Deploy the ESX Compute
        Proxy</xref></p>
  </section>
  
<section id="create-a-new-cloud-template-and-bring-the-cloud-nodes-up">
      <title>Provision the cloud nodes and bring the cloud nodes up</title>
      <ol>
        <li>On the HLM host, use the following script to start the provisioning of the HP Helion
          OpenStack cloud:
            <codeblock>hprovision &lt;cloudname&gt;          </codeblock><p>Where:</p><ul>
            <li><codeph>&lt;cloudname&gt;</codeph> is the name of the cloud you created</li>
          </ul><p>This script will PXE boot the nodes specified in
              <codeph>node-provision.json</codeph> file. The script also tracks the PXE boot
            completion process and will create the <codeph>nodes.json</codeph> file in the
            directory. </p><p>You can log in to the iLO of each of the nodes to monitor the boot
            process.</p></li>
        <li>Make sure the nodes are booted up using iLO.</li>
        <li>Once the baremetal nodes are provisioned, make sure the <codeph>nodes.json</codeph> file
          is generated. The <codeph>nodes.json</codeph> file will have entries of 3 controllers, 2
          DCN Hosts, 1 VRS-G and the compute proxy node.</li>
        <li>Verify that each node in the <codeph>nodes.json</codeph> file is installed and active. <ol>
            <li>Ping proxy node from HLM VM with the IP using IP specified in
                <codeph>nodes.json</codeph> file.</li>
            <li>SSH to the compute proxy node from the HLM VM using IP specified in
                <codeph>nodes.json</codeph> file.</li>
          </ol></li>
        <li>Configure the VMware VMDK driver to enable management of the OpenStack Block Storage volumes
          on vCenter-managed data stores:<ol id="ul_btr_l52_xs">
            <li>Change to the <codeph>cinder/blocks</codeph> directory:
              <codeblock>cd ~/&lt;cloudname>/services/cinder/blocks</codeblock> Where &lt;cloudname>
              is the name you assigned to the cloud.</li>
            <li>Edit the <codeph>cinder_conf</codeph> to configure VMware VMDK:
              <codeblock>[DEFAULT]
enabled_backends=vmdk
...                                
[vmdk]
volume_backend_name = &lt;mybackendname3>
vmware_host_ip = &lt;hostip>
vmware_host_username = &lt;username>
vmware_host_password = &lt;password>
vmware_volume_folder = &lt;volumes_folder>
vmware_image_transfer_timeout_secs = 7200
vmware_task_poll_interval = 0.5
vmware_max_objects_retrieval = 100
volume_driver = cinder.volume.drivers.vmware.vmdk.VMwareVcVmdkDriver</codeblock></li>
          </ol></li>
        <li>After editing the JSON files, validate each JSON file to make sure there are no
          syntactical errors using the tool of your preference. For example, using the Python
          json.tool: <codeblock>python -m json.tool &lt;filename>.json</codeblock></li>
        <li>Once you have correctly edited all the json Cloud Model files, run the HP Helion
          OpenStack Configuration Processor:
            <codeblock>hcfgproc -d definition.json</codeblock><p>The HP Helion OpenStack
            Configuration Processor is a script, called <codeph>hcfgproc</codeph>, that is
            incorporated into the installation environment. </p></li>
        <li>Review the CloudDiagram, <codeph>hosts.hf</codeph>, and
            <codeph>net/interfaces.d/eth.cfg</codeph> files to make sure the network settings are
          correct.</li>
        <li>Initialize network interfaces on all the cloud nodes using the following command:
            <codeblock><codeph>hnetinit &lt;cloudname&gt;</codeph></codeblock><p>After this command
            completes, all cloud nodes and CLM network interfaces are configured.</p></li>
        <li>Use the following command to deploy the cloud:
            <codeblock><codeph>hdeploy &lt;cloudname&gt;</codeph></codeblock><p>When this command is
            complete the non-KVM cloud will be installed.</p></li>
      </ol>
</section>
  <section>        
          <title>  Apply the Horizon interface patch:</title> 
          <ol><li>Locate the hcg_1.1.0_horizon_patch1.tar file you downloaded to the HLM host and extract the files.</li> 
           <li>Copy user.py file to first controller node in the
              /opt/stack/venvs/horizon/lib/python2.7/site-packages/openstack_auth/ directory.</li>
            <li>Use the following command to restart the Horizon service on that node:
              <codeblock>sudo service apache2 restart</codeblock></li>
             <li>Repeat above two steps on the other two controller nodes in the non-KVM region.</li> 
          </ol>
  </section>

  <section><title>Create external network and subnet</title>
    <ol>
      <li>Obtain the static IP route for the external network/FIP EXT Uplink Subnet:
          <codeblock>        #
        interface Vlan-interface 1553
        description NFV TestBed - EXT UPLINK SUBNET
        ip address 10.200.70.1 255.255.255.192
        #
        .
        .
        .
        IP Packet Frame Type: PKTFMT_ETHNT_2,  Hardware Address: abcd-1234-lmno
        .
        .
        .
        Destination/Mask    Proto  Pre  Cost         NextHop         Interface
        
        10.200.70.0/26      Direct 0    0            10.200.70.1     Vlan1553
        10.200.70.1/32      Direct 0    0            127.0.0.1       InLoop0
        10.200.70.64/26     Static 60   0            10.200.70.2     Vlan1553
        10.200.70.128/26    Static 60   0            10.200.70.2     Vlan1553
        10.200.70.192/26    Static 60   0            10.200.70.2     Vlan1553
      </codeblock></li>
        <li>From the HLM VM, SSH into any controller node and source the stackrc file, using the
          cghelion username and password: <codeblock>cd ~/
source stackrc</codeblock></li>  
      <li>Execute the following command on any controller node to create the external network:
            <codeblock>neutron net-create &lt;external-network-name> --router:external</codeblock><p>where
              <codeph>&lt;external-network-name></codeph> is a descriptive name for the network.</p></li>
      <li>Execute the following command to create a subnet for the external network:
            <codeblock>neutron subnet-create &lt;external-network-name> &lt;subnet-ip> --name &lt;name></codeblock><p>where:
            &lt;subnet-ip> is the IP address range to assign and &lt;name> is a descriptive
            name.</p><p>For
            example:</p><codeblock>neutron subnet-create ext-net 10.200.70.64/26 --name extsub1 </codeblock><p>where
            &lt;external-network-name> is a descriptive name for the network.</p></li>
      <li>Use the following command to create a virtual router:
            <codeblock>neutron router-create &lt;name></codeblock><p>where: &lt;name> is a
            descriptive name.</p></li></ol>
  </section>
  <section id="vrsg-config"><title>Configure the VRS-G </title>
      <p>The HP Helion OpenStack Carrier Grade installation creates the required VRS-G gateway. You
        need to use the HP DCN Virtualized Services Directory Architect (VSD) interface to add the
        gateway to VSD and configure the <codeph>eth0</codeph> port.</p>
    
    <ol><li>Launch the VSD dashboard:<ul id="ul_ylj_thz_xs">
      <li>Using Chrome or Safari, navigate to <codeph>https://&lt;VSD address>:8443/</codeph> and enter user
        name, password and organization.<p><image href="../../media/CGH-install-vsd-login.png" id="image_dls_phz_xs"
          width="300">
          <alt>VSD login screen</alt>
        </image></p></li>
      <li>Click the arrow icon to log in.</li>
          </ul></li>
      <li>Click the <b>Gateways</b> tab. The new gateway appears in the <b>Pending Gateways</b> list
          on the left.</li>
      <li>Click the blue arrow next to the new gateway to manage that gateway.<p><image
              href="../../media/CGH-install-nuage-gateway.png" id="image_rgy_5fz_xs" width="500">
              <alt>Discover the Gateway</alt>
            </image></p></li>
      <li>Click <b>Organization Profile</b> in the ribbon to add permissions to the Gateway.</li>
        <li>Add a port for the gateway by clicking <b>Create a Port</b> in the second panel from the
          left.</li>
        <li>Enter the information as required, specifying  eth0 in the Physical Name field  and
          entering the VLAN ID [EXT Net] for the VLAN range.<p><image
              href="../../media/CGH-install-gateway-port.png" id="image_w3p_sgz_xs" width="250">
              <alt>Gatewy Port</alt>
          </image></p></li>
      <li>Click <b>Create</b>.</li></ol>
          <p>For more information on performing these tasks, refer to the DCN
            documentation </p>
    
  <p><b>Note:</b> To create a redundant grouping of the VRS-G, refer to the DCN documentation.</p>
</section>
  <section><title>Retrieving the Floating IP Name from VSD for external Network</title>
    <ol><li>Using the VSD. dashboard</li>
        <li>Click <b>Open Data Center Configuration</b>, then <b>Shared Network</b> tab. <image
            href="../../media/CGH-install-vsd-fip.png" id="image_okn_sjz_xs">
            <alt>Gatewy Port</alt>
          </image></li>
        <li>For the Floating IP subnet, right-click and select <b>Edit</b> to get the name. <p>This
            is the same name you assigned when you created the subnet. Make note of this name, as
            you will use it during the installation.</p></li>
      <li> You will need the Floating IP subnet name during the installation when you create and
          edit a CuRL script.</li>
      </ol>
  </section>
    <section><title>Add VLAN to VRSG network device </title>
      <ol><li>Based on your VLAN ID, run the below command on VRS-G node:<p> For example:
              <codeph>nuage-vlan-config mod eth0 Access 1500-2000</codeph></p></li>
        
        <li>Add the VLAN ID [EXT Net] on the VSD dashboard for eth0</li>
      </ol>
  </section>
    <section><title>Run the curl script</title></section>
    <ol>
      <li>From the KVM host, copy the following code to create a cURL script.</li>
      <li>Modify the cURL script for your environment:
        <codeblock>
            ##########################################
            #!/bin/bash
            set -x
            #
            # Starting with VSP 3.0R2, there is an officially supported way to enable FIP using
            # a VSG or VRS-G uplink port
            #
            
            # Parameters
            VLAN="200"
            VSD_IP="10.x.x.x"
            FIP_NAME="eec14785-be36-4975-9ef8-2bf7edc5590e" # unique name show in VSD created for external network floating ip pool
            GW_NAME="10.20.6.106" # use the ip of the VRSG node created on VSD / VSC
            PORT_NAME="eth0" # pyshical nic use don VRSG node for trunk/ tagged external trafic
            
            #
            # IANA has reserved 192.0.0.0/29 for DS-lite transition
            #
            UPLINK_SUBNET="10.20.11.0"
            UPLINK_MASK="255.255.255.192"
            VRSG_IP="10.20.11.1"  # Modified :P
            UPLINK_GW="10.20.11.2"                         # ROUTER IP ON SWITCH main switch 10.1.86.21
            UPLINK_GW_MAC="bc:bc:bc:bc:bc:bc"       # VLAN 1209 MAC address not VRSG is from main switch 10.1.86.21
            
            if [ $# -eq 1 ]; then # remote install
            echo "Performing remote install to root@'$1' (requires PermitRootLogin=yes in sshd config)..."
            ssh root@$1 'bash -s ' &lt; $0
            exit 0
            elif [ $# -ne 0  ]; then
            cat &lt;&lt;END
              Usage (as root) $0 [remote IP]
              END
              exit -1
              fi
              
              # Install required software packages, if not already
              if [ -e /usr/bin/yum ]; then
              [[ `which jq` != "" ]] || yum install -y jq
              QEMU_KVM="/usr/libexec/qemu-kvm"
              QEMU_USR="qemu:qemu"
              LIBVIRTD="libvirtd"
              else
              [[ `which jq` != "" ]] || apt-get install -y jq
              QEMU_KVM="/usr/bin/kvm"
              QEMU_USR="libvirt-qemu:kvm"
              LIBVIRTD="libvirt-bin"
              fi
              
              # Determine Domain name for the Floating IP pool ( based on pool name )
              APIKEY=`curl -ks -H "X-Nuage-Organization: csp" -H "Content-Type: application/json" -H "Authorization: XREST Y3Nwcm9vdDpjc3Byb290" https://$VSD_IP:8443/nuage/api/v3_0/me | jq -r '.[0].APIKey'`
              TOKEN=`echo -n "csproot:$APIKEY" | base64`
              ZONE_ID=`curl -ks -H "X-Nuage-Organization: CSP" -H "X-Nuage-Filter: name=='$FIP_NAME'" -H "Content-Type: application/json" -H "Authorization: XREST $TOKEN" \
              https://$VSD_IP:8443/nuage/api/v3_0/sharednetworkresources | jq -r '.[0].parentID'`
              if [ "$ZONE_ID" == "" ]; then
              echo "Error: Floating IP pool named '$FIP_NAME' not found"
              exit 1
              fi
              
              # Lookup VLAN to use
              GW_ID=`curl -ks -H "X-Nuage-Filter: name=='$GW_NAME'" -H "X-Nuage-Organization: CSP" -H "Content-Type: application/json" -H "Authorization: XREST $TOKEN" \
              https://$VSD_IP:8443/nuage/api/v3_0/gateways | jq -r '.[0].ID'`
              PORT_ID=`curl -ks -H "X-Nuage-Filter: name=='$PORT_NAME'" -H "X-Nuage-Organization: CSP" -H "Content-Type: application/json" -H "Authorization: XREST $TOKEN" \
              https://$VSD_IP:8443/nuage/api/v3_0/gateways/$GW_ID/ports | jq -r '.[0].ID'`
              VLAN_ID=`curl -ks -H "X-Nuage-Filter: value==$VLAN" -H "X-Nuage-Organization: CSP" -H "Content-Type: application/json" -H "Authorization: XREST $TOKEN" \
              https://$VSD_IP:8443/nuage/api/v3_0/ports/$PORT_ID/vlans | jq -r '.[0].ID'`
              if [ "$VLAN_ID" == "" ]; then
              echo "Error: VLAN on gateway '$GW_NAME' with port '$PORT_NAME' and value '$VLAN' not found"
              exit 1
              fi
              echo "VLAN $VLAN ID: $VLAN_ID"
              # Get/Create uplink subnet
              SUBNET_ID=`curl -ks -H "X-Nuage-Filter: type=='UPLINK_SUBNET'" -H "X-Nuage-Organization: CSP" -H "Content-Type: application/json" \
              -H "Authorization: XREST $TOKEN" https://$VSD_IP:8443/nuage/api/v3_0/sharednetworkresources | jq -r '.[0].ID'`
              #if [ "$SUBNET_ID" == "" ]; then
              echo "Creating new FIP uplink subnet in ZONE $ZONE_ID"
              curl -ks -H "X-Nuage-Organization: CSP" -H "Content-Type: application/json" -H "Authorization: XREST $TOKEN" \
              https://$VSD_IP:8443/nuage/api/v3_0/sharednetworkresources -d "{ \
              \"name\": \"FIP uplink subnet\", \
              \"description\": \"uplink subnet\", \
              \"address\": \"$UPLINK_SUBNET\", \
              \"netmask\": \"$UPLINK_MASK\", \
              \"gateway\": \"$VRSG_IP\", \
              \"type\": \"UPLINK_SUBNET\", \
              \"uplinkInterfaceIP\" : \"$UPLINK_GW\", \
              \"uplinkInterfaceMAC\" : \"$UPLINK_GW_MAC\", \
              \"sharedResourceParentID\" : \"$ZONE_ID\", \
              \"uplinkGWVlanAttachmentID\" : \"$VLAN_ID\", \
              \"uplinkVPortName\" : \"uplink vport1\" \
              }"
              #else
              #echo "FIP uplink subnet already exists: $SUBNET_ID"
              #fi
              
              #############################################################
            </codeblock></li>
      <li>Execute the curl script <codeblock>bash +x &lt;script_name>.sh</codeblock></li>
      <li>Use the VSD dashboard to confirm that uplink port appears in the Monitoring tab under
        VRS-G node. <b><i>image</i></b></li>
    </ol>

<section><title>Enable GRE in iptables</title>
<!-- CG-1265 -->
  <p>If an environment uses more than one VRS-G node, you must enable Generic Routing Encapsulation (GRE) in the IP tables.</p>
  <ol><li>On each VRS-G node, navigate to the <codeph>/etc/ufw/before.rules</codeph> file.</li>
    <li>Add the following lines to the file, before the <codeph>COMMIT</codeph> line:
          <codeblock>-A ufw-before-input -p 47 -j ACCEPT</codeblock></li>
    <li>Save and close the file.</li>
  </ol>
</section>

<section><title>Post-Installation Steps </title><p>After the installation of the KVM + ESX
        deployment is complete, perform the steps in this section.</p>
      <b>Verify the VSC VMs</b>
      <p>Use the following steps to verify that the VSC VMs are installed and are operational </p><ol>
        <li>SSH to your VSC VM from HLM Host using the DCM IP. The default username and password:
            <codeph>admin/admin</codeph></li>
        <li>Execute the following command: <codeblock>admin display-config</codeblock></li>
        <li>Execute the following commands to verify the VMs are active:
          <codeblock>show vswitch-controller vsd
          show vswitch-controller xmpp-server
          ping router "management" &lt;vsd IP or domain name>  </codeblock></li>
      </ol><p><b>Verify the VRS-G Node</b>
      </p><p>The installation creates a VRS-G node as part of the cloud deployment. Use the
        following command to verify that the VRS-G is active:
        </p><codeblock>show vswitch-controller vswitches  </codeblock><p>The output for the command
        should appear similar to the following image:</p>
      <image href="../../media/CGH-install-verify-vrsg.png" id="image_x3t_jvp_vs"/>
      <p>Verify that the domain name and DNS server are listed in the
          <codeph>/etc/resolv.conf</codeph> file on all the cloud nodes </p><codeblock>cat etc/resolv.conf          </codeblock>
      <image href="../../media/CGH-install-resolv-conf.jpg" id="image_idc_5wp_vs">
        <alt>Verify VRS-G </alt>
      </image>
</section>
  <section><title>Upload a Glance VMDK Image</title>
  <ol>
        <li>Launch the Horizon dashboard.</li>
    <li>Click the <b>Images</b> link on the <b>Admin</b>upload the VMDK image.</li>
    <li>In the <b>Images</b> screen, click <b>Create Image</b>.</li> <!-- Where is this file??? -->
    <li>In the <b>Create an Image</b> screen, fill out the fields as appropriate:
      <ol><li>From the <b>Image Source</b> list, select <b>Image File</b>.</li>
        <li>From the <b>Image File</b> list that appears, navigate to the VMDK file.</li>
        <li>From the <b>Format</b> list, select <codeph>VMDK</codeph>.</li></ol></li>
      <li>Click <b>Create Image</b>.</li>
      <li>For the new image, click <b>Actions -> Edit</b> to update the metadata for the
          image.</li>
        <li> Add <codeph>vmware_adaptertype</codeph> and <codeph>vmware_disktype</codeph> properties
          . </li>
      </ol>
    
    <p>Once the images are uploaded, you are ready to launch the instance with different
        flavors. </p>
    </section>


  <section id="next-step"> <title>Next Step</title>
<p>
        <xref href="carrier-grade-install-kvm-cloud-GA.dita">Deploying the KVM Region</xref>
      </p>
</section>
</body>
</topic>
