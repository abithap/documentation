<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE task-wr PUBLIC "-//WindRiver.com//DTD DITA 1.2 Wind River General Task//EN" "task-wr.dtd">
<task-wr id="jow1429544935360" xml:lang="en-us">
<!-- Modification History
        
-->
    <title ixia_locid="356">Specifying Dedicated CPUs for a VM</title>
    <shortdesc ixia_locid="357">You can specify the use of dedicated CPUs for a VM using an extra
        specification.</shortdesc>
    <prolog>
        <author ixia_locid="358">Jim Owens</author>
    </prolog>
    <taskbody>
        <context id="context_N1001F_N1001C_N10001" ixia_locid="359">
            <p ixia_locid="204">When a virtual machine is launched, its virtual CPUs use resources
                from a shared pool of cores<!-- (physical cores or logical processors)--> on the
                target compute node. Each virtual CPU is scheduled for processing as an independent
                thread. The <option ixia_locid="206">CPU Policy</option> extra specification affects
                two aspects of the scheduling process: the CPU affinity mask, and the CPU
                over-commit limit. </p>
            <p ixia_locid="205">If the <option ixia_locid="207">CPU Policy</option> is set to
                    <nameliteral ixia_locid="208">Dedicated</nameliteral>, each virtual CPU thread
                is scheduled to run on a single core exclusively. The cores are removed from the
                shared resource pool, and each virtual CPU is scheduled to run as a dedicated thread
                on its corresponding target core. The cores are returned to the shared pool upon
                termination of the virtual machine.</p>
            <p ixia_locid="189">Selecting <nameliteral ixia_locid="209">Dedicated</nameliteral>
                effectively affines each virtual CPU thread to a dedicated core, and ensures that
                the core is not shared with other virtual CPU threads.
                <!--When launching virtual machines on a HT-enabled compute node, this option affines each virtual CPU thread to a logical processor instead; therefore, as many threads as logical processors are available can be scheduled to run on the same core. See <cite ixia_locid="185">HT-Enabled Compute Nodes</cite> in <xref href="jow1404333563170.xml" ixia_locid="187"/> for further information.-->
            </p>
            <p ixia_locid="249">The <nameliteral ixia_locid="211">Dedicated</nameliteral> setting is
                recommended for Carrier-Grade requirements to prevent over-commitment of host
                resources, and to minimize the impact that scheduling events may have on the latency
                and throughput responses of the guest kernel.</p>
            <p ixia_locid="251">If the <option ixia_locid="212">CPU Policy</option> is set to
                    <nameliteral ixia_locid="213">Shared</nameliteral>, the virtual CPU threads are
                scheduled to run on any available core on the compute node. Also, they can be moved
                within the compute node from one core to another at any time. In a sense, the
                virtual CPU threads can be considered to be floating threads. This is the CPU
                affinity mask portion of the scheduling action.</p>
            <p ixia_locid="253">Additionally, when <nameliteral ixia_locid="216">Shared</nameliteral> is selected, the virtual CPU threads can share the core
                with other threads from the same virtual machine or other virtual machines. The
                default OpenStack virtual CPU over-commit ratio is 16:1, which means that up to 16
                virtual CPU threads can share a single core. Over-committing of virtual CPUs is
                highly discouraged in Carrier-Grade operations.</p>
            <p ixia_locid="17">To add this extra spec to a flavor using the web administration
                interface, use the <uicontrol ixia_locid="314">CPU Policy</uicontrol> selection in
                the <uicontrol ixia_locid="316">Create Flavor Extra Spec</uicontrol> drop-down menu.
                To access this menu, see <xref href="jow1429538642910.xml" ixia_locid="318"/>.</p>
            <p ixia_locid="308">To add the extra spec using the CLI, use the following command:</p>
            <codeblock ixia_locid="315"><systemoutput ixia_locid="322">~(keystone_admin)$ </systemoutput><userinput ixia_locid="329">nova flavor-key <varname ixia_locid="336">flavor_name</varname> set hw:cpu_policy=<varname ixia_locid="347">policy</varname></userinput></codeblock>
            <p ixia_locid="231">where <varname ixia_locid="232">policy</varname> is either <option ixia_locid="354">dedicated</option> or <option ixia_locid="355">shared</option>.</p>
            <note id="note_N10271_N101E7_N101DE_N101C7_N1014F_N1001F_N10001" ixia_locid="235">
                <p ixia_locid="217">Using a single compute node for both shared and dedicated
                    threads is not recommended. For clusters that must support threads of both
                    types, use a host aggregate for each
                    type.<!-- JO 2015-03-13 Chris F suggests adding "in combination with the AggregateInstanceExtraSpecsFilter scheduler filter." -->
                    For more information, see <xref href="jow1426182913166.xml" ixia_locid="218"/>.</p>
            </note>
        </context>
    </taskbody>
</task-wr>