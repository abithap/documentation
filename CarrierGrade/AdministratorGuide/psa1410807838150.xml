<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE task-wr PUBLIC "-//WindRiver.com//DTD DITA 1.2 Wind River General Task//EN" "task-wr.dtd">
<task-wr domains="(topic task task-wr)                            (topic hi-d)                            (topic pr-d)                            (topic sw-d)                            (topic ui-d)                            (topic wr-sw-d)                            (topic xml-d)   " id="psa1410807838150" xml:lang="en-us">
<!-- Modification History
        
-->
    <title >Restore Procedure</title>
    <shortdesc >You can restore a HP Helion OpenStack Carrier Grade cluster from
        available system data backup files to bring it back to the operational state it was when the
        backup procedure took place.</shortdesc>
    <prolog>
        <author >Pedro Sanchez</author>
    </prolog>
    <taskbody>
        <context id="context_N1001F_N1001C_N10001" >
            <p >Restoring a HP Helion OpenStack Carrier Grade cluster from a set of
                backup files is done by restoring one host at a time, starting with the controllers,
                then  the compute nodes.</p>
        </context>
        <prereq id="prereq_N1002D_N1001F_N10001" >
            <p >Before you start the restore procedure you must ensure the following conditions are
                in place:</p>
            <ul id="ul_rfq_qfg_mp">
                <li >
                    <p >All cluster hosts must be powered down, just as if they were
                        going to be initialized anew.</p>
                </li>
                <li >
                    <p >All backup files are accessible from a USB flash drive
                        locally attached to the controller where the restore operation takes place
                            (<codeph >controller-0</codeph>).</p>
                </li>
                <li >
                    <p >You have the HP Helion OpenStack Carrier Grade installation image available on
                        a USB flash drive, just as when the software was installed the first time.
                        It is mandatory that you use the exact same version of the software used
                        during the original installation, otherwise the restore procedure will
                        fail.</p>
                </li>
                <li >
                    <p >The restore procedure requires all hosts but <codeph >controller-0</codeph> to boot over the internal
                        management network using the PXE protocol, just as it was done during the
                        initial software installation. Ideally, you cleaned up all hard drives in
                        the cluster, and the old boot images are no longer present; the hosts then
                        default to boot from the network when powered on. If this is not the case,
                        you must configure each host manually for network booting right after this
                        exercise asks you to power them on.</p>
                </li>
            </ul>
        </prereq>
        <steps>
            <step id="step_N1002B_N10028_N1001C_N10001" >
                <cmd >Install the HP Helion OpenStack Carrier Grade software on
                        <codeph >controller-0</codeph> from the USB flash drive.</cmd>
                <info >
                    <p >Refer to the <cite >HP Helion OpenStack Carrier Grade Software Installation Guide</cite> for
                        details.</p>
                </info>
                <stepresult >
                    <p >When the software installation is complete, you should be
                        able to log in using the host's console and the web administration
                        interface.</p>
                </stepresult>
            </step>
            <step id="step_N10085_N1005F_N1001F_N10001" >
                <cmd >Log in to the console as user <codeph >wrsroot</codeph> with password
                        <codeph >wrsroot</codeph>.</cmd>
            </step>
            <step id="step_N10096_N1005F_N1001F_N10001" >
                <cmd >Ensure the backup files are available to the controller.</cmd>
                <info >
                    <p >Plug the USB flash drive containing the backup files into the
                        system. The USB flash drive is mounted automatically. Use the command
                            <cmdname >df</cmdname> to list the mount points.</p>
                    <p >The following steps assume that the backup files are
                        available from a USB flash drive mounted in <filepath >/media/wrsroot</filepath> in the directory <filepath >backups</filepath>.</p>
                </info>
            </step>
            <step id="step_N100CB_N1005F_N1001F_N10001" >
                <cmd >Update the controller's software to the previous patching
                    level.</cmd>
                <info >
                    <p >When restoring the system configuration, the current
                        software version on the controller, at this time, the original software
                        version from the ISO image, is compared against the version available in the
                        backup files. They differ if the latter includes patches applied to the
                        controller's software at the time the backup files were created. If
                        different, the restore process automatically applies the patches and forces
                        an additional reboot of the controller to make them effective.</p>
                    <p >The following is the command output if patching is
                        necessary:</p>
                </info>
                <stepxmp >
                    <codeblock ><systemoutput >$ </systemoutput><userinput >sudo config_controller --restore-system \
/media/wrsroot/backups/backup_20140918_system.tgz</userinput>
<systemoutput >Restoring system (this will take several minutes):
Step  4 of 19 [#########                                    ] [21%]
This controller has been patched. A reboot is required.
After the reboot is complete, re-execute the restore command.
Enter 'reboot' to reboot controller:</systemoutput></codeblock>
                    <p >You must enter <codeph >reboot</codeph> at the prompt as requested. Once the controller is
                        back, log in as user <codeph >wrsroot</codeph> as
                        before to continue.</p>
                </stepxmp>
                <stepresult >
                    <p >After the reboot, you can verify that the patches were applied, as
                        illustrated in the following example:</p>
                    <codeblock ><systemoutput >$ </systemoutput><userinput >sudo wrs-patch query</userinput>
<systemoutput >        Patch ID          Repo State  Patch State
========================  ==========  ===========
COMPUTECONFIG             Available       n/a
LIBCUNIT_CONTROLLER_ONLY   Applied        n/a</systemoutput></codeblock>
                </stepresult>
            </step>
            <step id="step_N10125_N1006E_N1001F_N10001" >
                <cmd >Restore the system configuration.</cmd>
                <info >
                    <p >The controller's software is up to date now, at the same
                        stage it was when the backup operation was executed. The restore procedure
                        can be invoked again, and should run without interruptions.</p>
                </info>
                <stepxmp >
                    <codeblock ><systemoutput >$ </systemoutput><userinput >sudo config_controller --restore-system \
/media/wrsroot/backups/backup_20140918_system.tgz</userinput>
<systemoutput >Restoring system (this will take several minutes):
Step 19 of 19 [##########################] [100%]
Restoring node states (this will take several minutes):

Locking nodes:Step 10 of 10 [#############################################] [100%]
Powering-off nodes:
Step 10 of 10 [#############################################] [100%]

System restore complete </systemoutput></codeblock>
                </stepxmp>
            </step>
            <step id="step_N100ED_N1005F_N1001F_N10001" >
                <cmd >Authenticate to the system as Keystone user <codeph >admin</codeph>.</cmd>
                <info >
                    <p >Source the <codeph >admin</codeph>
                        user environment as follows:</p>
                </info>
                <stepxmp >
                    <codeblock ><systemoutput >$ </systemoutput><userinput >cd; source /etc/nova/openrc</userinput></codeblock>
                </stepxmp>
            </step>
            <step id="step_N10113_N1005F_N1001F_N10001" >
                <cmd >Restore the system images.</cmd>
                <stepxmp >
                    <codeblock ><systemoutput >~(keystone_admin)$ </systemoutput><userinput >sudo config_controller --restore-images \
/media/wrsroot/backups/backup_20140918_images.tgz</userinput>
<systemoutput >Step 2 of 2 [#############################################] [100%]

Images restore complete</systemoutput></codeblock>
                </stepxmp>
                <info >
                    <p >This step assumes that the backup file resides on the
                        attached USB drive. If instead you copied the image backup file to the
                            <filepath >/opt/backups</filepath> directory, for example using the
                            <cmdname >scp</cmdname> command, you can remove it now.</p>
                </info>
            </step>
            <step id="step_N10135_N1005F_N1001F_N10001" >
                <cmd >Restore <codeph >controller-1</codeph>.</cmd>
                <substeps id="substeps_wt4_5dh_mp">
                    <substep >
                        <cmd >List the current state of the hosts.</cmd>
                        <stepxmp >
                            <codeblock ><systemoutput >~(keystone_admin)$ </systemoutput><userinput >system host-list</userinput>
<systemoutput >+----+--------------+-------------+----------------+-------------+--------------+
| id | hostname     | personality | administrative | operational | availability |
+----+--------------+-------------+----------------+-------------+--------------+
| 1  | controller-0 | controller  | unlocked       | enabled     | available    |
| 2  | controller-1 | controller  | locked         | disabled    | offline      |
| 3  | compute-0    | compute     | locked         | disabled    | offline      |
+----+--------------+-------------+----------------+-------------+--------------+</systemoutput></codeblock>
                        </stepxmp>
                    </substep>
                    <substep >
                        <cmd >Power on the host.</cmd>
                        <info >
                            <p >Remember to ensure that the host boots from the
                                network and not from an old disk image that might still be present
                                on its hard drive.</p>
                        </info>
                    </substep>
                    <substep >
                        <cmd >Unlock the host.</cmd>
                        <stepxmp >
                            <codeblock ><systemoutput >~(keystone_admin)$ </systemoutput><userinput >system host-unlock 2</userinput>
<systemoutput >+-----------------+--------------------------------------+
| Property        | Value                                |
+-----------------+--------------------------------------+
| action          | none                                 |
| administrative  | locked                               |
| availability    | online                               |
| ...             | ...                                  |
| uuid            | 5fc4904a-d7f0-42f0-991d-0c00b4b74ed0 |
+-----------------+--------------------------------------+</systemoutput></codeblock>
                        </stepxmp>
                    </substep>
                    <substep >
                        <cmd >Verify the new state of the hosts.</cmd>
                        <stepxmp >
                            <codeblock ><systemoutput >~(keystone_admin)$ </systemoutput><userinput >system host-list</userinput>
<systemoutput >+----+--------------+-------------+----------------+-------------+--------------+
| id | hostname     | personality | administrative | operational | availability |
+----+--------------+-------------+----------------+-------------+--------------+
| 1  | controller-0 | controller  | unlocked       | enabled     | available    |
| 2  | controller-1 | controller  | unlocked       | enabled     | available    |
| 3  | compute-0    | compute     | locked         | disabled    | offline      |
+----+--------------+-------------+----------------+-------------+--------------+</systemoutput></codeblock>
                        </stepxmp>
                    </substep>
                </substeps>
                <stepresult >
                    <p >The unlocking operation forces a reboot of the host, which
                        is then initialized with the corresponding image available from the system
                        backup.</p>
                    <p >You must wait for the host to become enabled and available
                        before proceeding to the next step.</p>
                </stepresult>
            </step>
            <step id="step_N101ED_N1005F_N1001F_N10001" >
                <cmd >Restore the compute nodes, one at a time.</cmd>
                <info >
                    <p >You restore these hosts following the same procedure used to
                        restore <codeph >controller-1</codeph>.</p>
                </info>
                <stepresult >
                    <p >The state of the hosts when the restore operation is
                        complete is as follows:</p>
                    <codeblock ><systemoutput >~(keystone_admin)$ </systemoutput><userinput >system host-list</userinput>
<systemoutput >+----+--------------+-------------+----------------+-------------+--------------+
| id | hostname     | personality | administrative | operational | availability |
+----+--------------+-------------+----------------+-------------+--------------+
| 1  | controller-0 | controller  | unlocked       | enabled     | available    |
| 2  | controller-1 | controller  | unlocked       | enabled     | available    |
| 3  | compute-0    | compute     | unlocked       | enabledd    | available    |
+----+--------------+-------------+----------------+-------------+--------------+</systemoutput></codeblock>
                    <p >As each compute node is restored, the original instances at the time the
                        backups were done are started automatically.</p>
                </stepresult>
            </step>
        </steps>
        <result id="result_N10213_N1001F_N10001" >
            <p >The system is fully restored. The state of the system, including
                virtual machines, is identical to the state the cluster was in when the backup
                procedure took place.</p>
            <p >Remember however, that passwords for local user accounts must be
                restored manually since they are not included as part of the backup and restore
                procedures. See <xref href="jow1404333563834.xml" /> for additional
                information.</p>
        </result>
    </taskbody>
</task-wr>
