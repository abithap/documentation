<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE concept-wr PUBLIC "-//WindRiver.com//DTD DITA 1.2 Wind River Concept//EN" "concept-wr.dtd">
<concept-wr domains="(topic concept concept-wr)                            (topic hi-d)                            (topic indexing-d)                            (topic pr-d)                            (topic sw-d)                            (topic ui-d)                            (topic wr-sw-d)                            (topic xml-d)   " id="jow1404333800588" xml:lang="en-us" xmlns:ditaarch="http://dita.oasis-open.org/architecture/2005/">
    <!-- Modification History
   -->
    <title id="1">The Virtual Machine Scheduler</title>
    <shortdesc id="2">The virtual machine scheduler in the HP Helion OpenStack Carrier Grade is a modified
        version of the OpenStack Nova scheduler. It allows for better placement of virtual machines
        in order to exploit hardware and network resources, and for a better distribution of the
        system workload overall.</shortdesc>
    <prolog>
        <author id="3">Pedro Sanchez</author>
    </prolog>
    <conbody>
        <p id="5">In addition to the scheduler actions taken based upon the virtual machine
            flavor in use, as discussed in <xref href="jow1404333663095.xml" id="6"/>, the
            following are enhancements integrated into the HP Helion OpenStack Carrier Grade Nova scheduler:</p>
        <dl>
            <dlentry id="7">
                <dt id="8">Memory over-commit policy</dt>
                <dd id="9">
                    <p id="10">There is no support for memory over-commit when scheduling
                        virtual machines. When launching a virtual machine, the Nova scheduler
                        reserves the requested memory block from the system in its entirety. The
                        launch operation fails if not enough memory can be allocated from the
                        compute cluster.</p>
                </dd>
            </dlentry>
            <dlentry id="11">
                <dt id="12">Network load balancing across processor nodes</dt>
                <dd id="13">
                    <p id="14">When scheduling threads for new virtual CPUs, the Nova
                        scheduler selects the target core taking into account the load average of
                        the running AVS cores on each processor node. In a common scenario, the AVS
                        is deployed on two cores, not necessarily part of the same processor. When a
                        new virtual machine is launched, the Nova scheduler looks at the average
                        load incurred by AVS cores on each processor, and then selects the processor
                        with the lighter load as the target for the new virtual CPUs.</p>
                    <p id="15">This scheduling approach aims at balancing the switching load
                        on the AVS across virtual machines as they are deployed, and minimizing the
                        cross-NUMA inefficiencies for AVS switching.</p>
                </dd>
            </dlentry>
            <dlentry id="20">
                <dt id="21">Provider networks access verification</dt>
                <dd id="22">
                    <p id="23">When scheduling a virtual machine, the Nova scheduler
                        verifies that the target compute node has data interfaces on all needed
                        provider networks, as determined by the list of tenant networks the virtual
                        NICs are attached to. This verification helps prevent unnecessary debugging
                        steps incurred when the networking services on a guest application fail to
                        operate due to the lack of proper network access.</p>
                </dd>
            </dlentry>
            <dlentry id="140">
                <dt id="141">NUMA node pinning</dt>
                <dd id="142">
                    <p id="143">When scheduling a virtual machine, the Nova scheduler
                        selects a host on which the virtual NUMA nodes can be affined to specific
                        host NUMA nodes according to the flavor extra specifications or image
                        properties.</p>
                </dd>
            </dlentry>
        </dl>
        <section id="section_N10081_N1001F_N10001" id="77">
            <title id="78">The Nova Scheduler and Server Group Policies</title>
            <p id="79">Virtual machines launched as part of a Server Group are subject to scheduling actions
                determined by the selection of scheduling policy. See <xref href="jow1404333660893.xml" id="80"/> for details.</p>
            <dl>
                <dlentry id="81">
                    <dt id="82">Affinity policy</dt>
                    <dd id="83">
                        <p id="84">The goal of this policy is to schedule all virtual
                            machines in the Server Group to execute on the same host. The target
                            compute node is selected by the Nova scheduler when the first instance
                            in the Server Group is launched, in compliance with its corresponding
                            flavor, network, and system requirements.</p>
                        <p id="85"><!--The Nova scheduler takes no special considerations regarding the nature of the target host, HT-enabled or not. If an HT-enabled compute node is selected, then the instance is scheduled to run on any available logical processors as needed. You must live-migrate the instance if you prefer to use a different host.--></p>
                        <p id="86">If the <nameliteral id="87">Best
                                Effort</nameliteral> flag of the Server Group is clear, then the
                            scheduling policy is strictly enforced. Any new instance in the Server
                            Group will fail to launch if any run-time requirements cannot be
                            satisfied by the selected compute node.</p>
                        <p id="88">If the <nameliteral id="89">Best
                                Effort</nameliteral> flag of the Server Group is set, then the
                            scheduling policy is relaxed. New instances are scheduled to run on the
                            selected compute node whenever possible, but are scheduled on a
                            different host if otherwise necessary.</p>
                    </dd>
                </dlentry>
                <dlentry id="90">
                    <dt id="91">Anti-affinity policy</dt>
                    <dd id="92">
                        <p id="93">The goal of this policy is to schedule each virtual
                            machine in the Server Group to execute on a different host. The target
                            compute node for each instance is selected by the Nova scheduler in
                            compliance with the corresponding flavor, network, and system
                            requirements. As with the affinity policy, the Nova scheduler takes no
                            special considerations regarding the nature of the target host,
                            HT-enabled or not.</p>
                        <p id="94">If the <nameliteral id="95">Best
                                Effort</nameliteral> flag of the Server Group is clear, then the
                            scheduling policy is strictly enforced. Any new instance in the Server
                            Group will fail to launch if its run-time requirements can not be
                            satisfied by any newly selected compute node.</p>
                        <p id="96">If the <nameliteral id="97">Best
                                Effort</nameliteral> flag of the Server Group is set, then the
                            scheduling policy is relaxed. New instances are scheduled to run on a
                            different compute node whenever possible, but are scheduled on any
                            already selected host if otherwise necessary.</p>
                    </dd>
                </dlentry>
            </dl>
        </section>
    </conbody>
</concept-wr>