<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE task-wr PUBLIC "-//WindRiver.com//DTD DITA 1.2 Wind River General Task//EN" "task-wr.dtd">
<task-wr id="jow1429544935360" xml:lang="en-us">
<!-- Modification History
        
-->
    <title id="356">Specifying Dedicated CPUs for a VM</title>
    <shortdesc id="357">You can specify the use of dedicated CPUs for a VM using an extra
        specification.</shortdesc>
    <prolog>
        <author id="358">Jim Owens</author>
    </prolog>
    <taskbody>
        <context id="context_N1001F_N1001C_N10001" id="359">
            <p id="204">When a virtual machine is launched, its virtual CPUs use resources
                from a shared pool of cores<!-- (physical cores or logical processors)--> on the
                target compute node. Each virtual CPU is scheduled for processing as an independent
                thread. The <option id="206">CPU Policy</option> extra specification affects
                two aspects of the scheduling process: the CPU affinity mask, and the CPU
                over-commit limit. </p>
            <p id="205">If the <option id="207">CPU Policy</option> is set to
                    <nameliteral id="208">Dedicated</nameliteral>, each virtual CPU thread
                is scheduled to run on a single core exclusively. The cores are removed from the
                shared resource pool, and each virtual CPU is scheduled to run as a dedicated thread
                on its corresponding target core. The cores are returned to the shared pool upon
                termination of the virtual machine.</p>
            <p id="189">Selecting <nameliteral id="209">Dedicated</nameliteral>
                effectively affines each virtual CPU thread to a dedicated core, and ensures that
                the core is not shared with other virtual CPU threads.
                <!--When launching virtual machines on a HT-enabled compute node, this option affines each virtual CPU thread to a logical processor instead; therefore, as many threads as logical processors are available can be scheduled to run on the same core. See <cite id="185">HT-Enabled Compute Nodes</cite> in <xref href="jow1404333563170.xml" id="187"/> for further information.-->
            </p>
            <p id="249">The <nameliteral id="211">Dedicated</nameliteral> setting is
                recommended for Carrier-Grade requirements to prevent over-commitment of host
                resources, and to minimize the impact that scheduling events may have on the latency
                and throughput responses of the guest kernel.</p>
            <p id="251">If the <option id="212">CPU Policy</option> is set to
                    <nameliteral id="213">Shared</nameliteral>, the virtual CPU threads are
                scheduled to run on any available core on the compute node. Also, they can be moved
                within the compute node from one core to another at any time. In a sense, the
                virtual CPU threads can be considered to be floating threads. This is the CPU
                affinity mask portion of the scheduling action.</p>
            <p id="253">Additionally, when <nameliteral id="216">Shared</nameliteral> is selected, the virtual CPU threads can share the core
                with other threads from the same virtual machine or other virtual machines. The
                default OpenStack virtual CPU over-commit ratio is 16:1, which means that up to 16
                virtual CPU threads can share a single core. Over-committing of virtual CPUs is
                highly discouraged in Carrier-Grade operations.</p>
            <p id="17">To add this extra spec to a flavor using the web administration
                interface, use the <uicontrol id="314">CPU Policy</uicontrol> selection in
                the <uicontrol id="316">Create Flavor Extra Spec</uicontrol> drop-down menu.
                To access this menu, see <xref href="jow1429538642910.xml" id="318"/>.</p>
            <p id="308">To add the extra spec using the CLI, use the following command:</p>
            <codeblock id="315"><systemoutput id="322">~(keystone_admin)$ </systemoutput><userinput id="329">nova flavor-key <varname id="336">flavor_name</varname> set hw:cpu_policy=<varname id="347">policy</varname></userinput></codeblock>
            <p id="231">where <varname id="232">policy</varname> is either <option id="354">dedicated</option> or <option id="355">shared</option>.</p>
            <note id="note_N10271_N101E7_N101DE_N101C7_N1014F_N1001F_N10001" id="235">
                <p id="217">Using a single compute node for both shared and dedicated
                    threads is not recommended. For clusters that must support threads of both
                    types, use a host aggregate for each
                    type.<!-- JO 2015-03-13 Chris F suggests adding "in combination with the AggregateInstanceExtraSpecsFilter scheduler filter." -->
                    For more information, see <xref href="jow1426182913166.xml" id="218"/>.</p>
            </note>
        </context>
    </taskbody>
</task-wr>