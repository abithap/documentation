<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic
  PUBLIC "-//OASIS//DTD DITA Topic//EN" "http://docs.oasis-open.org/dita/v1.1/OS/dtd/topic.dtd" ><topic xml:lang="en-us" id="topic8612">
<title>HP Helion <tm tmtype="reg">OpenStack</tm> Carrier Grade 1.1: Performing a System Data Restore</title>
<prolog>
<metadata>
<othermeta name="layout" content="default"/>
<othermeta name="product-version" content="HP Helion Openstack"/>
<othermeta name="product-version" content="HP Helion Openstack 1.1"/>
<othermeta name="role" content="All"/>
<othermeta name="role" content="Paul F"/>
<othermeta name="product-version1" content="HP Helion Openstack"/>
<othermeta name="product-version2" content="HP Helion Openstack 1.1"/>
</metadata>
</prolog>
<body>
<p>
<!--UNDER REVISION-->
 <!--./CarrierGrade/AdminGuideNew/Backup/carrier-grade-admin-wr-retore.md-->
 <!--permalink: /helion/openstack/carrier/admin/backup/restore/--></p>
<p>You can restore a HP Helion OpenStack Carrier Grade cluster from available system data and Cinder volume backup files to bring it back to the operational state it was when the backup procedure took place.</p>
<p>Restoring a HP Helion OpenStack Carrier Grade cluster from a set of backup files is done by restoring one host at a time, starting with the controllers, then the storage nodes, and finally the compute nodes.</p>
<!-- ===================== horizontal rule ===================== -->
<p>
<b>Note:</b> This feature applies only to servers in the VNF Region.</p>
<!-- ===================== horizontal rule ===================== -->
<p>Before you start the restore procedure you must ensure the following conditions are in place:</p>
<ul>
<li>All cluster hosts must be powered down, just as if they were going to be initialized anew.</li>
<li>All backup files are accessible from a USB flash drive locally attached to the controller where the restore operation takes place (controller-0).</li>
<li>You have the HP Helion OpenStack Carrier Grade installation image available on a USB flash drive, just as when the software was installed the first time. It is mandatory that you use the exact same version of the software used during the original installation, otherwise the restore procedure will fail.</li>
<li>The restore procedure requires all hosts but controller-0 to boot over the internal management network using the PXE protocol, just as it was done during the initial software installation. </li>
</ul>
<p>Ideally, you cleaned up all hard drives in the cluster, and the old boot images are no longer present; the hosts then default to boot from the network when powered on. If this is not the case, you must configure each host manually for network booting right after this exercise asks you to power them on.</p>
<ul>
<li>
<xref type="section" href="#topic8612/controller0">Restore controller-0</xref>
</li>
<li>
<xref type="section" href="#topic8612/controller1">Restore controller-1</xref>
</li>
<li>
<xref type="section" href="#topic8612/storage">Restore the storage nodes</xref>
</li>
<li>
<xref type="section" href="#topic8612/cinder">Restore Cinder volumes</xref>
</li>
</ul>
<section id="controller0"> <title>Restore controller-0</title>
<ol>
<li>
<p><xref
              href="http://gaf2871b9d2d13cf45c1306b35bf01764.cdn.hpcloudsvc.com/HP_Helion_OpenStack_Carrier_Grade_Software_Install.pdf"
              format="pdf" scope="external">Install the HP Helion OpenStack
  Carrier Grade software on controller-0</xref></p>
       <p>When the software installation is complete, you should be able to log in using the host's console and the web administration interface.</p>
</li>
<li>
<p>Log in to the console as user <codeph>wrsroot</codeph> with password <codeph>wrsroot</codeph>.</p>
</li>
<li>
<p>Ensure the backup files are available to the controller.</p>

<p>Plug the USB flash drive containing the backup files into the system. The USB flash drive is mounted automatically. Use the command df to list the mount points.</p>

<p>The following steps assume that the backup files are available from a USB flash drive mounted in <codeph>/media/wrsroot</codeph> in the directory <codeph>backups</codeph>.</p>
</li>
<li>
<p>Use the following command to update the controller's software to the previous patching level.</p>

<codeblock>
<codeph>sudo config_controller --restore-system \
</codeph>
</codeblock>

<p>When restoring the system configuration, the current software version on the controller, at this time, the original software version from the ISO image, is compared against the version available in the backup files. They differ if the latter includes patches applied to the controller's software at the time the backup files were created. If different, the restore process automatically applies the patches and forces an additional reboot of the controller to make them effective.</p>

<p>You must enter reboot at the prompt as requested. Once the controller is back, log in as user wrsroot as before to continue.</p>
</li>
<li>
<p>Use the following command to verify that the patches were applied, as illustrated in the following example:</p>

<codeblock>
<codeph>sudo wrs-patch query


Patch ID                    Repo State      Patch State
=======================     ==========      ===========
COMPUTECONFIG               Available       n/a
LIBCUNIT_CONTROLLER_ONLY    Applied         n/a
STORAGECONFIG               Applied         n/a
</codeph>
</codeblock>
</li>
<li>
<p>Use the following command to restore the system configuration.</p>

<codeblock>
<codeph>sudo config_controller --restore-system \
/media/wrsroot/backups/backup_20140918_images.tgz
</codeph>
</codeblock>

<p>The controller's software is up to date now, at the same stage it was when the backup operation was executed. The restore procedure can be invoked again, and should run without interruptions.</p>
</li>
<li>
<p>Authenticate to the system as Keystone user <b>admin</b>.</p>

<p>Use the following command to source the admin user environment as follows:</p>

<codeblock>
<codeph>source /etc/nova/openrc
</codeph>
</codeblock>
</li>
<li>
<p>Use the following command to restore the system images.</p>

<codeblock>
<codeph>sudo config_controller --restore-images \
/media/wrsroot/backups/backup_20140918_images.tgz
</codeph>
</codeblock>
</li>
</ol>
</section>
<section id="controller1"> <title>Restore controller-1</title>
<p>Use the following command to restore <codeph>controller-1</codeph>.</p>
<ol>
<li>
<p>List the current state of the hosts.</p>

<codeblock>
<codeph>system host-list
</codeph>
</codeblock>
</li>
<li>
<p>Power on the host.</p>

<p>Remember to ensure that the host boots from the network and not from an old disk image that might still be present on its hard drive.</p>
</li>
<li>
<p>Use the following command to unlock the host.</p>

<codeblock>
<codeph>system host-unlock 2
</codeph>
</codeblock>
</li>
<li>
<p>Use the following command to verify the new state of the hosts.</p>

<codeblock>
<codeph>system host-list
</codeph>
</codeblock>
</li>
</ol>
<p>The unlocking operation forces a reboot of the host, which is then initialized with the corresponding image available from the system backup.</p>
<p>You must wait for the host to become enabled and available before proceeding to the next step.</p>
</section>
<section id="storage"> <title>Restore the storage nodes</title>
<p>You need to restore the storage nodes if you are using the Cinder Ceph backend. Follow the same procedure used to <xref type="section" href="#topic8612/controller1">restore controller-1</xref>, first restoring host storage-0 and then storage-1.</p>
</section>
<section id="cinder"> <title>Restore Cinder volumes</title>
<p>You restore Cinder volumes by importing them from the backup files. You must import all volume backup files,
one at a time.</p>
<ol>
<li>
<p>Delete any snapshots that may exist in the system.</p>

<p>The restore operation fails if a snapshot exists for a volume that is about to be restored. You can list the available snapshots with the command cinder snapshot-list --all-tenants , and remove them with the command cinder snapshot-delete snapshot-id.</p>
</li>
<li>
<p>Use the following command to list the current state of the Cinder volumes.</p>

<codeblock>
<codeph>cinder list --all-tenants
</codeph>
</codeblock>

<p>All volumes are reported to be in the error state because they are not available yet, but they are registered in the storage database.</p>
</li>
<li>
<p>Copy the Cinder volumes to the /opt/backups folder.</p>

<codeblock>
<codeph>sudo cp /media/wrsroot/backups/volume-* /opt/backups
</codeph>
</codeblock>
</li>
<li>
<p>Use the following command to import a volume.</p>

<codeblock>
<codeph>cinder import volume-53ad007a-841a-4fd3-a22b-813d4a6a8a85.tgz
</codeph>
</codeblock>

<p>The volume remains in the importing state while the operation takes place. Use the backup_status filed on the output of the cinder show command to monitor the progress. You must wait until the original volume state, <codeph>in-use</codeph> or <codeph>available</codeph>, is restored. Note that in-use volumes are fully restored even if their corresponding instances are not running yet.</p>
</li>
<li>
<p>Use the following command to import all other volumes in the backup files.</p>

<codeblock>
<codeph>cinder list --all-tenants
</codeph>
</codeblock>

<p>Once finished, all volumes are listed in their original states.</p>
</li>
<li>
<p>Use the following command to remove any in-use volumes that remain in error.</p>

<codeblock>
<codeph>cinder list
</codeph>
</codeblock>

<p>You must remove all in-use volumes that for any reason failed to recover, and their associated virtual machine instances. This is necessary to prevent errors from occurring when restoring the compute nodes used to launch the virtual machines. If an in-use volume is in error at the time its virtual machine is launched, the Nova scheduler reports an error and the restore operation fails.</p>

<p>For the purposes of this example, assume that volume 53ad007a-... failed to restore. Its status is then reported as error.</p>
</li>
<li>
<p>Use the following command to remove the virtual machine instance.</p>

<codeblock>
<codeph>nova delete c820df55-...
</codeph>
</codeblock>
</li>
<li>
<p>Use the following command to remove the volume in error status.</p>

<codeblock>
<codeph>cinder delete 53ad007a-...
</codeph>
</codeblock>
</li>
</ol>
</section>
<section id="compute"> <title>Restore compute</title>
<p>Once the cinder volumes are restored, you must restore the compute nodes, one at a time.</p>
<p>You restore these hosts following the same procedure used to <xref type="section" href="#topic8612/controller1">restore controller-1</xref>.</p>
<p>The state of the hosts when the restore operation is complete is as follows:</p>
<codeblock>
  <codeph>system host-list
</codeph>
</codeblock>
<p>
  <image href="../../../media/CGH-WR-System-Restore.png"/>
</p>
<p>As each compute node is restored, the original instances at the time the backups were done are started automatically.</p>
<p>The HP Helion OpenStack Carrier Grade sever is fully restored. The state of the system, including storage resources and virtual machines, is identical to the state the cluster was in when the backup procedure took place.</p>
<p>Remember however, that passwords for local user accounts must be restored manually since they are not included as part of the backup and restore procedures. For more information, see [Linux User Accounts].</p>
<p>
  <xref type="section" href="#topic8612"> Return to Top </xref>
</p>
<!-- ===================== horizontal rule ===================== -->
</section>
</body>
</topic>
