<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic
  PUBLIC "-//OASIS//DTD DITA Topic//EN" "http://docs.oasis-open.org/dita/v1.1/OS/dtd/topic.dtd" ><topic xml:lang="en-us" id="topic10581">
<title>HP Helion <tm tmtype="reg">OpenStack</tm> Carrier Grade 1.1: Creating the HLM Virtual Machine</title>
<prolog>
<metadata>
<othermeta name="layout" content="default"/>
</metadata>
</prolog>
<body>
<p>
<!--UNDER REVISION-->
 <!--./CarrierGrade/Installation/carrier-grade-install-pb-hlm-vm.md-->
 <!--permalink: /helion/openstack/carrier/install/pb/hlm-vm/--></p>
<p>Helion Lifecycle Management (HLM) consists of the ongoing operations/maintenance as well as the initial deployment of the HP Helion <tm tmtype="reg">OpenStack</tm> Carrier Grade system.</p>
<p>HLM consists of initial HLM node deployment followed by Helion cloud nodes deployment.</p>
<section id="logging"> <title>Logging</title>
<p>For all scripts that are executed (including <codeph>hlm-build</codeph> and <codeph>h*</codeph> scripts below), separate log files are generated on <codeph>/var/log/hlm/</codeph> folder.</p>
<p>For example, if you run the <codeph>hprovsion</codeph> command, all command line output will also be logged in <codeph>/var/log/hlm/hprovision.log</codeph> file.</p>
<p>The following is the list of scripts for which logging is implemented:</p>
<ul>
<li>hlm_updatepackages.sh</li>
<li>hlm_initcobbler.sh</li>
<li>hlm_importiso.sh</li>
<li>hlm_prepareenv.sh</li>
<li>hnewcloud</li>
<li>hprovision</li>
<li>hcfgproc</li>
<li>hnetinit</li>
</ul>
</section>
<section id="prepare"> <title>Prepare the system for deployment</title>
<p>Use the following steps to prepare the KVM host to deploy the VMs.</p>
<ol>
<li>Copy the installer files and Ansible playbooks to KVM host.<p>a. Copy the
              <codeph>cg-hlm.qcow2</codeph> file to <codeph>/home/images</codeph>.</p><p>b. Copy the
              <codeph>cg-hos-dcn-vsd.tar.gz.gpg</codeph> to the <codeph>/home/images</codeph>
            directory.</p><p>c. Decrypt and untar the PGP file using password
              <codeph>cghelion</codeph> when
            prompted.</p><codeblock><codeph>gpg -d cg-hos-dcn.tar.gz.gpg | tar -xzvf
</codeph></codeblock><p>d.
            Copy the <codeph>infra-ansible-playbooks</codeph> file to <codeph>/root</codeph>. e.
            Decrypt and untar the file using password <codeph>cghelion</codeph> when
            prompted.</p><p>f. Copy the <codeph>cg-hos.tar.gz.gpg</codeph> file to
              <codeph>/root/cg</codeph>. Create the directory if it does not exist.</p><p>g. Copy
            the <codeph>cg-hos-dcn-vsc.tar.gz.gpg</codeph> file to
            <codeph>/root/cg</codeph>.</p><p>h. Copy the files from the
              <codeph>Patches/build-33</codeph>
            folder</p><codeblock><codeph>hnewcloud.sh
ansible.cfg
ilopxebootonce.py
venom-patch.tar.gz
</codeph></codeblock><p>i.
            Extract the <codeph>venom-patch.tar.gz</codeph> and follow the readme file.</p><p>The
              <codeph>cg-hlm.qcow2</codeph> and <codeph>VSD-3.0.0_HP_r3.0_16.qcow2</codeph> are in
            the <codeph>/home/images</codeph> folder. Refer to the <codeph>group_vars/all</codeph>
            file on the KVM host if you need the location of the <codeph>/home/images</codeph>
            file.</p></li>
<li>Edit the <codeph>/root/infra-ansible-playbooks/group_vars/all</codeph> file for you environment.
          Comments added in the file explain each variable.</li>
<li>Check the hosts file <codeph>/root/infra-ansible-playbooks/hosts</codeph> file and make sure it
          appears similar to the following. Make sure the VSD IP (DCM network) is
          correct.<codeblock><codeph>Update the vsd CLM IP in this file
[vsd]
10.200.50.21 ansible_ssh_user=root ansible_ssh_pass=Alcateldc

[hlm]

[hlm_kvm_host]
192.168.122.1 
</codeph></codeblock></li>
</ol>
</section>
<section id="deploy-the-hlm-and-vsd-vm"> <title>Deploy the HLM and VSD VM</title>
<p>The steps provided below will get you to use ansible playbook and bring up HLM and VSD VM up on the HLM KVM Host.</p>
<ol>
<li>Make sure the Ansible playbook file is not in executable mode.</li>
<li>Execute the following
            command:<codeblock><codeph>ansible-playbook -i hosts setup_hlm_onBM.yml
</codeph></codeblock><p>The
            above command will do the following
            :</p><codeblock><codeph>a. Copy both installation files (tar balls) to your HLM, decrypt, and extract the files.
b. Execute the `updatepackages` command.
c. Execute the `prepareenv` command.
d. Execute the `Init cobbler` command.
e. Execute the `Importiso` command.
</codeph></codeblock><p>You
            will see similar message when the playbook is run successful.</p><p>
            <image href="../../media/CGH-Install-Ansible.png"/>
          </p></li>
</ol>
</section>
<section id="apply-the-patch-to-fix-qemu-venom-vulnerability"> <title>Apply the patch to fix QEMU Venom vulnerability</title>
<p>There is a <xref href="https://bugs.launchpad.net/openstack-ansible/+bug/1454677" scope="external" format="html" >known vulnerability</xref> in <xref href="http://wiki.qemu.org/Main_Page" scope="external" format="html" >QEMU</xref>.</p>
<p>Use the following steps to patch the vulnerability:</p>
<ol>
<li>Copy <codeph>venom-patch.tar.gz</codeph> to the HLM VM and extract the files. Follow
          instructions in Readme.txt file that is in the extracted files.</li>
<li>Copy the <codeph>packages-patch.tar.gz.gpg</codeph> to <codeph>/root</codeph> directory on HLM
          VM.</li>
<li>Decrypt and extract the <codeph>packages-patch.tar.gz.gpg</codeph> file, using the password
            <codeph>cghelion</codeph> when
          prompted:<codeblock><codeph>gpg -d packages-patch.tar.gz.gpg | tar -xzvf -
</codeph></codeblock></li>
<li>Execute the following command to update the package list on the HLM
          VM:<codeblock><codeph>rm /root/packages/repo/debian/qemu*2.1.3*.deb
apt-get update
apt-get -y --force-yes install dpkg-dev
cd /root/packages/repo/debian
dpkg-scanpackages . /dev/null | gzip -9c &gt; Packages.gz
</codeph></codeblock></li>
<li>Execute the following command to update the package repository on the HLM
          VM:<codeblock><codeph>cd /root/cg-hlm/hlm-build
./hlm_updatepackages.sh
</codeph></codeblock></li>
</ol>
</section>
<section id="logging-into-vsd-node-creating-user-and-applying-license"> <title>Logging into VSD node, creating user and applying License</title>
<p>VSD node provisioning happens as part of Infrastructure Ansible playbook run.</p>
<p>You can SSH to the VSD VM from HLM KVM host using the DCM IP.</p>
<codeblock><codeph> virsh console vsd
</codeph></codeblock>
<p>If everything is installed correctly, you should see the status as below from VSD VM.</p>
<p>
  <image href="../../media/CGH-install-virsh-vsd.png"/>
</p>
</section>
<section id="vsd-performance-workaround"> <title>VSD Performance workaround</title>
<p>By default VSD has only 8G memory. For better performance behavior, you can update the VSD memory to 16G.</p>
<ol>
<li>From KVM host,use the following command to bring down the VSD
          VM:<codeblock><codeph>virsh destroy vsd
</codeph></codeblock></li>
<li>Use the following command to edit the memory setting in the VSD XML
          file:<codeblock><codeph>virsh edit vsd 

Current value 
    &lt;memory unit='KiB'&gt;8388608&lt;/memory&gt;
    &lt;currentMemory unit='KiB'&gt;8388608&lt;/currentMemory&gt;

Change to 
    &lt;memory unit='KiB'&gt;16777216&lt;/memory&gt;
    &lt;currentMemory unit='KiB'&gt;16777216&lt;/currentMemory&gt;
</codeph></codeblock></li>
<li>Save the value<codeblock><codeph>virsh save vsd
</codeph></codeblock></li>
<li>Use the following command to start the
            VM:<codeblock><codeph>virsh start vsd
</codeph></codeblock><p>It might take 15 min or
            more to sync with NTP and to get all the other VSD services up.</p></li>
</ol>
</section>
<section id="launch-vsd-dashboard"> <title>Launch VSD Dashboard</title>
<p>As VSD VM has DCM interface to HLM KVM host, you should be able to XRDP from jump server to the HLM KVM host and can launch the dash board.</p>
<ol>
<li>Using an internet browser such as Chrome or FireFox, navigate to the DCM IP of VSD.</li>
<li>In the log in page, enter the default
          credentials:<codeblock><codeph>User Name: Csproot 
Password: csproot 
Org: csp 
VSD Server : auto 
</codeph></codeblock></li>
</ol>
</section>
<section id="applying-the-license"> <title>Applying the License</title>
<p>To install the required DCN license:</p>
<ol>
<li>From VSD Dashboard, click the <b>Open VSP Configuration</b> tab on the top right corner of the
          dashboard.</li>
<li>Click the <b>Licenses</b> tab and click <b>+</b>.</li>
<li>Copy and paste the license that you have receive into the screen that displays.</li>
</ol>
</section>
<section id="create-user-for-plugin-login"> <title>Create User for Plugin Login</title>
<p>You must create a user and add it to CMS Group.</p>
<ol>
<li>From VSD Dashboard, click the <b>Open VSP Configuration</b> tab on the top right corner of the
          dashboard.</li>
<li>Click the <b>CSP Users</b> tab and click <b>+</b>.</li>
<li>Create a user named <codeph>OSadmin</codeph> with the password <codeph>OSadmin</codeph>.</li>
<li>Add the user to the <codeph>CMS Group</codeph>.</li>
</ol>
</section>
<section id="configure-a-json-file-for-installation"> <title>Configure a JSON file for installation</title>
<p>The HLM VM deployment requires a JSON file. Use the following steps to install and edit the file.</p>
<ol>
<li>On the KVM host, change to the home directory.<codeblock><codeph>cd ~
</codeph></codeblock></li>
<li>Provision and configure your HLM
            VM.<codeblock><codeph>hnewcloud  &lt;cloudname&gt; &lt;cloud_template&gt;
</codeph></codeblock><p>Where:</p><ul>
            <li>cloudname is the name of the cloud to create</li>
            <li>cloud_template is the name of the template to use.</li>
          </ul><p>The command creates the <codeph>&lt;cloudname&gt;</codeph> directory, which will
            contain a JSON template file <codeph>node-provision.json</codeph>. This template
            supplies input values to the <codeph>hprovision</codeph> script, later in the
            installation.</p></li>
<li>Edit <codeph>node-provision.json</codeph> file based on following guidelines:<table>
            <tgroup cols="2">
              <colspec colname="col1" colsep="1" rowsep="1"/>
              <colspec colname="col2" colsep="1" rowsep="1"/>
              <thead>
                <row>
                  <entry colsep="1" rowsep="1">Field</entry>
                  <entry colsep="1" rowsep="1">Baremetal</entry>
                </row>
              </thead>
              <tbody>
                <row>
                  <entry>name</entry>
                  <entry>Name of the system you want to add in cobbler</entry>
                </row>
                <row>
                  <entry>Pxe-mac-address</entry>
                  <entry>MAC address of the interface you want to pxe boot onto. This is not same as
                    iLO MAC address,</entry>
                </row>
                <row>
                  <entry>Pxe-interface</entry>
                  <entry>nterface name on which pxe boot should occur. For example: `eth0`</entry>
                </row>
                <row>
                  <entry>pm_type</entry>
                  <entry>ipmilan </entry>
                </row>
                <row>
                  <entry>pm_ip</entry>
                  <entry>Power management IP:ilo ip</entry>
                </row>
                <row>
                  <entry>pm_user</entry>
                  <entry>Power management user: ilo username</entry>
                </row>
                <row>
                  <entry>pm_pass</entry>
                  <entry>Power management password: ilo password</entry>
                </row>
                <row>
                  <entry>node_group</entry>
                  <entry>For now, this should have the same value as `node-type` in the `nodes.json`
                    file. For example: `CCN-001-001`.</entry>
                </row>
                <row>
                  <entry>failure_zone, vendor, model, os_partition_size, data_partition_size</entry>
                  <entry>These fields have same significance as they have in the `nodes.json` used
                    during cloud deployment</entry>
                </row>
              </tbody>
            </tgroup>
          </table><p>To see a sample <codeph>node-provision.json</codeph> file, see <xref
              href="../../CarrierGrade/Installation/carrier-grade-install-pb-hlm-vm-json.dita"
              >Create the HLM Virtual Machine</xref>.</p></li>
</ol>
</section>
<section id="create-new-cloud-template-and-bring-the-cloud-nodes-up"> <title>Create new cloud template and bring the cloud nodes up</title>
<p>After you edit the <codeph>node-provision.json</codeph> file, you must enable one-time PXE boot on cloud nodes to set the correct boot order. Execute the following on the HLM VM</p>
<ol>
<li>Use the following command to install the <codeph>python-hpilo</codeph> module on HLM VM:<codeblock><codeph>pip install python-hpilo
</codeph></codeblock><p>
            <codeph>python-hpilo</codeph> is a python library and command-line tool for
          iLO.</p></li>
<li>Copy the <codeph>ilopxebootonce.py</codeph> from the <codeph>Patches/build-33 folder</codeph>
          you <xref type="section" href="#topic10581/prepare">downloaded earlier</xref> to directory
          where you have <codeph>node-provision.json</codeph>.</li>
<li>Execute the following
          script<codeblock><codeph>python ilopxebootonce.py node-provision.json
</codeph></codeblock></li>
</ol>
<p>After the script is run, the <codeph>Current One-Time Boot Option</codeph> is set to <codeph>Network Device 1</codeph> on all the servers listed in <codeph>node-provision.json</codeph> file.</p>
</section>
<section id="create-a-new-cloud-template-and-bring-the-cloud-nodes-up"> <title>Create new cloud template and bring the cloud nodes up</title>
<ol>
<li>Use the following script to start the provisioning of the HLM VM:<p>hprovision
              <codeph>&lt;cloudname&gt;</codeph>
          </p><p>Where:</p><ul>
            <li>cloudname is the name of the cloud to create</li>
          </ul><p>This script will PXE boot the nodes specified in
              <codeph>node-provision.json</codeph> file. The script alsos track the PXE boot
            completion process and will create the <codeph>nodes.json</codeph> file in the
            directory. </p></li>
<li>Update the <codeph>node-provision.json</codeph> files used in the previous step.<p>a. Change to
            the <codeph>&lt;cloudname&gt;</codeph>
            directory:</p><codeblock><codeph>cd ~/&lt;cloudname&gt;
</codeph></codeblock><p>b.
            Verify that the each PXE-booted nodes has an IP address that matches the IP address
            specified in the <codeph>node-provision.json</codeph> file.</p></li>
<li>Modify the <codeph>environment.json</codeph> file to configure the VLANs and network addresses
          that need to be configured for respective cloud nodes.<codeblock><codeph>"cidr": 
"start-address": 
</codeph></codeblock><ul>
            <li>
              <p>The three controller nodes should have CLM, CAN, EXT, BLS on eth0 and TUL on
                eth1.</p>
            </li>
            <li>
              <p>The two compute nodes should have CLM, EXT, BLS on eth0 and TUL on eth1.</p>
            </li>
          </ul><p>
            <b>Example:</b>
          </p><codeblock><codeph>"cidr": "192.168.101.0/24",
"start-address": "192.168.101.100"
</codeph></codeblock><p>
            <b>NOTE:</b> The Helion Configuration Processor assigns the first address of the CLM
            address range to itself for serving python and debian repositories. Make sure that you
            set the first IP address of the CLM range for the eth2 (CLM) address of the HLM
            node.</p></li>
<li>Modify the <codeph>definition.json</codeph>
          file:<codeblock><codeph>a. Set the number of compute systems to 2.

    "count": 2, //number of computes in the resource pool.

b. Update `ansible-vars` section with all the information based on your setup.

c. Make sure you have two NTP entries at the end of this `definition.json` file as seen in the snapshot. If you have only one NTP server in your environment, specify the same NTP server twice.
</codeph></codeblock></li>
<li>Once you have correctly edited all the json "Cloud Model" files, run the Helion Configuration
          Processor on
            these<codeblock><codeph>cd /usr/local/bin

hcfgproc -d definition.json
</codeph></codeblock><p>The
              <codeph>hcfgproc</codeph> script gets installed in <codeph>/usr/local/bin</codeph> by
            the <codeph>prepare-env</codeph> script. The script generates a <codeph>clouds/</codeph>
            directory within the  directory. </p></li>
<li>Review the CloudDiagram, <codeph>hosts.hf</codeph>, and
            <codeph>net/interfaces.d/eth.cfg</codeph> files to make sure the network settings are
          correct.</li>
<li>Initialize network interfaces on all the cloud nodes using the following
            command:<codeblock><codeph>hnetinit &lt;cloudname&gt; 
</codeph></codeblock><p>You can
            run this command from any directory.</p></li>
</ol>
<p>After this command completes, all cloud nodes and CLM network interfaces should be set correctly.</p>
</section>
<section id="next-step"> <title>Next Step</title>
<p>
  <xref href="../../CarrierGrade/Installation/carrier-grade-install-pb-workarounds.dita" >HLM Post-Installation Tasks</xref>
</p>
<p>
  <xref type="section" href="#topic10581"> Return to Top </xref>
</p>
<!-- ===================== horizontal rule ===================== -->
</section>
</body>
</topic>
