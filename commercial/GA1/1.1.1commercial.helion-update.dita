<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic
  PUBLIC "-//OASIS//DTD DITA Topic//EN" "http://docs.oasis-open.org/dita/v1.1/OS/dtd/topic.dtd" ><topic xml:lang="en-us" id="topic51655">
<title>HP Helion <tm tmtype="reg">OpenStack</tm> 1.1.1: Update Procedure</title>
<titlealts>
<searchtitle>HP Helion Openstack 1.1: Update Procedure</searchtitle>
</titlealts>
<prolog>
<metadata>
<othermeta name="layout" content="default"/>
<othermeta name="product-version" content="HP Helion Openstack"/>
<othermeta name="product-version" content="HP Helion Openstack 1.1"/>
<othermeta name="role" content="Systems Administrator"/>
<othermeta name="role" content="Cloud Architect"/>
<othermeta name="role" content="Storage Administrator"/>
<othermeta name="role" content="Network Administrator"/>
<othermeta name="role" content="Service Developer"/>
<othermeta name="role" content="Cloud Administrator"/>
<othermeta name="role" content="Application Developer"/>
<othermeta name="role" content="Network Engineer"/>
<othermeta name="role" content="Michael B, Paul F"/>
<othermeta name="product-version1" content="HP Helion Openstack"/>
<othermeta name="product-version2" content="HP Helion Openstack 1.1"/>
</metadata>
</prolog>
<body>
<p>
<!--PUBLISHED-->
 <!--./commercial/GA1/1.1.1commercial.helion-update.md-->
 <!--permalink: /helion/openstack/1.1.1/update/--></p>
<p>Welcome to the HP Helion OpenStack v1.1.1 update instructions. These instructions apply to existing HP Helion OpenStack v1.1 installations and describe how you can update your HP Helion OpenStack cloud environment from v1.1 to v1.1.1. (The process of updating a Helion OpenStack v1.0 or v1.01 release to Helion OpenStack v1.1.1 is not supported.)</p>
<p>
<b>Note to Helion Development Platform users</b>: 
The HP Helion Development Platform requires HP Helion OpenStack v1.1.1. However, HP Helion OpenStack v1.1.1 does not currently support updating the HP Helion Development Platform from v1.1 to v1.2.</p>
<!--At this time, the Helion Development Platform requires HP Helion Open Stack v1.1.1. Upgrading Helion Development Platform from v1.1 to v1.2 is currently not supported. -->
<p>
<b>Note</b>: You can also deploy the HP Helion OpenStack v1.1.1 release from scratch. For those instructions, please see <xref href="../../commercial/GA1/1.1commercial.install-GA-overview.dita" >Installing Helion OpenStack</xref>.
<!--verify link--></p>
<!-- Helion OpenStack v1.1.1 update scripts update HP Helion OpenStack v1.1 to version v1.1.1 and starts the update process of Helion Development Platform v1.1 to v1.2. To complete the HP Helion Development Platform update to v1.2, refer to the HP Helion Development Platform documentation listed in [Updating Helion Development Platform](/helion/devplatform/1.2/upgrade/). -->
<!--
**Note for Helion Development Platform version v1.1 users**: This Helion OpenStack version v1.1.1 update procedure supports Helion Development Platform version v1.2, (and only Helion Development Platform version v1.2). For users who have installed Helion OpenStack v1.1 and Helion Development Platform v1.1, when you update to Helion OpenStack v1.1.1, you also need to update to Helion Development Platform v1.2.  

**Important for HP Helion Development Platform users**: During the HP Helion OpenStack v1.1.1 update, you will need to stop your Helion Development Platform services. Failure to do so will likely damage your existing Helion Development Platform v1.1 installation. 
For details on the HP Helion Development Platform update to v1.2, see [Updating Helion Development Platform](/helion/devplatform/1.2/upgrade/). -->
<p>The process of updating HP Helion OpenStack v1.1 to HP Helion OpenStack v1.1.1 consists of three procedures:</p>
<ol>
<li>
<xref type="section" href="#topic51655/seed-update-process">Update the seed host</xref>
</li>
<li>
<xref type="section" href="#topic51655/uc-update-process">Update the undercloud</xref>
</li>
<li>
<xref type="section" href="#topic51655/oc-update-process">Update the overcloud</xref>
</li>
</ol>
<p>These procedures are described in the following sections. You must perform each of these three procedures in the order presented: first the seed host, then the undercloud, and finally, the overcloud.</p>
<p>Allow a full day for the  HP Helion OpenStack v1.1.1 update to complete. During the update process, affected systems will be offline.</p>
<p>
<b>Note</b>: Nodes to be updated must be in a "good" state before running the update. Specifically, being in a "good" state means:</p>
<ul>
<li>Nodes must be up</li>
<li>Nodes must be running</li>
<li>Heat state of each node to be updated must not include any FAIL status</li>
</ul>
<p>
<b>Note</b>: You must take a backup of the seed, undercloud, and overcloud using the standard procedures before commencing the update process.
<!-- DOCS-1354 --></p>
<section id="deployment-considerations"> <title>Deployment considerations</title>
<p>Check that the deployed nodes (all seed VM, undercloud, and overcloud nodes) are accessible (SSH) and pingable. Note the network and bridge configurations on all nodes using the <codeph>ifconfig</codeph> and <codeph>ovs-vsctl show</codeph> commands.</p>
<p>Check the version of the Glance images that were used to deploy each of the overcloud nodes before the update. To check the Glance image versions:</p>
<ol>
<li>SSH to the undercloud node.</li>
<li>
<p>Enter:</p>

<codeblock>
<codeph>source stackrc
glance image-list
nova list
nova show &lt;instance id&gt;
</codeph>
</codeblock>
</li>
<li>
<p>Compare the Image ID returned from the <codeph>glance image-list</codeph> command  to that returned from the <codeph>nova show</codeph> command.</p>
</li>
<li>
<p>Make sure the following commands are working before you start the update:</p>

<ul>
<li>keystone user-list</li>
<li>nova list</li>
<li>neutron net-list</li>
<li>glance image-list</li>
<li>os-refresh-config</li>
<li>os-collect-config</li>
</ul>
</li>
<li>
<p>If you are unsure of the version of Helion you are running, from any Helion node (seed VM, undercloud, or overcloud node), enter:</p>

<codeblock>
<codeph># cat /etc/HP_Helion_version
</codeph>
</codeblock>
</li>
</ol>
<p>A sample of the output is:</p>
<codeblock>
  <codeph>HP Helion Openstack 1.1 Build 81
</codeph>
</codeblock>
</section>
<section id="seed-update-process"> <title>Helion v1.1 to v1.1.1 seed host update process</title>
<p>The procedure will guide you through updating to Helion v1.1.1 from the seed host using <codeph>update_sd.sh</codeph>.</p>
<p>
<b>Note</b>: You must have the <codeph>jq</codeph> package for your version of Ubuntu installed on the seed host. If you need to install this package, enter:</p>
<codeblock>
  <codeph>apt-get install jq
</codeph>
</codeblock>
<!--
**Warning for HP Helion Development Platform users**: Failure to source your `kvm-custom-ips.json` file could result in damage to your HP Helion Development Platform installation. For more information see [Environment Variables being ignored](#envvarsignored).
-->
<p>To prepare the seed VM:</p>
<ol>
<li>
<p>Log into the seed VM by entering:</p>

<codeblock>
<codeph>ssh root@&lt;seed-ip&gt;
</codeph>
</codeblock>
</li>
<li>
<p>Remove any update folders and content from previous update attempts in the <codeph>/root</codeph> folder using the <codeph>rm</codeph> command.</p>
</li>
<li>
<p>From <codeph>/root</codeph>, create the <codeph>helion-update-1.1-to-&lt;version&gt;</codeph> directory then change to this directory by entering:</p>

<codeblock>
<codeph>cd /root
mkdir helion-update-1.1-to-&lt;version&gt;
cd helion-update-1.1-to-&lt;version&gt;
</codeph>
</codeblock>
</li>
<li>
<p>Download the update kit <codeph>HP_Helion_Openstack</codeph> to the <codeph>helion-update</codeph> directory on the seed VM.</p>
</li>
<li>
<p>Extract the update kit by entering:</p>

<codeblock>
<codeph>tar -xvzf `HP_Helion_Openstack
</codeph>
</codeblock>
</li>
<li>
<p>Copy the <codeph>kvm-custom-ips.json</codeph> file to the new directory by entering:</p>

<codeblock>
<codeph>cp ~/tripleo/configs/kvm-custom-ips.json ~/helion-update-1.1-to-&lt;version&gt;/tripleo/configs/kvm-custom-ips.json
</codeph>
</codeblock>
</li>
<li>
<p>Verify that there is enough free space on the system to carry out the update. The  space you need must support a copy of the following folders:</p>

<ul>
<li>/root/tripleo </li>
<li>/mnt/state </li>
<li>/root/helion-update/helion-update-1.1-to-1.1.1</li>
<li>/tftpboot</li>
</ul>
</li>
<li>
<p>To see the amount of space these folders require, enter:</p>

<codeblock>
<codeph>du -hcs /root/tripleo /mnt/state /root/helion-update* /tftpboot
</codeph>
</codeblock>
</li>
<li>
<p>Check the free space on the system by entering:</p>

<codeblock>
<codeph>df -h /
</codeph>
</codeblock>

<p>The space required cannot exceed the space available (that is, the output from the <codeph>du -hcs</codeph> command).</p>

<p>If you need to free up space, you can safely remove the <codeph>HP_Helion_Openstack</codeph> update kit that you just extracted</p>
</li>
<li>
<p>To prepare the seed host for starting the update, verify that the following files exist in the seed VM.</p>

<ul>
<li>/root/eca.key</li>
<li>/root/eca.crt</li>
</ul>
<p>If you specified the ephemeral CA key and certificate from a different directory using environment variables <codeph>EPHEMERAL_CA_KEY_FILE</codeph> and <codeph>EPHEMERAL_CA_CERT_FILE</codeph>, you must copy them to the seed VM <codeph>/root</codeph> directory. Then rename them to <codeph>eca.crt</codeph> and <codeph>eca.key</codeph>.</p>

<p>If you do not have these files, you may be able to regenerate them from the installer data using the following commands:</p>

<p>echo -e $(jq '.parameters.EphemeralCaKey' ../tripleo/overcloud-env.json | sed 's/\"//g') &gt; /root/eca.key
echo -e $(jq '.parameters.EphemeralCaCert' ../tripleo/overcloud-env.json | sed 's/\"//g') &gt; /root/eca.crt</p>
</li>
<li>
<p>If you generated these files from the installer data, you  need to verify that the files are well formed by entering:</p>

<codeblock>
<codeph>openssl rsa -noout -in /root/eca.key
openssl x509 -noout -in /root/eca.crt
</codeph>
</codeblock>

<p>If nothing displays, it means the file is readable and at least reasonably well formatted. Otherwise, a message similar to this one displays:</p>

<codeblock>
<codeph>unable to load certificate
140170817259168:error:0906D06C:PEM routines:PEM_read_bio:no start line:pem_lib.c:703:Expecting: TRUSTED CERTIFICATE".
</codeph>
</codeblock>
</li>
<li>
<p>From <codeph>/root</codeph> on the seed host, create the <codeph>helion-update-1.1-to-&lt;version&gt;</codeph> directory then change to this directory by entering:</p>

<codeblock>
<codeph>cd /root
mkdir helion-update-1.1-to-&lt;version&gt;
cd helion-update-1.1-to-&lt;version&gt; 
</codeph>
</codeblock>
</li>
<li>
<p>Copy the seed update script from the downloaded update kit on the seed VM to the seed host by entering:</p>

<codeblock>
<codeph>scp root@&lt;seed-vm-ip&gt;:/root/helion-update-1.1-to-&lt;version&gt;/tripleo/helion-update/seed_update/update_sd.sh .
</codeph>
</codeblock>
</li>
<li>
<p>Run the seed update script <codeph>update_sd.sh</codeph> from the seed host. Make sure you run <codeph>update_sd.sh</codeph> from the update directory you created: <codeph>/root/helion-update-1.1-to-&lt;version&gt;)</codeph> using the seed host's  <codeph>tripleo</codeph> root directory (<codeph>/root/tripleo</codeph>) and remote update root (extracted updated kit directory on the seed VM) as arguments.</p>

<p>For example:</p>

<codeblock>
<codeph>host_tripleo_root_directory = /root/tripleo
remote_update_tripleo_root_directory = /root/helion-update-1.1-to-&lt;version&gt;
</codeph>
</codeblock>
</li>
<li>
<p>To source the <codeph>kvm-custom-ips.json</codeph> file and run the update script enter:</p>

<codeblock>
<codeph>source ~/tripleo/tripleo-incubator/scripts/hp_ced_load_config.sh ~/tripleo/configs/kvm-custom-ips.json
./update_sd.sh &lt;host_tripleo_root_directory&gt; &lt;remote_update_tripleo_root_directory&gt; | tee seed_update.log
</codeph>
</codeblock>

<p>
<b>NOTE:</b> If you used any custom variables for the initial installation of the seed, you will need to export them again prior to executing the <codeph>update_sd.sh</codeph> script.</p>

<p>Once the update is completed, the <codeph>ssh host_key</codeph> of the seed will have changed.</p>
</li>
<li>
<p>To purge the known_hosts entries on the seed host machine, enter:</p>

<codeblock>
<codeph>ssh-keygen -R &lt;seed_ip_address&gt;
</codeph>
</codeblock>

<p>You will be prompted to re-add the seed host to your known_hosts file when you SSH into the seed to update the undercloud and overcloud.</p>
</li>
</ol>
</section>
<section id="uc-update-process"> <title>HP Helion OpenStack 1.1 to 1.1.X undercloud update process</title>
<p>The procedure will guide you through updating the undercloud by running the <codeph>update_uc.sh</codeph> shell script.</p>
<!-- copy note from seed host for devplat users -->
<p>
<b>Important</b>:
If you are updating to a release that includes new Orchestration (Heat) templates, you must apply these new templates to your system before you attempt to update it.</p>
<p>The update process does not automatically overwrite your Heat template, which would cause you to lose your modifications. If you have modified your Heat template, then you will need to update the new templates with these modifications.</p>
<p>To apply updated Heat templates, as root, run:</p>
<codeblock>
  <codeph>./tripleo/tripleo-incubator/scripts/hp_ced_installer.sh --update-undercloud 
</codeph>
</codeblock>
<p>To update your HP Helion OpenStack undercloud from v1.1 to v1.1.1:</p>
<ol>
<li>
<p>Log on to the seed VM.</p>

<codeblock>
<codeph>ssh root@&lt;seed-ip&gt;
</codeph>
</codeblock>
</li>
<li>
<p>Download the update kit onto the seed VM.</p>

<codeblock>
<codeph>cd /root
mkdir helion-update-1.1-to-&lt;version&gt;
cd helion-update-1.1-to-&lt;version&gt;
wget http://&lt;url&gt;/helion_ee_&lt;version&gt;.tgz
</codeph>
</codeblock>
</li>
<li>
<p>Extract the update kit, copy the <codeph>kvm-custom-ips.json</codeph>, and run the update script.</p>

<codeblock>
<codeph>tar -xvzf helion_ee_&lt;version&gt;.tgz
cp ~/tripleo/configs/kvm-custom-ips.json ~/helion-update-1.1-to-&lt;version&gt;/tripleo/configs/kvm-custom-ips.json
source tripleo/tripleo-incubator/scripts/hp_ced_load_config.sh tripleo/configs/kvm-custom-ips.json
cd tripleo/helion-update/undercloud_update
./update_uc.sh ~/helion-update-1.1-to-&lt;version&gt;/tripleo
</codeph>
</codeblock>
</li>
<li>
<p>Your HP Helion OpenStack undercloud is now updated. Confirm that update was successful by examining the <codeph>/var/log/ansible/ansible.log</codeph>.  This report should not list any unreachable nodes:</p>

<codeblock>
<codeph>PLAY RECAP ********************************************************************
10.23.67.141   : ok=122  changed=74   unreachable=0failed=0
10.23.67.143   : ok=37   changed=22   unreachable=0failed=0
10.23.67.144   : ok=122  changed=74   unreachable=0failed=0
10.23.67.145   : ok=103  changed=62   unreachable=0failed=0
10.23.67.146   : ok=37   changed=22   unreachable=0failed=0
10.23.67.147   : ok=60   changed=30   unreachable=0failed=0
10.23.67.148   : ok=60   changed=30   unreachable=0failed=0
</codeph>
</codeblock>
</li>
<li>
<p>Change file permissions by entering:</p>

<codeblock>
<codeph>chmod 775 /mnt/state/var/eon
chmod 775 /mnt/state/var/eon/data
</codeph>
</codeblock>
</li>
</ol>
</section>
<section id="oc-update-process"> <title>Helion v1.1 to v1.1.1 overcloud update process</title>
<p>This procedure explains how to update the overcloud using <codeph>update_oc.sh</codeph>.</p>
<!-- add note from seed host about devplat users not destroying their install -->
<!--
**Note for Helion Development Platform users**: The Helion Development Platform must be shut down before updating the overcloud. For instructions on shutting down the Development Platform, see [Updating Helion Development Platform](/helion/devplatform/1.2/upgrade/).
-->
<p>The v1.1.1 release includes new Orchestration (Heat) templates and new passthrough files. To apply these new templates to your system before updating the overcloud:</p>
<ol>
<li>
<p>Log on to the seed VM.</p>

<codeblock>
<codeph>ssh root@&lt;seed-ip&gt;
</codeph>
</codeblock>
</li>
<li>
<p>Download the update kit on the seed.</p>

<codeblock>
<codeph>cd /root
mkdir helion-update-1.1-to-&lt;version&gt;
cd helion-update-1.1-to-&lt;version&gt;
wget http://&lt;url&gt;/helion_ee_&lt;version&gt;.tgz
</codeph>
</codeblock>
</li>
<li>
<p>Extract the update kit:</p>

<codeblock>
<codeph>tar -xvzf helion_ee_&lt;version&gt;.tgz
</codeph>
</codeblock>
</li>
<li>
<p>Copy the new templates from the location where you unpacked the new kit to the original <codeph>/root/tripleo/tripleo-heat-templates</codeph> directory</p>

<codeblock>
<codeph>cp -r /root/helion-update-1.1-to-&lt;version&gt;/tripleo/tripleo-heat-templates /root/tripleo/
</codeph>
</codeblock>

<p>
<b>Note</b>: if you have made modifications to your original heat templates, you will need to re-apply them to the new templates. These instructions are included in this procedure.</p>
</li>
<li>
<p>Copy the new passthrough files from the location where you unpacked the new kit to the original <codeph>/root/tripleo/hp_passthrough</codeph> directory.</p>

<codeblock>
<codeph>cp -r /root/helion-update-1.1-to-&lt;version&gt;/tripleo/hp_passthrough /root/tripleo/
</codeph>
</codeblock>

<p>
<b>Note</b>: if you have made modifications to your original passthrough files, you will need to re-apply them to the new passthrough files. These instructions are included in this procedure.</p>
</li>
<li>
<p>Copy the new installation scripts from the location where you unpacked the new kit to the original <codeph>/root/tripleo/tripleo-incubator/scripts</codeph> directory</p>

<codeblock>
<codeph>cp -r /root/helion-update-1.1-to-&lt;version&gt;/tripleo/tripleo-incubator/scripts /root/tripleo/tripleo-incubator/
</codeph>
</codeblock>
</li>
<li>
<p>Copy the new <codeph>my.cnf</codeph> image element from the undercloud to each of the overcloud controller nodes. Log on to the seed and source the seed credentials. To do this, enter:</p>

<codeblock>
<codeph>root@hLinux:~# . stackrc
root@hLinux:~# undercloudip=`nova list | grep ctlplane | awk '{print $12}' | sed -e "s/.*=\\([0-9.]*\\).*/\1/"`
root@hLinux:~# scp heat-admin@${undercloudip}:/usr/libexec/os-apply-config/templates/mnt/state/etc/mysql/my.cnf /tmp/my.cnf
</codeph>
</codeblock>

<p>Source the undercloud creds and copy the file to each of the overcloud controllers by entering:</p>

<codeblock>
<codeph>TE_DATAFILE=/root/tripleo/ce_env.json . /root/tripleo/tripleo-incubator/undercloudrc
for ip in `nova list | grep controller[0-2] | awk '{print $12}' | sed -e "s/.*=\\([0-9.]*\\).*/\1/"`; do
echo $ip;
scp /usr/libexec/os-apply-config/templates/mnt/state/etc/mysql/my.cnf heat-admin@$ip:~
ssh heat-admin@$ip "sudo cp ~heat-admin/my.cnf /usr/libexec/os-apply-config/templates/mnt/state/etc/mysql/"
done
</codeph>
</codeblock>

<!--DOCS-1355 says this step makes copying to /tmp and /usr/libexec unnecessary and /tmp should be cleaned up. Referring back to Tom Q. --></li>
<li>
<p>Create the new <codeph>from-heat.conf</codeph> file on the seed using the following command:</p>

<codeblock>
<codeph>root@hLinux:~# cat &gt; from-heat.conf 
{{#stunnel}}
{{#connect_host}}
pid = /var/run/stunnel4/from-heat.pid
cert = {{stunnel.cert_location}}
key = {{stunnel.key_location}}
options = NO_SSLv2
options = NO_SSLv3
debug = 4
output = /var/log/stunnel4/helion_stunnel.log
syslog = no

{{#verify}}
verify = {{{verify}}}
{{/verify}}
{{#ports}}

[{{{name}}}]
accept = {{#accept_host}}{{{.}}}:{{/accept_host}}{{{accept}}}
connect = {{connect_host}}:{{connect}}
{{#client}}
client = {{.}}
{{/client}}
{{#timeout}}
TIMEOUTclose = {{.}}
{{/timeout}}
{{#session_cache}}
sessionCacheSize = {{.}}
{{/session_cache}}
{{#ciphers}}
ciphers = {{{.}}}
{{/ciphers}}
{{/ports}}
{{/connect_host}}
{{/stunnel}}
</codeph>
</codeblock>
</li>
<li>
<p>Copy the new stunnel image element to <codeph>/usr/libexec/os-apply-config/templates/etc/stunnel/from-heat.conf</codeph> on each of the overcloud controllers. Log on to the seed and source the undercloud credentials and copy the file to each of the overcloud controllers by entering:</p>

<codeblock>
<codeph>TE_DATAFILE=/root/tripleo/ce_env.json . /root/tripleo/tripleo-incubator/undercloudrc
for ip in `nova list | grep controller[0-2] | awk '{print $12}' | sed -e "s/.*=\\([0-9.]*\\).*/\1/"`; do
echo $ip;
scp from-heat.conf heat-admin@$ip:~
ssh heat-admin@$ip "sudo cp ~heat-admin/from-heat.conf /usr/libexec/os-apply-config/templates/etc/stunnel/"
ssh heat-admin@$ip "sudo /usr/local/bin/os-apply-config  --validate"
done
</codeph>
</codeblock>
</li>
<li>
<p>Run <codeph>update-overcloud</codeph> to apply the new templates and passthrough files by logging on to the seed as root and 
changing to the original /root directory and running:</p>

<codeblock>
<codeph>cd /root
source tripleo/tripleo-incubator/scripts/hp_ced_load_config.sh tripleo/configs/kvm-custom-ips.json
./tripleo/tripleo-incubator/scripts/hp_ced_installer.sh --update-overcloud
</codeph>
</codeblock>

<p>
<b>Note</b>: If you defined any custom variables for the initial installation of the overcloud, you need to export them again before executing the hp_ced_installer.sh script. For example:</p>

<codeblock>
<codeph>root@hLinux:~# OVERCLOUD_COMPUTESCALE=5
OVERCLOUD_NEUTRON_DVR=True
OVERCLOUD_NTP_SERVER=10.22.33.44 
./tripleo/tripleo-incubator/scripts/hp_ced_installer.sh --update-overcloud --skip-demo |&amp; tee /roo/tq_up_oc_1.log
</codeph>
</codeblock>

<p>The above command should apply the new templates and passthrough files. However, because the images have not yet been updated, the process should end in failure with the message:</p>

<codeblock>
<codeph>++ wait_for 30 10 nova service-list --binary nova-compute '2&gt;/dev/null' '|' grep 'enabled.*\ up\ '
Timing out after 300 seconds:
COMMAND=nova service-list --binary nova-compute 2&gt;/dev/null | grep enabled.*\ up\
OUTPUT=
root@hLinux:~#
</codeph>
</codeblock>

<p>This is expected behavior for KVM-based systems.</p>
</li>
<li>
<p>Log onto each overcloud controller in turn and restart MySQL and stunnel. This ensures that they are listening on the correct ports. To restart MySQL, enter:</p>

<codeblock>
<codeph>root@overcloud-ce-controller-controller0-xoni6co2hxes:~# service mysql stop
[ ok ] Stopping MySQL (Percona XtraDB Cluster): mysqld.
root@overcloud-ce-controller-controller0-xoni6co2hxes:~# service mysql start
[ ok ] Starting MySQL (Percona XtraDB Cluster) database server: mysqld . . . .[....] SST in progress, setting sleep higher: mysqld ..
root@overcloud-ce-controller-controller0-xoni6co2hxes:~# service mysql status
[info] Percona XtraDB Cluster up and running.
</codeph>
</codeblock>

<p>To restart stunnel, enter:</p>

<codeblock>
<codeph>root@overcloud-ce-controller-controller0-xoni6co2hxes:~# service stunnel4 stop
Stopping SSL tunnels: [stopped: /etc/stunnel/from-heat.conf] stunnel.
root@overcloud-ce-controller-controller0-xoni6co2hxes:~# service stunnel4 start
Starting SSL tunnels: [Started: /etc/stunnel/from-heat.conf] stunnel.
root@overcloud-ce-controller-controller0-xoni6co2hxes:~# service stunnel4 status
SSL tunnels status: /etc/stunnel/from-heat.conf: running
root@overcloud-ce-controller-controller0-xoni6co2hxes:~#
</codeph>
</codeblock>

<p>Verify that MySQL is now listening on port 3307 by entering:</p>

<codeblock>
<codeph>root@overcloud-ce-controller-controller0-xoni6co2hxes:~# netstat -napd | grep 3307 | grep LIST
tcp        0      0 127.0.0.1:3307          0.0.0.0:*               LISTEN      32049/mysqld    
root@overcloud-ce-controller-controller0-xoni6co2hxes:~# 
</codeph>
</codeblock>

<p>
<b>Warning</b>: Do not proceed with the following steps if MySQL is not listening on port 3307.</p>

<p>
<b>Note</b>: The Nova service-list will report that overcloud services are 'down'. At this stage, this is expected behavior. (You may need to wait for approximately 10 minutes to allow <codeph>os-apply-config</codeph> to complete running on each controller node before <codeph>nova service-list</codeph> returns this information.)</p>
</li>
<li>
<p>Apply the following edit to the pre-flight_check.yml file:</p>

<codeblock>
<codeph>root@hLinux:~# sed -i 's/status=started/state=started/g' ~/helion-update-1.1-to-&lt;version&gt;/tripleo/helion-update/tripleo-ansible/playbooks/pre-flight_check.yml
</codeph>
</codeblock>
</li>
<li>
<p>Add the 61-sleep playbook by creating the 61-sleep file as follows:</p>

<codeblock>
<codeph>root@hLinux:~# cat &gt; ~/helion-update-1.1-to-&lt;version&gt;/tripleo/helion-update/tripleo-ansible/playbooks/files/61-sleep  
#!/bin/bash
set -x
echo 'Short sleep'
sleep 20
date
</codeph>
</codeblock>
</li>
<li>
<p>Update the playbook by entering:</p>

<codeblock>
    root@hLinux:~# cp -p ~/helion-update-1.1-to/tripleo/helion-update/tripleo-ansible/playbooks/step_run_occ.yml ~/helion-update-1.1-to-/tripleo/helion-update/tripleo-ansible/playbooks/step_run_occ.yml.orig
    root@hLinux:~# cat &amp;lt; ~/helion-update-1.1-to-
/tripleo/helion-update/tripleo-ansible/playbooks/step_run_occ.yml 
    # Copyright (C) 2014 Hewlett-Packard Development Company, L.P.
    #
    # Licensed under the Apache License, Version 2.0 (the "License");
    # you may not use this file except in compliance with the License.
    # You may obtain a copy of the License at
    #
    #    http://www.apache.org/licenses/LICENSE-2.0
    #
    # Unless required by applicable law or agreed to in writing, software
    # distributed under the License is distributed on an "AS IS" BASIS,
    # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either 
    # express or implied.
    # See the License for the specific language governing permissions and
    # limitations under the License.
    ---
    - name: Remove os-collect-config disable sentinel file
      file: path=/mnt/state/disable-os-collect-config state=absent
      sudo: yes
    - name: "Setting default fact to run os-collect-config"
      set_fact: test_bypass_os_collect_config="False"
    - name: "Evaluate if os-collect-config needs to be run"
      command: grep -q -i "Completed phase migration" /var/log/upstart/os-collect-config.log
      register: test_did_os_collect_config_complete
      ignore_errors: yes
      when: online_upgrade is not defined
    - name: "Setting fact to bypass os-collect-config if applicable"
      set_fact: test_bypass_os_collect_config="True"
      when: online_upgrade is not defined and test_did_os_collect_config_complete.rc == 0
    - name: "SLEEP20"
      copy:
        dest: /opt/stack/os-config-refresh/configure.d/61-sleep
        src: files/61-sleep
        owner: root
        group: root
        mode: 0755
    - name: "Execute os-collect-config"
      command: os-collect-config --force --one
      when: test_bypass_os_collect_config != true







</codeblock>
</li>
<li>
<p>To verify that the new file is correct, please run:</p>

<codeblock>
<codeph>root@hLinux:~# python -c 'import yaml; f=open("/root/helion-update-1.1-to-&lt;version&gt;/tripleo/helion-update/tripleo-ansible/playbooks/step_run_occ.yml", "r"); yaml.safe_load(f);'
root@hLinux:~# echo $?
0
</codeph>
</codeblock>
</li>
<li>
<p>Run the update script:</p>

<codeblock>
<codeph>source tripleo/tripleo-incubator/scripts/hp_ced_load_config.sh tripleo/configs/kvm-custom-ips.json
cd helion-update-1.1-to-&lt;version&gt;/tripleo/helion-update/overcloud_update
./update_oc.sh ~/helion-update-1.1-to-&lt;version&gt;/tripleo |&amp; tee update_oc.log
</codeph>
</codeblock>

<p>Your HP Helion OpenStack overcloud is now updated.</p>
</li>
<li>
<p>Confirm that update was successful by examining the <codeph>/var/log/ansible/ansible.log</codeph>.  This report should not list any unreachable nodes:</p>

<codeblock>
<codeph>PLAY RECAP ********************************************************************
10.23.67.141   : ok=122  changed=74   unreachable=0failed=0
10.23.67.143   : ok=37   changed=22   unreachable=0failed=0
10.23.67.144   : ok=122  changed=74   unreachable=0failed=0
10.23.67.145   : ok=103  changed=62   unreachable=0failed=0
10.23.67.146   : ok=37   changed=22   unreachable=0failed=0
10.23.67.147   : ok=60   changed=30   unreachable=0failed=0
10.23.67.148   : ok=60   changed=30   unreachable=0failed=0
</codeph>
</codeblock>
</li>
</ol>
</section>
<section id="post-update-cleanup"> <title>Post update cleanup</title>
<p>After you have successfully updated the overcloud, there are some files that you should delete. Perform the following steps on all nodes in the overcloud (including controller nodes, Swift nodes, Nova compute nodes, etc.) To find and delete these files:</p>
<ol>
<li>
<p>Stop the <codeph>os-collect-config</codeph> service by entering:</p>

<codeblock>
<codeph>service os-collect-config stop
</codeph>
</codeblock>
</li>
<li>
<p>Find the files named <codeph>host_metadata.js*</codeph> by entering:</p>

<codeblock>
<codeph>find / -name host_metadata.json* -ls
</codeph>
</codeblock>
</li>
<li>
<p>Delete the files by entering:</p>

<codeblock>
<codeph>find / -name host_metadata.json* -delete
</codeph>
</codeblock>
</li>
<li>
<p>Verify that the files are all gone by entering:</p>

<codeblock>
<codeph>find / -name host_metadata.json* -ls
</codeph>
</codeblock>

<p>There should be no files found.</p>
</li>
<li>
<p>Restart the service by entering:</p>

<codeblock>
<codeph>service os-collect-config start
</codeph>
</codeblock>
</li>
</ol>
<!--
**Note**: Restart the Helion Development Platform VMs after updating the overcloud. For instructions on restarting Development Platform VMS, go to [Restarting Helion Development Platform control plane VMs](#HDPstart).
-->
<!-- replace with link to /helion/devplatform/1.2/upgrade/ -->
<!-- ###Updating the overcloud manually

This procedure allows you to update your overcloud installation from HP Helion OpenStack v1.1 to Helion v1.1.1 by executing the update commands directly and not relying on the update scripts. (HP Helion Development Platform users must shutdown before updating the overcloud. When the update is complete, the Development Platform VMs need to be restarted.) 

To update your overcloud:

1. Log on to the seed
 
        ssh root@<seed-ip>

2. Create the new Helion update directory, change to it, and download the update kit on the seed by entering:
 
        cd /root
        mkdir helion-update-1.1-to-<version>
        cd helion-update-1.1-to-<version>
        wget http://<url>/helion_ee_1.1.X.tgz

3. Untar the file by entering:

        tar -xvzf helion_ce_1.1.X.tgz

4. Log onto the seed and change to the downloaded directory bu entering:

        ssh root@<seed-ip>
        cd helion-update-1.1-to-<version>

5. Source the undercloud credentials by entering:

        export TRIPLEO_ROOT=~/tripleo
        export TE_DATAFILE=~/tripleo/ce_env.json
        source ~/tripleo/tripleo-incubator/undercloudrc

6. Download the image service (Glance) images locally by entering:
 
        OLD_BUILD_NO=$(glance image-show overcloud-compute-vmlinuz \
                       | grep "Property 'build_no'" | awk '{print $5};')
        mkdir build-$OLD_BUILD_NO
        cd build-$OLD_BUILD_NO
        for image in $(glance image-list | awk '{print $4};' | grep -);do
            glance image-download -??-file ./$image $image;
        done

7. Remove old images from Glance once saved locally by entering: 

        for image in $(glance image-list | awk '{print $4}' | grep -);do
            glance image-delete  $image;done

8. Upload new images to Glance by entering:

        BUILD_NO=$(cat /root/helion-update-1.1-to-<version>/tripleo/ce_env.json  | grep build_number | awk '{print $2}')
        /root/helion-update-1.1-to-<version>/tripleo/tripleo-incubator/scripts/load-image \
          -d /root/helion-update-1.1-to-<version>/tripleo/images/overcloud-compute-$BUILD_NO.qcow2
        /root/helion-update-1.1-to-<version>/tripleo/tripleo-incubator/scripts/load-image \
          -d /root/helion-update-1.1-to-<version>/tripleo/images/overcloud-control-$BUILD_NO.qcow2
        /root/helion-update-1.1-to-<version>/tripleo/tripleo-incubator/scripts/load-image \
          -d /root/helion-update-1.1-to-<version>/tripleo/images/overcloud-swift-$BUILD_NO.qcow2
        glance image-create -??-name bm-deploy-kernel -??-is-public True \
          -??-disk-format aki < /root/helion-update-1.1-to-<version>/tripleo/images/deploy-ramdisk-ironic.kernel
        glance image-create -??-name bm-deploy-ramdisk -??-is-public True \
          -??-disk-format ari < /root/helion-update-1.1-to-<version>/tripleo/images/deploy-ramdisk-ironic.initramfs

9. Update image names and set build metadata by entering:
  
        cd /root/helion-update-1.1-to-<version>/tripleo/tripleo-incubator/scripts/
        ./set-sherpa-metadata overcloud-compute-$BUILD_NO
        ./set-sherpa-metadata overcloud-control-$BUILD_NO
        ./set-sherpa-metadata overcloud-swift-$BUILD_NO

10. Update the TripleO Ansible playbook by entering:

        mv /opt/stack/triple-ansible /opt/stack/triple-ansible-1.1
        cp -r /root/helion-update-1.1-to-<version>/tripleo/helion-update/tripleo-ansible/ /opt/stack/

Refer to the Ansible update README for more details on running the play.

    source /opt/stack/venvs/ansible/bin/activate
    cd /opt/stack/tripleo-ansible/
    ./scripts/inject_nova_meta.bash
    ./scripts/populate_image_vars
    ansible-playbook -vvvv -M library/cloud -i plugins/inventory/heat.py -u heat-admin playbooks/pre-flight_check.yml
    ansible-playbook -vvvv -u heat-admin -i plugins/inventory/heat.py playbooks/update_cloud.yml
 -->
<!-- see Patrick's Development Platform update doc 


### Stopping HP Helion Development Platform services<a name="HDPstop"></a> 
 
Before stopping HP Helion Development Platform services, check that the HP Helion Development Platform service control plane is in healthy state (this is required for Trove and Marketplace). 

To run the HP Helion Development Platform scripts, perform the following steps:

1. Set `dev_plat_service` to the name of the service (Trove/Marketplace).
1. Set `skip_ha_checks` if the service control plane is not running in HA mode. 
1. Provide path to the SSH private key that can be used to connect to the service control plane VMs.
1. Source credentials for the Nova account which was used to set up the HPD service by entering:

        cd /root/helion-update-1.1-to-<version>/tripleo/helion-update/tripleo-ansible 
        ansible-playbook -??-extra-vars dev_plat_service=<service> -??-private-key <path to SSH private key> -??-skip_ha_checks=True -u heat-admin -i plugins/inventory/dev_platform_heat.py playbooks/dev-platform/dev_platform_pre_check.yml 
   


1. If service is in healthy state, stop it by entering (if the service is not in a healthy state, contact customer support): 
  
        ansible-playbook -??-extra-vars dev_plat_service=<service> -??-private-key <path to SSH private key> -u heat-admin -i plugins/inventory/dev_platform_heat.py 
        playbooks/dev-platform/dev_platform_stop.yml
 
1. Stop the Helion Development Platform control plane VMs by entering:

        scripts/dev-platform/stop_dev_platform_instances.bash 
   
You can now run the update script.

### Restarting HP Helion Development Platform control plane VMs<a name="HDPstart"></a> 


After running update on the overcloud, the VMs running on the Compute hosts are in a shutdown state, so you need to first start the HP Helion Development Platform service control plane VMs. To do this: 


1. Source the credentials for the Nova account which was used to set up the HPD service by entering: 

            cd /root/helion-update-1.1-to-<version>/tripleo/helion-update/tripleo-ansible 
            scripts/dev-platform/restart_dev_platform_instances.bash 
   

1. Since you have already sourced correct overcloud Nova credentials, you can simply verify the status of HPD service VMs by running the nova list command and checking the status field. 



1. To start the HP Helion Development Platform service, enter:

        ansible-playbook -??-extra-vars dev_plat_service=<service> -??-private-key <path to SSH private key> -u heat-admin -i plugins/inventory/dev_platform_heat.py playbooks/dev-platform/dev_platform_start.yml 

 -->
</section>
<section id="updating-the-hp-version-type"> <title>Updating the HP-version type</title>
<p>It is a good practice to copy the <codeph>HP_Helion_Version</codeph> type to the Tripleo directory so that it reflects the version that was updated. This lets other administrators know that the environment has been updated.</p>
<p>To replace the version file with the latest,  on the seed VM, enter:</p>
<codeblock>
  <codeph>cp ~/helion-update-1.1-to-&lt;version&gt;/tripleo/HP_Helion_Version ~/tripleo/
</codeph>
</codeblock>
</section>
<section id="validating-the-update"> <title>Validating the update</title>
<p>This section explains how you should validate your update.</p>
</section>
<section id="validating-the-overcloud-controller-update"> <title>Validating the overcloud controller update</title>
<p>After the update, the overcloud controller should be back online without any errors. You should be able to  SSH to the Nova compute node from the seed host.<!--A BR tag was used here in the original source.-->
You should be able to open the Horizon dashboard as well as execute lifecycle operations with any instance already deployed in the overcloud.</p>
<p>The node should re-join the MySQL Cluster,<!--A BR tag was used here in the original source.-->
HAProxy should show the servers as online, the<!--A BR tag was used here in the original source.-->
instances deployed before the update should still be up and accessible. It should be possible to deploy new instances and ping them and SSH to them.</p>
<p>Icinga should show that all the monitors are green for the update node. To check Icinga, go to the <b>Icinga Dashboard</b> (http://&lt;Undercloud_IP&gt;/icinga/). 
<!-- modified the following link. 
[http://<undercloudip>/icinga/](http://undercloudip/icinga)-->Log in with the credential: icingaadmin/icingaadmin</p>
<p>Do the following:</p>
<ol>
<li>Enable the HAProxy status page and check if the all the services from the node are green (meaning they are up/running). </li>
<li>SSH to the overcloud Management controller.</li>
<li>Edit the file <codeph>/etc/haproxy/haproxy.cfg</codeph> to add at the end of the file <codeph>mode http</codeph>. (Make this edit in the listen <codeph>haproxy.stats</codeph>:1993 section.)</li>
<li>Restart HAProxy (<codeph>service haproxy restart</codeph>).</li>
<li>Open the status page and check all the services. <!-- ([http://managementcontroller](http://managementcontroller.ip:1993).) --></li>
<li>
<p>Make sure that the following commands are working after the update:</p>

<ul>
<li>keystone user-list</li>
<li>nova list   </li>
<li>neutron net-list </li>
<li>glance image-list  </li>
<li>os-refresh-config</li>
<li>os-collect-config  </li>
</ul>
</li>
<li>
<p>Use <codeph>ifconfig</codeph> to check that all your networks still exist after the update.</p>
</li>
<li>Use <codeph>ovs-vsctl show</codeph> to check that all bridges and ports still exist after the update.   </li>
</ol>
</section>
<section id="validating-the-compute-node"> <title>Validating the compute node</title>
<p>After an update, verify that instances deployed before  the update are still up and accessible.
You should be able to SSH to the Nova Compute node from  the seed host. You should be able to ping and SSH to  new instances.</p>
<p>The <codeph>os-refresh-config</codeph> command needs to be working  after  an update.</p>
<p>Use <codeph>ifconfig</codeph> to check that all your networks still exist after the update.<!--A BR tag was used here in the original source.-->
Use <codeph>ovs-vsctl show</codeph> to check that all bridges and ports still exist after the update.</p>
</section>
<section id="validating-swift"> <title>Validating Swift</title>
<p>After the update:</p>
<ul>
<li>Check that the images are still there.</li>
<li>Confirm that you can SSH to the Nova compute node from seed host.</li>
<li>Verify that you can load new images.</li>
<li>Check that you can deploy new instances with a new image.</li>
</ul>
<p>The <codeph>os-refresh-config</codeph> command needs to be working  after  an update.</p>
</section>
<section id="validating-virtual-storage-appliances"> <title>Validating Virtual Storage Appliances</title>
<p>After an update:</p>
<ul>
<li>Check if the node can join the cluster (CMC).</li>
<li>Verify that you can SSH to the Nova compute node from  the seed host.</li>
<li>Check that the volumes are still present.</li>
<li>Verify that you can create new volumes.</li>
<li>Verify that you can delete old volumes.</li>
<li>Verify that you can  attach/detach volumes.</li>
<li>Check that the </li>
<li>version of the Glance image used to update is the same by SSHing to the undercloud node and run:

<ul>
<li>
<codeph>glance image-list</codeph>
</li>
<li>
<codeph>nova list</codeph>
</li>
<li>
<codeph>nova show &lt;instance id&gt;</codeph>
</li>
</ul>
</li>
</ul>
<p>Compare the Image ID from <codeph>glance image-list</codeph> with 
<codeph>nova show</codeph>.</p>
</section>
<section id="troubleshooting-an-update"> <title>Troubleshooting an update</title>
<p>This section explains how to fix common problems  that might arise when performing an update to Helion.</p>
</section>
<section id="backup-fails"> <title>Backup fails</title>
<p>If you see the error message:</p>
<codeblock>
  <codeph>+ echo 'ERROR: Backup of seed failed!!!!'
ERROR: Backup of seed failed!!!!
+ exit 1
</codeph>
</codeblock>
<p>Your backup attempt has failed. To recover from this state, you need to get <codeph>rabbitmq</codeph>, <codeph>mysql</codeph> and <codeph>Openstack</codeph> services running again by running the seed recovery script. to do this, enter:</p>
<codeblock>
  <codeph>ssh root@&lt;seed-ip&gt;
cd helion-update-1.1-to-&lt;version&gt;/tripleo/helion-update/seed_update
./seed_recover.sh
</codeph>
</codeblock>
</section>
<section id="backup-failed-due-to-seed-host-running-out-of-disk-space"> <title>Backup failed due to seed host running out of disk space</title>
<p>If your backup fails because the disk lacks sufficient space you will note that the backup sequence froze or failed. You will also see that the disk space utilization check of the seed host returns 100%. You can check disk utilization by entering:</p>
<codeblock>
  <codeph>df -h /
</codeph>
</codeblock>
<p>To fix this problem:</p>
<ol>
<li>
<p>Remove the failed backup from the <codeph>/tmp</codeph> folder by entering:</p>

<codeblock>
<codeph>rm -r -f /tmp/backp_root
</codeph>
</codeblock>
</li>
<li>
<p>Restart MySQL and RabbitMQ by entering:</p>

<codeblock>
<codeph>service mysql restart
service rabbitmq-server restart
</codeph>
</codeblock>
</li>
<li>
<p>Run <codeph>os-collect-config</codeph> by entering:</p>

<codeblock>
<codeph>servie os-collect-config stop
os-collect-config --force
service os-collect-config start
</codeph>
</codeblock>

<p>If this command returns an error, RabbitMQ may be in a bad state as a result of running out of disk space. If executing <codeph>os-collect-config</codeph> errors with RabbitMQ, enter:</p>

<codeblock>
<codeph>rabbitmqctl stop_app
rabbitmqctl reset
service rabbitmq-server stop
</codeph>
</codeblock>
</li>
<li>
<p>Once RabbitMQ has been reset, re-attempt step 3 which will re-configure RabbitMQ and restart services.  If the <codeph>rabbitmqctl reset</codeph> command does not work, use the <codeph>rabbitmqctl force_reset</codeph> command.</p>
</li>
</ol>
</section>
<section id="seed-host-update-fails-noting-unable-to-ping-192021"> <title>Seed host update fails noting unable to ping 192.0.2.1</title>
<p>If your seed host update fails, your deployment will NOT utilize the 192.0.2.0/24 demo IP network range. You will see in your log:</p>
<codeblock>
  <codeph>"Waiting for seed host to configure"
</codeph>
</codeblock>
<p>Pings to host 192.0.2.1 will time out with the following message:</p>
<codeblock>
  <codeph>Timing out after 10 seconds:
COMMAND=ping -c 1 192.0.2.1
OUTPUT=PING 192.0.2.1 (192.0.2.1) 56(84) bytes of data.
</codeph>
</codeblock>
<p>The likely reason for this failure is the installer update of the seed host image has failed as the configuration that was used during the install was not available or passed on to the installer to perform the seed host update operation.  As a result, the installer has indicated that it has failed, although the seed host has likely rebooted without issue.</p>
<p>To fix this problem:</p>
<ol>
<li>
<p>Identify the following:</p>

<ul>
<li>Expected IP address of the seed host.</li>
<li>Location of the backup folder.  It should be  at <codeph>/root/helion-update-1.1-to-&lt;version&gt;/backup*</codeph>.  For example:<codeph>/root/helion-update-1.1-to-1.1.74/backup-0/</codeph>
</li>
</ul>
</li>
<li>
<p>Verify that you can ping the IP address of the seed host by entering:</p>

<codeblock>
<codeph>ping -c 1 &lt;seed IP address&gt;
</codeph>
</codeblock>
</li>
<li>
<p>Run the restore script directly by entering:</p>

<codeblock>
<codeph>/root/helion-update-1.1-to-&lt;version&gt;/helion-update/seed_update/seed_update.sh --restore-seed &lt;backup directory&gt; --ip-address &lt;seed IP address&gt;
</codeph>
</codeblock>

<p>For example:</p>

<codeblock>
<codeph>/root/helion-update-1.1-to-&lt;version&gt;/helion-update/seed_update/seed_update.sh --restore-seed /root/helion-update-1.1-to-&lt;version&gt;/backup-0/ --ip-address 192.2.0.1
</codeph>
</codeblock>
</li>
<li>
<p>The restoration command will take some time, but once completed you should be able to log in to the seed host.  To verify that the node is in working status, enter:</p>

<codeblock>
<codeph>source /root/stackrc &amp;&amp;
nova list &amp;&amp;
glance image-list &amp;&amp;
heat stack-list &amp;&amp;
neutron net-list
</codeph>
</codeblock>
</li>
</ol>
</section>
<section id="retrying-failed-actions"> <title>Retrying failed actions</title>
<p>In some cases, steps may fail because some components may still be initializing and not yet be ready for
use.
In this event, you have two options: to re-attempt or resume playbook executions.</p>
<ol>
<li>Use the Ansible <codeph>ansible-playbook</codeph> command with the  <codeph>--start-at-task="TASK NAME"</codeph> option. This command allows resumption of a playbook, when used with the <codeph>-l limit</codeph> option.</li>
<li>Use the Ansible <codeph>ansible-playbook</codeph> command with the <codeph>--step</codeph> option. This command allows you to confirm  each task before it is executed by Ansible.</li>
</ol>
</section>
<section id="a-node-goes-to-error-state-during-rebuild"> <title>A node goes to ERROR state during rebuild</title>
<p>A node can go into an error state due to network errors or a temporary overload of the undercloud. 
This can happen from time to time due to network errors or a temporary overload of the undercloud. In this case, the <codeph>nova list</codeph> command returns node in ERROR. To fix this:</p>
<ul>
<li>Make sure your hardware is in working order.</li>
<li>Verify that approximately 20% of the disk space on the Ironic server node is free.</li>
<li>
<p>Get the image ID of the machine in question using <codeph>nova show</codeph>
</p>

<codeblock>
<codeph>nova show $node_id
</codeph>
</codeblock>
</li>
<li>
<p>Manually rebuild by running:</p>

<codeblock>
<codeph>nova rebuild --preserve-ephemeral $node_id $image_id
</codeph>
</codeblock>
</li>
</ul>
</section>
<section id="a-node-times-out-after-rebuild"> <title>A node times out after rebuild</title>
<p>While rare, there is the possibility that something unexpected happened during a rebuild
and the host has failed to reboot. When this happens, you will get this error Message:</p>
<codeblock>
  <codeph>msg: Timeout waiting for the server to come up.. Please check manually
</codeph>
</codeblock>
<p>To fix this problem, follow the steps detailed in: "A node goes to ERROR state during rebuild".</p>
</section>
<section id="mysql-cli-configuration-file-is-missing"> <title>MySQL CLI configuration file is missing</title>
<p>Should the post-rebuild restart fail, the cause might be that the MySQL CLI configuration file is missing. If you have this issue, attempts to access the MySQL CLI command return:</p>
<codeblock>
  <codeph>ERROR 1045 (28000): Access denied for user 'root'@'localhost' (using password: NO)
</codeph>
</codeblock>
<p>To fix this problem:</p>
<ul>
<li>
<p>Verify that the MySQL CLI config file stored on the state drive is present and has content.  To display the contents, run:</p>

<codeblock>
<codeph>sudo cat /mnt/state/root/metadata.my.cnf
</codeph>
</codeblock>
</li>
<li>
<p>If the file is empty, retrieve current metadata and update  the config files on disk by running:</p>

<codeblock>
<codeph>sudo os-collect-config --force --one --command=os-apply-config
</codeph>
</codeblock>
</li>
<li>
<p>Verify that the MySQL CLI config file is present in the root user directory by running:</p>

<codeblock>
<codeph>sudo cat /root/.my.cnf
</codeph>
</codeblock>
</li>
<li>
<p>If that file does not exist, or is empty, you have two options.</p>

<ol>
<li>
<p>Add the following to your MySQL CLI command line:</p>

<codeblock>
<codeph>--defaults-extra-file=/mnt/state/root/metadata.my.cnf
</codeph>
</codeblock>
</li>
<li>
<p>Or copy the configuration from the state drive by entering:</p>

<codeblock>
<codeph>sudo cp -f /mnt/state/root/metadata.my.cnf /root/.my.cnf
</codeph>
</codeblock>
</li>
</ol>
</li>
</ul>
</section>
<section id="mysql-fails-to-start-after-retrying-the-update"> <title>MySQL fails to start after retrying the update</title>
<p>If the update was aborted or failed during the Update sequence before a single MySQL controller was operational, MySQL will fail to start upon retrying. In this case, you will see the following error messages:</p>
<codeblock>
  <codeph>* `msg: Starting MySQL (Percona XtraDB Cluster) database server: mysqld . . . . The server quit without updating PID file (/var/run/mysqld/mysqld.pid)`

* `stderr: ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/var/run/mysqld/mysqld.sock' (111)`

* `FATAL: all hosts have already failed  - aborting`

* Update automatically aborts.
</codeph>
</codeblock>
<p>
  <b>WARNING:</b>
</p>
<p>The command <codeph>/etc/init.d/mysql bootstrap-pxc</codeph>,  mentioned below, 
      should only ever be executed when an entire MySQL cluster is down, and
      then only on the last node to have been shut down.  Running this command
      on multiple nodes will cause the MySQL cluster to enter a split brain
      scenario effectively breaking the cluster which will result in
      unpredictable behavior.</p>
<p>To fix this problem:</p>
<ul>
<li>
<p>Verify that NTP is working.</p>
</li>
<li>
<p>Use the <codeph>nova list</codeph> command to determine the IP address of the controller0 node, then SSH into it by entering:</p>

<codeblock>
<codeph>ssh heat-admin@$IP
</codeph>
</codeblock>
</li>
<li>
<p>Verify that MySQL is down by running (as root) the <codeph>mysql</codeph> client. It should fail:</p>

<codeblock>
<codeph>sudo mysql -e "SELECT 1"
</codeph>
</codeblock>
</li>
<li>
<p>Attempt to restart MySQL in case another cluster node is online. This should fail in this error state. However, if it succeeds your cluster should again be operational and you can skip the next step:</p>

<codeblock>
<codeph>sudo /etc/init.d/mysql start
</codeph>
</codeblock>
</li>
<li>
<p>Start MySQL in single node bootstrap mode by entering:</p>

<codeblock>
<codeph>sudo /etc/init.d/mysql bootstrap-pxc
</codeph>
</codeblock>
</li>
</ul>
</section>
<section id="mysqlperconagalera-is-out-of-sync"> <title>MySQL/Percona/Galera is out of sync</title>
<p>OpenStack is configured to store all of its states in a multi-node, synchronous replication Percona XtraDB Cluster database, which uses Galera for replication. This database must be in sync and have the full
complement of servers before updates can be performed safely.</p>
<p>The problem is update fails with errors about Galera and/or MySQL being <codeph>Out of Sync</codeph>.</p>
<p>To fix this issue:</p>
<ul>
<li>
<p>Use the  <codeph>nova list</codeph> command to determine the IP address of controller0 node, then SSH to it by entering:</p>

<codeblock>
<codeph>ssh heat-admin@$IP
</codeph>
</codeblock>
</li>
<li>
<p>Verify that replication is out of sync by entering:</p>

<codeblock>
<codeph>sudo mysql -e "SHOW STATUS like 'wsrep_%'"
</codeph>
</codeblock>
</li>
<li>
<p>Stop MySQL by entering:</p>

<codeblock>
<codeph>sudo /etc/init.d/mysql stop
</codeph>
</codeblock>
</li>
<li>
<p>Verify that MySQL is down by running (as root) the <codeph>mysql</codeph> client. It should fail:</p>

<codeblock>
<codeph>sudo mysql -e "SELECT 1"
</codeph>
</codeblock>
</li>
<li>
<p>Start controller0 MySQL in single-node bootstrap mode by entering:</p>

<codeblock>
<codeph>sudo /etc/init.d/mysql bootstrap-pxc
</codeph>
</codeblock>
</li>
<li>
<p>On the remaining controller nodes observed to be having issues, get their IP addresses using the  <codeph>nova list</codeph>  command and log in to them by entering:</p>

<codeblock>
<codeph>ssh heat-admin@$IP
</codeph>
</codeblock>
</li>
<li>
<p>Verify that replication is out of sync by entering:</p>

<codeblock>
<codeph>sudo mysql -e "SHOW STATUS like 'wsrep_%'"
</codeph>
</codeblock>
</li>
<li>
<p>Stop MySQL by entering:</p>

<codeblock>
<codeph>sudo /etc/init.d/mysql stop
</codeph>
</codeblock>
</li>
<li>
<p>Verify that MySQL is down by running (as root) the <codeph>mysql</codeph> client. It should fail:</p>

<codeblock>
<codeph>sudo mysql -e "SELECT 1"
</codeph>
</codeblock>
</li>
<li>
<p>Start MySQL so it attempts to connect to controller0 by entering:</p>

<codeblock>
<codeph>sudo /etc/init.d/mysql start
</codeph>
</codeblock>
</li>
</ul>
<p>If restarting MySQL fails, then the database is most certainly out of sync and the MySQL error logs, located at <codeph>/var/log/mysql/error.log</codeph>, will need to be checked.  In this case, never attempt to restart MySQL with <codeph>sudo /etc/init.d/mysql bootstrap-pxc</codeph> as it will bootstrap the host as a single-node cluster thus worsening what already appears to be a split-brain scenario. If you need help with this matter, contact HP support.</p>
</section>
<section id="mysql-node-appears-to-be-the-last-node-in-a-cluster-error"> <title>MysQL "Node appears to be the last node in a cluster" error</title>
<p>This error occurs when one of the controller nodes does not have MySQL running.
The playbook has detected that the current node is the last running node,
although based on its sequence, it should not be the last node.  As a result the
error is thrown and update is aborted.</p>
<p>The full message is:</p>
<codeblock>
  <codeph>Galera Replication. Node appears to be the last node in a cluster; cannot safely proceed unless overridden via `single_controller`setting. See README.rst
</codeph>
</codeblock>
<p>To fix this problem:</p>
<ul>
<li>
<p>Run the <codeph>pre-flight_check.yml</codeph> playbook.  It will attempt to restart MySQL on each node in the <codeph>Ensuring MySQL is running</codeph> step.  If that step succeeds, you should be able to re-run the playbook and not encounter this <codeph>Node appears to be last node in a cluster</codeph> error.</p>
</li>
<li>
<p>If pre-flight_check fails to restart MySQL, review the MySQL logs (<codeph>/var/log/mysql/error.log</codeph>) to determine why the other nodes are not restarting.</p>
</li>
</ul>
</section>
<section id="ssh-connectivity-is-lost"> <title>SSH connectivity is lost</title>
<p>Ansible uses SSH to communicate with remote nodes. In heavily loaded, single
host virtualized environments, SSH can lose connectivity.  It should be noted
that similar issues in a physical environment may indicate issues in the
underlying network infrastructure.</p>
<p>When Ansible loses SSH connectivity causing an update attempt to fail, you will see the following output:</p>
<codeblock>
  <codeph>fatal: [192.0.2.25] =&gt; SSH encountered an unknown error. The
output was: OpenSSH_6.6.1, OpenSSL 1.0.1i-dev xx XXX xxxx
debug1: Reading configuration data /etc/ssh/ssh_config debug1:
/etc/ssh/ssh_config line 19: Applying options for * debug1:
auto-mux: Trying existing master debug2: fd 3 setting
O_NONBLOCK mux_client_hello_exchange: write packet: Broken
pipe FATAL: all hosts have already failed  - aborting
</codeph>
</codeblock>
<p>To fix this problem, you can generally re-run the playbook to complete the upgrade, unless SSH connectivity is lost while all MySQL nodes are down. (See 'MySQL fails to start upon retrying update' to correct this issue.)</p>
<p>Early Ubuntu Trusty kernel versions have known issues with KVM which severely impact SSH connectivity to instances. To avoid this issue, Test hosts should have a minimum kernel version of 3.13.0-36-generic.</p>
<p>The update steps, running as root, are:</p>
<codeblock>
  <codeph>apt-get update
apt-get dist-upgrade
reboot
</codeph>
</codeblock>
<p>If you continue to encounter this issue in a physical environment, check the network infrastructure for errors.</p>
<p>Similar error messages may occur with long running processes, such as database creation/upgrade steps.  These cases will generally have partial program execution log output immediately before the broken pipe message visible. Should this be the case, Ansible and OpenSSH may need to have their configuration files tuned to meet the needs of the environment.</p>
<p>Consult the Ansible configuration file to see available connection settings ssh_args, timeout, and possibly pipelining. To see this file, enter:</p>
<codeblock>
  <codeph>    https://github.com/ansible/ansible/blob/release1.7.0/examples/ansible.cfg
</codeph>
</codeblock>
<p>As Ansible uses OpenSSH, consult the <codeph>ssh_config</codeph> manual, in paricular the <codeph>ServerAliveInterval</codeph> and <codeph>ServerAliveCountMax</codeph> options.</p>
</section>
<section id="postfix-fails-to-reload"> <title>Postfix fails to reload</title>
<p>Occasionally the postfix mail transfer agent will fail to reload because it is not running when the system expects it to be running.</p>
<p>Confirm that this is the case by examining  the <codeph>/var/log/upstart/os-collect-config.log</codeph> for an indication that <codeph>service postfix reload</codeph> failed.</p>
<p>To fix this issue, start postfix by entering:</p>
<codeblock>
  <codeph>    sudo service postfix start
</codeph>
</codeblock>
</section>
<section id="ephemeral-certificates-location"> <title>Ephemeral certificates location</title>
<!-- DOCS-1101, DOCS-1257 -->
<p>The default ephemeral certificate location is <codeph>/root</codeph>. This works for normal Helion installations. If you have specified another location, when you update Helion, you must specify the certificate location as <codeph>/root</codeph>. For example, the correct environment variable location and file name is:</p>
<codeblock>
  <codeph>EPHEMERAL_CA_KEY_FILE=/root/eca.key
EPHEMERAL_CA_CERT_FILE=/root/eca.crt
</codeph>
</codeblock>
</section>
<section id="apache2-fails-to-start"> <title>Apache2 fails to start</title>
<p>Apache2 requires several self-signed SSL certificates to be properly configured but because of earlier failures in the setup process these certificaties may not have been configured correctly. If this is the case, you will see the following error message:</p>
<codeblock>
  <codeph>* failed: [192.0.2.25] =&gt; (item=apache2) =&gt; {"failed": true, "item": "apache2"}
* msg: start: Job failed to start
</codeph>
</codeblock>
<p>You will also note the following symptoms:</p>
<ul>
<li>the Apache2 service fails to start</li>
<li>the <codeph>/etc/ssl/certs/ssl-cert-snakeoil.pem</codeph> file is missing or empty.</li>
</ul>
<p>To fix this problem, re-run <codeph>os-collect-config</codeph> to reassert the SSL certificates by entering:</p>
<codeblock>
  <codeph>    sudo os-collect-config --force --one
</codeph>
</codeblock>
</section>
<section id="rabbitmq-still-running-when-restart-is-attempted"> <title>RabbitMQ still running when restart is attempted</title>
<p>There are certain system states that cause RabbitMQ to ignore normal kill signals. In these cases, RabbitMQ continues to run. You will notice that you have this issue when your attempts to start <codeph>rabbitmq</codeph> fail because it is already running.</p>
<p>To fix this problem, find any processes running as <codeph>rabbitmq</codeph> on the server, and kill them, forcibly if need be.</p>
</section>
<section id="instance-reported-with-status-as-shutoff-and-task-state-as-powering-on"> <title>Instance reported with status as "SHUTOFF" and task_state as "powering on"</title>
<p>When Nova Compute attempts to restart an instance when the Compute node is not ready, it is possible that Nova can enter a confused state where it thinks that an instance is starting when in fact the Compute node is doing nothing. You have reason to suspect that this is the case when:</p>
<ul>
<li>The command <codeph>nova list --all-tenants</codeph> reports instance(s) with STATUS of "SHUTOFF" and <codeph>task_state</codeph> is "powering on".</li>
<li>The instance does not respond to pings.</li>
<li>No instance appears to be running on the Compute node.</li>
<li>Nova hangs when retrieving logs or returns old logs from the previous boot.</li>
<li>A console session cannot be established.</li>
</ul>
<p>To fix this problem:</p>
<ul>
<li>
<p>Log in to a controller as root and enter:</p>

<p>
<codeph>source stackrc</codeph>
</p>
</li>
<li>
<p>Execute <codeph>nova list --all-tenants</codeph> to obtain instance ID(s)</p>
</li>
<li>Execute <codeph>nova show &lt;instance-id&gt;</codeph> on each suspected ID to identify suspected Compute nodes.</li>
<li>Log into the suspected Compute node(s) and execute:
 <codeph>os-collect-config --force --one</codeph>
</li>
<li>
<p>Return to the controller node that you were logged into previously, and using the instancce IDs obtained previously, take the following steps:</p>

<ol>
<li>Execute <codeph>nova reset-state --active &lt;instance-id&gt;</codeph>
</li>
<li>Execute <codeph>nova stop &lt;instance-id&gt;</codeph>
</li>
<li>Execute <codeph>nova start &lt;instance-id&gt;</codeph>
</li>
</ol>
</li>
</ul>
<ol>
<li>Once the above steps have been taken in order, you should see the instance status return to ACTIVE and the instance become accessible via the network.</li>
</ol>
</section>
<section id="state-drive-mnt-is-not-mounted"> <title>State drive <codeph>/mnt</codeph> is not mounted</title>
<p>In the rare event that an error occurred between the state drive being unmounted and the rebuild command being triggered, the <codeph>/mnt</codeph> volume on the instance upon which the rebuild command was executed will be in an unmounted state.</p>
<p>In this state, you will not be able to start MySQL and RabbitMQ. You will likely see these (pre-flight check) error messages:</p>
<codeblock>
  <codeph>failed: [192.0.2.24] =&gt; {"changed": true, "cmd":
"rabbitmqctl -n rabbit@$(hostname) status" stderr: Error:
unable to connect to node
'rabbit@overcloud-controller0-vahypr34iy2x': nodedown
</codeph>
</codeblock>
<p>Attempts to start MySQL or RabbitMQ  manually will fail and you will see:</p>
<codeblock>
  <codeph>start: Job failed to start
</codeph>
</codeblock>
<p>Upgrade attempts return with an error indicating:</p>
<codeblock>
  <codeph>TASK: [fail msg="Galera Replication, Node appears to be the last node in a cluster;  cannot safely proceed unless overridden via `single_controller` setting. See README.rst"]
</codeph>
</codeblock>
<p>If you run the <codeph>df</codeph> command, the return does not show a volume mounted as <codeph>/mnt</codeph>.</p>
<p>To fix this problem:</p>
<ul>
<li>
<p>Execute the <codeph>os-collect</codeph> config which will re-mount the state drive. This command may fail without additional intervention. However it should mount the state drive which is all that is needed to proceed to the next step. To run <codeph>os-collect-config</codeph>, enter:</p>

<codeblock>
<codeph>sudo os-collect-config --force --one
</codeph>
</codeblock>
</li>
<li>
<p>At this point, the <codeph>/mnt</codeph> volume should be visible in the output of the <codeph>df</codeph> command.</p>
</li>
<li>
<p>Start MySQL by entering:</p>

<codeblock>
<codeph>sudo /etc/init.d/mysqld start
</codeph>
</codeblock>
</li>
<li>
<p>If MySQL fails to start, and it has been verified that MySQL is not running on any controller nodes, then you will need to identify the Last node that MySQL was stopped on and consult the section "MySQL fails to start upon retrying update" for guidance on restarting the cluster.</p>
</li>
<li>
<p>Start RabbitMQ by entering:</p>

<codeblock>
<codeph>service rabbitmq-server start
</codeph>
</codeblock>
</li>
<li>
<p>If <codeph>rabbitmq-server</codeph> fails to start, then the cluster may be down. If this is the case, then the last node to be stopped will need to be identified and started before attempting to restart RabbitMQ on this node.</p>
</li>
<li>At this point, re-execute the pre-flight check, and proceed.</li>
</ul>
</section>
<section id="vms-do-not-shut-down-properly-during-upgrade"> <title>VMs do not shut down properly during upgrade</title>
<p>During the upgrade process, VMs on Compute nodes are shut down gracefully. If the VMs do not shut down, this can cause the upgrade to stop. If this is the case, you will see a playbook run which ends with a message similar to:</p>
<codeblock>
  <codeph>failed: [10.23.210.31] =&gt; {"failed": true}} msg: The ephemeral
storage of this system failed to be cleaned up properly and
processes or files are still in use. The previous ansible play
should have information to help troubleshoot this issue.
</codeph>
</codeblock>
<p>The output of the playbook run prior to this message contains a process listing and a listing of open files.</p>
<p>The state drive on the Compute node, <codeph>/mnt</codeph>, is still in use and cannot be unmounted. You can confirm this by entering:</p>
<codeblock>
  <codeph>    lsof -n | grep /mnt
</codeph>
</codeblock>
<p>To see which VMs are running, enter:</p>
<codeblock>
  <codeph>    virsh list
</codeph>
</codeblock>
<p>If <codeph>virsh list</codeph> fails, you may need to restart <codeph>libvirt-bin</codeph> or <codeph>libvirtd</codeph> depending on which process you are running. To restart, enter:</p>
<codeblock>
  <codeph>    service libvirt-bin restart
</codeph>
</codeblock>
<p>or</p>
<codeblock>
  <codeph>    service libvirtd restart
</codeph>
</codeblock>
<p>To fix this problem, you will have to intervene manually. You will need to determine why the VMs did not shut down properly, and resolve the issue.</p>
<p>You can forcely shutdown non-responsive VMs by entering:</p>
<codeblock>
  <codeph>virsh destroy &lt;id&gt;
</codeph>
</codeblock>
<p>Note that this can corrupt filesystems on the VM.</p>
<p>Resume the playbook run once the VMs have been shut down.</p>
</section>
<section id="instances-are-inaccessible-via-network"> <title>Instances are inaccessible via network</title>
<p>Upon restarting, it is possible that the virtual machine is unreachable due to Open vSwitch not being ready for the virtual machine networking. If this is the case, you will not be able to ping instances after a restart.</p>
<p>To fix this problem:</p>
<ul>
<li>
<p>Log into a controller node and execute:</p>

<codeblock>
<codeph>source /root/stackrc
</codeph>
</codeblock>
</li>
<li>
<p>Stop all virtual machines on a Compute node by entering:</p>

<codeblock>
<codeph>nova hypervisor-servers &lt;hostname&gt; 
</codeph>
</codeblock>

<p>and</p>

<codeblock>
<codeph>nova stop &lt;id&gt;
</codeph>
</codeblock>
</li>
<li>
<p>Log into the undercloud node and enter:</p>

<codeblock>
<codeph>source /root/stackrc
</codeph>
</codeblock>
</li>
<li>
<p>Obtain a list of nodes by entering:</p>

<codeblock>
<codeph>nova list
</codeph>
</codeblock>
</li>
<li>
<p>Execute <codeph>nova stop &lt;id&gt;</codeph> for the affected Compute node.</p>
</li>
<li>
<p>Once the compute node has stopped, execute <codeph>nova start &lt;id&gt;</codeph> to reboot the Compute node.</p>
</li>
</ul>
</section>
<section id="online-upgrade-fails-with-message-saying-glanceclient-is-not-found"> <title>Online upgrade fails with message saying glanceclient is not found</title>
<p>This problem occurs when you attempt to perform an online upgrade, However the playbook execution failed when you attempted to download the new image from Glance. You get a message that <codeph>glanceclient</codeph> was not found.</p>
<p>If you are attempting to execute the Ansible playbook on the seed host or undercloud node, source the Ansible virtual environment by entering:</p>
<codeblock>
  <codeph>  source /opt/stack/venvs/ansible/bin/activate
</codeph>
</codeblock>
<p>Once the Ansible virtual environment has been sourced, on the node from which you are attempting to execute Ansible, enter:</p>
<codeblock>
  <codeph>  sudo pip install python-glanceclient 
</codeph>
</codeblock>
</section>
<section id="online-upgrade-of-compute-node-failed"> <title>Online upgrade of Compute node failed</title>
<p>In the event that an online upgrade of a Compute node fails, you can recover the node utilizing a traditional rebuild.</p>
<p>The problem occurs when you perform an online upgrade. The result of which is that a Compute node cannot be logged into, or is otherwise in a non-working state.</p>
<p>To fix this issue, from the undercloud enter:</p>
<codeblock>
  <codeph>source /root/stackrc
</codeph>
</codeblock>
<p>Identify the instance ID of the broken Compute node using the <codeph>nova list</codeph> command.
Then stop the instance by entering:</p>
<codeblock>
  <codeph>nova stop &lt;instance-id&gt; 
</codeph>
</codeblock>
<p>Return to the host from which you ran the upgrade and re-run the playbook without the <codeph>-e online_upgrade=True</codeph> option.</p>
<p>You can also utilize the <codeph>-e force_rebuild=True</codeph>  option to force the instance to rebuild.</p>
</section>
<section id="ironic-fails-because-nodes-are-in-maintenance-mode"> <title>Ironic fails because nodes are in maintenance mode</title>
<p>During an update, nodes must NOT be in maintenance mode; otherwise Ironic returns an error message such as the following:</p>
<codeblock>
  <codeph>During sync_power_state, max retries exceeded for node 0677d7e8-2e2b-4b12-b426-4b2950d7a5f2, node state None does not match expected state 'power on'. Updating DB state to 'None' Switching node to maintenance mode.
</codeph>
</codeblock>
<p>If the command <codeph>ironic node-set-maintenance</codeph> fails to properly change nodes from maintenance mode, you must either update the database directly using:</p>
<codeblock>
  <codeph>mysql&gt; update nodes set maintenance=0;
</codeph>
</codeblock>
<p>or enter:</p>
<codeblock>
  <codeph> ironic node-update &lt;ironic node-id&gt; replace maintenance=False.
</codeph>
</codeblock>
</section>
<section id="envvarsignored"> <title>Environment variables being ignored</title>
<p>When running the <codeph>update_sd.sh</codeph> script to upgrade from HP Helion OpenStack v1.1 to v1.1.1,  the default 192.0.6.0/24 network is used instead of the networks that are defined by the <codeph>kvm-custom-ips.json</codeph> file.
The reason this happens is the person doing the update neglected to set the ENV variables before running the update command. These ENV variables were set when the system was installed and the same ENV variable set (with the same values) must be used for the update.</p>
<p>The following is an example of ENV variables:</p>
<codeblock>
  <codeph>SEED_NTP_SERVER=10.34.56.78 BRIDGE_INTERFACE=eth2
BM_NETWORK_SEED_IP=10.34.55.66
BM_NETWORK_CIDR=10.34.55.0/24
BM_NETWORK_GATEWAY=10.34.55.1
SEED_NAMESERVER=172.16.222.5 
./update_sd.sh /root/tripleo /root/helion-update-1.1-to-&lt;version&gt; | tee seed_update.log
</codeph>
</codeblock>
</section>
<section id="overcloud-error-message-ansible-host-key-checkingfalse"> <title>Overcloud error message: ANSIBLE_HOST_KEY_CHECKING=False</title>
<p>If, during the update-overcloud operation, you see the following failure:</p>
<codeblock>
  <codeph>    Attempting to connect to each host
    + ANSIBLE_HOST_KEY_CHECKING=False
    + ansible -o -i /opt/stack/tripleo-ansible/plugins/inventory/heat.py -u heat-admin -m ping all
    Traceback (most recent call last):
    File "/opt/stack/venvs/ansible/bin/ansible", line 194, in &lt;module&gt;
        (runner, results) = cli.run(options, args)
    File "/opt/stack/venvs/ansible/bin/ansible", line 112, in run
        inventory_manager = inventory.Inventory(options.inventory, vault_password=vault_pass)
    File "/opt/stack/venvs/ansible/local/lib/python2.7/site-packages/ansible/inventory/__init__.py", line 118, in __init__
        self.parser = InventoryScript(filename=host_list)
    File "/opt/stack/venvs/ansible/local/lib/python2.7/site-packages/ansible/inventory/script.py", line 49, in __init__
        self.groups = self._parse(stderr)
    File "/opt/stack/venvs/ansible/local/lib/python2.7/site-packages/ansible/inventory/script.py", line 57, in _parse
        self.raw  = utils.parse_json(self.data)
     File "/opt/stack/venvs/ansible/local/lib/python2.7/site-packages/ansible/utils/__init__.py", line 552, in parse_json
        results = json.loads(data)
    File "/usr/lib/python2.7/json/__init__.py", line 338, in loads
        return _default_decoder.decode(s)
    File "/usr/lib/python2.7/json/decoder.py", line 366, in decode
        obj, end = self.raw_decode(s, idx=_w(s, 0).end())
    File "/usr/lib/python2.7/json/decoder.py", line 384, in raw_decode
        raise ValueError("No JSON object could be decoded")
    ValueError: No JSON object could be decoded
</codeph>
</codeblock>
<p>then, you can check the source of the issue by:</p>
<codeblock>
  <codeph>    source /opt/stack/venvs/ansible/bin/activate

    (ansible)root@hLinux:~/helion-update-1.1-to-&lt;version&gt;/tripleo/helion-update/overcloud_update# ansible -vvv -o -i /opt/stack/tripleo-ansible/plugins/inventory/heat.py -u heat-admin -m ping all
    Traceback (most recent call last):
    File "/opt/stack/venvs/ansible/bin/ansible", line 194, in &lt;module&gt;
        (runner, results) = cli.run(options, args)
    File "/opt/stack/venvs/ansible/bin/ansible", line 112, in run
        inventory_manager = inventory.Inventory(options.inventory, vault_password=vault_pass)
    File "/opt/stack/venvs/ansible/local/lib/python2.7/site-packages/ansible/inventory/__init__.py", line 118, in __init__
        self.parser = InventoryScript(filename=host_list)
    File "/opt/stack/venvs/ansible/local/lib/python2.7/site-packages/ansible/inventory/script.py", line 49, in __init__
        self.groups = self._parse(stderr)
    File "/opt/stack/venvs/ansible/local/lib/python2.7/site-packages/ansible/inventory/script.py", line 57, in _parse
        self.raw  = utils.parse_json(self.data)
    File "/opt/stack/venvs/ansible/local/lib/python2.7/site-packages/ansible/utils/__init__.py", line 552, in parse_json
        results = json.loads(data)
    File "/usr/lib/python2.7/json/__init__.py", line 338, in loads
        return _default_decoder.decode(s)
    File "/usr/lib/python2.7/json/decoder.py", line 366, in decode
        obj, end = self.raw_decode(s, idx=_w(s, 0).end())
    File "/usr/lib/python2.7/json/decoder.py", line 384, in raw_decode
        raise ValueError("No JSON object could be decoded")
    ValueError: No JSON object could be decoded
    (ansible)root@hLinux:~/helion-update-1.1-to-&lt;version&gt;/tripleo/helion-update/overcloud_update# /opt/stack/tripleo-ansible/plugins/inventory/heat.py --list
    overcloud-ce-vsastorage0/6dd10abb-482e-4eaf-b061-158efe5f8a15 stack is incomplete, in state FAILED
    (ansible)root@hLinux:~/helion-update-1.1-to-&lt;version&gt;/tripleo/helion-update/overcloud_update# 
</codeph>
</codeblock>
<p>In this case, the overcloud-ce-vsastorage0 node was in the FAILED state for Heat.</p>
<p>
  <xref type="section" href="#topic51655"> Return to Top </xref>
</p>
<!-- ===================== horizontal rule ===================== -->
</section>
</body>
</topic>
