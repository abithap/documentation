<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="topic_qkg_l4v_bt">
  <title>How to Modify Input Model</title>
  <body>
    <p>This section describes the configuration of  Swift services with the various type of input
      model supported in this release.<ul id="ul_c5c_gjm_dt">
        <li><xref href="#topic_qkg_l4v_bt/node-type" format="dita">Node Type</xref></li>
        <li><xref href="#topic_qkg_l4v_bt/config-swift-services" format="dita">Configuration of
            Swift Services in the Mid Sized Cloud Model</xref></li>
        <li><xref href="#topic_qkg_l4v_bt/allocate-server" format="dita">Allocating
          Server</xref></li>
        <li><xref href="#topic_qkg_l4v_bt/allocating-disk-drives" format="dita">Allocating Disk
            Drives</xref></li>
        <li><xref href="#topic_qkg_l4v_bt/allocation-network" format="dita">Allocating
            Network</xref></li>
        <li><xref href="#topic_qkg_l4v_bt/ring-specification" format="dita">Ring
            Specifications</xref></li>
      </ul></p>
    <section id="node-type"><b>Node Type</b></section>
    <p>There are two type of nodes that is associated with Swift services. They are as follows:</p>
    <ul>
      <li><b>Proxy, Container, Account (PAC) Node</b>: This node runs the Swift-proxy,
        Swift-account, and Swift-container services. The Swift-proxy service processes API requests
        and directs them to the Swift-account, Swift-container or Swift-object services for
        processing. The Swift-account and Swift-container handle requests to accounts and containers
        respectively.</li>
      <li><b>Object (OBJ) Node</b>: This node runs the Swift-object service. The Swift-object
        service handles requests for objects.</li>
    </ul>
    <section id="config-swift-services"><b>Configuration of Swift Services in the Mid Sized Cloud Model</b><p>In the example
        mid sized cloud model, the Swift services are configured in the following
          <codeph>yml</codeph> files:</p></section>
    <ul>
      <li><u>data/control_plane.yml</u><p>
          <ul>
            <li>The Proxy, Container, Account node type is assigned to a dedicated <i>cluster of
                nodes</i> as specified in the <b>swpac</b> cluster in <codeph>yml</codeph> file.
              These nodes are dedicated only to Swift services.</li>
            <li>The Object node type is assigned to a dedicated <i>resource nodes</i> group as
              specified in the <b>swobj</b> group in <codeph>yml</codeph> file.</li>
            <li>Request to Swift are directed to a virtual IP address (VIP) that is managed by a
              cluster as specified in the <b>core</b> cluster in <u>data/control_plane.yml</u> file.
              The requests are then directed on the MGMT network to the Swift-proxy service on one
              of the Proxy, Container, Account nodes.</li>
          </ul>
        </p></li>
      <li>
        <p><u>data/disks_swpac.yml</u></p>
        <p>The Proxy, Container, Account node type uses two disk drives to store account and
          container databases which is specified in this file.</p>
      </li>
      <li>
        <p>
          <u>data/disks_swobj.yml</u></p>
        <p>The Object node type uses two disk drives to store account and container databases which
          is specified in this file.</p>
      </li>
      <li><u>config/swift/rings.yml</u><p>Swift account, container, and object storage are managed
          by Swift using a data structure known as a <i>ring</i>. This <codeph>yml</codeph> file
          provides the specification of the rings. For more information on ring specification, refer
          to <xref href="#topic_qkg_l4v_bt/ring-specification" format="dita">ring
            specification.</xref></p></li>
      <li>
        <p>
          <u>data/network_groups.yml</u></p>
        <p>This <codeph>yml</codeph> file specifies the MGMT and Swift networks. The Swift-proxy
          service uses the Swift network to communicate with the other Swift services and among
          themselves.</p>
      </li>
      <li> The Swift-proxy service uses other cloud services as follows. Both of these are
        configured in the example to run on the <b>core</b> cluster:<ul>
          <li>Swift validates token by making requests to the Keystone service</li>
          <li>Swift caches tokens and other data using the memcached service</li>
        </ul></li>
    </ul>
    <section id="allocate-server"><b>Allocating Server</b></section>
    <p>In the <codeph>data/baremetal.yml</codeph> file, you specify the roles for servers. For
      example, a Swift Proxy, Container, Account (PAC) node is specified as
      follows:<codeblock>baremetal_servers:
.
.
.
- node_name: swpac1
  node_type: ROLE-SWPAC
  pxe_mac_addr: 26:67:3e:49:5a:a7
  pxe_interface: eth2
  pxe_ip_addr: 192.168.10.12
  ilo_ip: 192.168.9.12
  ilo_user: admin
  ilo_password: password
.
.
.</codeblock></p>
    <p>In the above example a <b>node_type</b> is assigned as <b>ROLE-SWPAC</b>, which indicates
      that this node is Swift Proxy, Container, Account (PAC) node.</p>
    <note>If you don't use Cobbler to install your servers then you must create
        <codeph>data/servers.yml</codeph> and configure nodes on the file.</note>
    <p>The number of servers is specified in the <b>swpac </b>cluster in the
        <codeph>data/control_plane.yml</codeph> file. In the following example, the number of
      servers is set to 3.</p>
    <p>
      <codeblock>control-planes:
    - name: ccp
      region-name: region1
      common-service-components:
        - logging-producer
        - monasca-agent
        - stunnel
      clusters:
.
.
.
- id: "4"
          name: swpac
          server-role: ROLE-SWPAC
          member-count: <b>3</b>
          service-components:
            - ntp-client
            - swift-proxy
            - swift-account
            - swift-container
            - swift-ring-builder
            - swift-client
.
.
.</codeblock>
    </p>
    <p>You can increase the <b>member-count</b> value so that more Proxy, Account, Container nodes
      are used. When you increase the <b>member-count</b> you must also increase the number of nodes
      with the <codeph>ROLE-SWPAC</codeph> role in the <codeph>data/baremetalConfig.yml</codeph> (or
        <codeph>data/servers.yml</codeph>) file.
      &lt;<!--<b>if we dont increase the nodes in the data/barementalconfig.yml file then what error message do we get?</b>>--></p>
    <p>A Swift Object (OBJ) node is specified in <codeph>data/baremetalConfig.yml</codeph> file as
      shown in the following
      example:<codeblock>baremetal_servers:
.
.
.
- node_name: swobj1
  node_type: ROLE-SWOBJ
  pxe_mac_addr: 8b:f6:9e:ca:3b:78
  pxe_interface: eth2
  pxe_ip_addr: 192.168.10.20
  ilo_ip: 192.168.9.20
  ilo_user: admin
  ilo_password: password
.
.
.</codeblock></p>
    <p>In the above example a <codeph>node_type</codeph> is assigned as <codeph>ROLE_SWOBJ</codeph>,
      which indicates that this node is Swift Object node.</p>
    <p>To specify the number of Swift Object servers add more servers of type
        <codeph>ROLE_SWOBJ</codeph> to the <codeph>data/baremetalConfig.yml</codeph> (or
        <codeph>data/servers.yml</codeph>) file. <note>For Swift Object, the
          <codeph>member-count</codeph> is automatically determined. It is recommended not to change
        or increment the <codeph>member count</codeph>.</note></p>
    <section id="allocating-disk-drives"><b>Allocating Disk Drives </b><p>The disk model describes
        the number of disk present on a particular server and its usage. The examples include
        several disk models. The disk model used by any given server is determined as follows:</p><p>
        <ul>
          <li>The <codeph>node_type</codeph> of server as specified in
              <codeph>data/baremetalConfig.yml</codeph> (or <codeph>data/servers.yml</codeph>)
            determines the role of the server.</li>
          <li>There is a specification for each sever role in the
              <codeph>data/server_roles.yml</codeph> file. This specifies a disk model for a given
            server role. For example, for the <codeph>ROLE-SWOBJ</codeph>, the disk model is called
              <codeph>DISK_SET_SWOBJ</codeph>. <p><codeph>data/server_roles.yml</codeph> file
              specifies the disk model as
            follows:</p><codeblock>server-roles:   


    - name: ROLE-SWPAC 16 
      interface-model: INTERFACE_SET_SWPAC 17 
      disk-model: DISK_SET_SWPAC 18 


    - name: ROLE-SWOBJ 24 
      interface-model: INTERFACE_SET_SWOBJ 25 
      disk-model: DISK_SET_SWOBJ 
</codeblock></li>
          <li>The Object server uses the <codeph>DISK_SET_SWOBJ</codeph> disk model in the
              <codeph>data/disks_swobj.yml</codeph> file.</li>
          <li>
            <p>The Proxy, Account, Container servers use the <codeph>DISK_SET_SWPAC</codeph> disk
              model in the <codeph>data/disks_swpac.yml</codeph> file.</p>
          </li>
        </ul>
      </p><p>The structure of the <codeph>DISK_SET_SWOBJ</codeph> and
          <codeph>DISK_SET_SWPAC</codeph> disk models are similar. The
          <codeph>data/disks_swpac.yml</codeph> file specifies the disk mode as
        follows:<codeblock>disk-models:
- name: DISK_SET_SWPAC
  volume-groups:
    - name: hlm-vg
      physical-volumes:
        - /dev/sda5
      ...
      consumer:
         name: os
 
  device-groups:
    - name: swiftpac
      devices:
        - name: /dev/sdb
        - name: /dev/sdc
      consumer:
        name: swift
        attrs:
          rings:
            - account
            - container</codeblock></p><p>In
        the above example, Swift uses <codeph>/dev/sdb</codeph> and <codeph>/dev/sdc</codeph> disk
        drives and operating system uses <codeph>/dev/sda</codeph> disk drives.</p><p>The disk model
        has the following fields:</p>
      <dl>
        <dlentry>
          <dt>device-groups</dt>
          <dd>There can be several device groups. This allows different sets of
          disks to be used for different purposes. In this example, there is only one entry in the
          device groups list.</dd>
        </dlentry>
        <dlentry>
          <dt>name</dt>
          <dd>This is an arbitrary name for the device group. The name must be unique,
            but is not otherwise used.</dd>
        </dlentry>
        <dlentry>
          <dt>devices</dt>
          <dd>This is a list of devices allocated to the device group. For Swift,
            these devices must meet the criteria as explained in <xref
              href="#topic_qkg_l4v_bt/requirement-disk-device">Requirements for a disk
              device</xref></dd>
        </dlentry>
        <dlentry>
          <dt>consumer</dt>
          <dd>This specifies the service that uses the device group. A
            <codeph>name</codeph> field containing <b>swift</b> indicates that the device group is
            used by Swift. If the name field contains other values, the Swift system will ignore the
            device group.</dd>
        </dlentry>
        <dlentry>
          <dt>attrs.rings</dt>
          <dd>This lists the rings that the devices are allocated to. In this
            example, the disk model is used by Proxy, Account, Container nodes, so the <b>account</b>
            and <b>container</b> rings are listed. In the <codeph>DISK_SET_SWOBJ</codeph> disk model,
            <codeph>object-0</codeph> ring is listed. It would be an error to add the
            <codeph>object-0</codeph> ring to the <codeph>DISK_SET_SWPAC</codeph> disk model because
            the Swift-object service does not run on PAC node.</dd>
        </dlentry>
      </dl>
<p>You should review the disk devices in the disk model before making any changes to the
        existing model. See <i><xref href="#topic_qkg_l4v_bt/making-changes-to-swift" format="dita"
            >Making Changes to a Swift Disk Model</xref></i>. </p>
      
      <sectiondiv id="requirement-disk-device"><b><i> Requirements
            for a Disk Device </i></b><p>Disk devices listed in the <b>devices</b> list must meet
          the following criteria:</p><ul id="ul_awk_kmg_2t">
          <li>The disk device must exist on the server. For example, if you add
              <codeph>/dev/sdx</codeph> to a server with only 3 devices then the deploy process
            fails.</li>
          <li>The disk device must be unpartitioned or have a single partition that uses the whole
            drive.</li>
          <li>Do not have a label on the partition.
            <!--&lt;<b>is there any example we can provide?</b>>--></li>
          <li>Do not have an XFS filesystem that contains a filesystem
            label.<!-- &lt;<b>is there any example we can provide?</b>>--></li>
          <li>If the disk drive is already labeled as described above, the
              <codeph>swiftlm-drive-provision</codeph> process will assume that the drive has
            valuable data and refuses to use or modify the drive.
            <!--&lt;<b>any procedure we should include in the troubleshooting section so that the user can clean the drives and run install the process? </b>>--></li>
        </ul></sectiondiv>
      <sectiondiv id="making-changes-to-swift"><p><i><b>Making Changes to a Swift Disk
            Model</b></i></p><p> There are several reasons for changing the disk model as
        follows:</p><ul>
        <li>It is advisable that if you have additional drives available then you can add the drives
          to the devices list.</li>
        <li>The disk devices listed in the example disk model have different names on your servers.
          This may be due different harware drives. Edit the disk model
            (<codeph>DISK_SET_SWPAC</codeph> or <codeph>DISK_SET_SWOBJ</codeph>) and change the
          device names to the correct
          names.<!--<table frame="all" rowsep="0" colsep="0"
            id="table_rwv_rpm_dt">
            <tgroup cols="1">
              <colspec colname="c1" colnum="1" colwidth="1.0*"/>
              <tbody>
                <row>
                  <entry>Useful tips:</entry>
                </row>
              </tbody>
            </tgroup>
          </table>--></li>
        <li>If you prefer a different disk drive than the one listed in the model. For example, if
            <codeph>/dev/sdb</codeph> and <codeph>/dev/sdc</codeph> are slow hard drives and you
          have SDD drives available in <codeph>/dev/sdd</codeph> and <codeph>/dev/sde</codeph>. In
          this case, delete <codeph>/dev/sdb</codeph> and <codeph>/dev/sdc</codeph> and replace with
            <codeph>/dev/sdd</codeph> and <codeph>/dev/sde</codeph>.</li>
      </ul></sectiondiv></section>
    <section id="allocation-network"><title>Changing Network Allocations</title><p>Changing network configurations is not recommended.
        network configurations.</p></section>
    <section id="ring-specification"><title>Ring Specifications</title><p>The ring maps the logical names
        of data to locations on a particular disks. There is a separate ring for account database,
        account database, and individual objects, but each ring works similarly. Ring-builder is a
        utility which builts and manually manages the rings. The ring-builder also keeps its own
        builder file with the ring information and additional data required to build future
        rings.</p><p>The rings are specified in the input model using the <b>ring-specifications</b>
        key. There is an entry for each distinct Swift system – hence the Keystone region name is
        specified in the <b>region-name</b> key of the input model.</p><p>In the example, a
        ring-specification is mentioned in <codeph>config/swift/rings.yml</codeph> file. The sample
        file of ring-specification is as follows:</p><p>
        <codeblock>ring-specifications:
    - region-name: region1
      rings:
        - name: account
          display-name: Account Ring
          min-part-time: 16
          partition-power: 17
          replication-policy:
            replica-count: 3
        - name: container
          display-name: Container Ring
          min-part-time: 16
          partition-power: 17
          replication-policy:
            replica-count: 3
        - name: object-0
          display-name: General
          default: yes
          min-part-time: 16
          partition-power: 17
          replication-policy:
            replica-count: 3</codeblock>
      </p><p>The above smaple file shows that the <b>region1</b> has three rings as follows: </p>
      <dl>
        <dlentry>
          <dt>Account ring</dt>
          <dd>You must always specify a ring called <b>account</b>. The
            account ring is used by Swift to store metadata about the projects in your system. In
            Swift, a Keystone project maps to a Swift account. The <codeph>display-name</codeph> is
            informational and not used. </dd>
        </dlentry>
        <dlentry>
          <dt>Container ring</dt>
          <dd>You must always specify a ring called "container". The
            <codeph>display-name</codeph> is informational and not used.</dd>
        </dlentry>
        <dlentry>
          <dt>Object ring</dt>
          <dd>It is also known as Storage Policy. You must always specify a
            ring called "object-0". It is possible to have multiple object rings, which is known as
            <i>storage policies</i>. But in this release we support only one storage policy, i.e.
            object-0. The <codeph>display-name</codeph> is the name of the storage policy and can be
            used by users of the Swift system when they create containers. It allows them to specify
            the storage policy that the container uses. In the example, the storage policy is called
            <b>General</b>.</dd>
        </dlentry>
        <dlentry>
          <dt>Replica count</dt>
          <dd>The recommended value for the <codeph>replica-count</codeph>
            attribute is 3. Three copies of data are kept to provide resiliency and
            availability.<p>The purpose of the <b>min-part-time, partition-power,
              replication-policy</b> and<b>replica-count</b> are described in the following
              section.</p></dd>
        </dlentry></dl>
        <note>The swift-ring-builder will enforce a minimum of 16 hours between ring rebuilds.</note></section>
      <section id="making-changes-ring-specification"><title>Making Changes to the Ring Specifications</title>
    <p>The ring specification provided in the example model contains adequate configuration
        details to deploy Swift. However, there may be reasons for changing the model as
        follows:</p><ul>
        <li>Your system has more servers and disks than in the mid sized cloud example. Therefore,
          you may require to pick a different value for <codeph>partition-power</codeph>. The
          appropriate value is related to the number of disk drives you allocate for the account and
          container storage. We recommend that you use the same drives for both the account and
          container rings. Hence, the <codeph>partition-power</codeph> value should be the same. For
          more information about selecting an appropriate value, refer to <i><xref
              href="#topic_qkg_l4v_bt/selecting-a-partition-power">Selecting a
              Partition Power</xref></i>.</li>
        <li>You require high resiliency or availability and want a higher replica count. The
            <codeph>replica-count</codeph>  is normally 3 (i.e., Swift will keep three copies of
          accounts and containers). It is <b>recommended</b> not to change the value to lower than
          3.</li>
        <li>You require a different name for the Storage Policy. If so, edit the
            <codeph>display-name</codeph> of the object-0 ring. This must be a single word
          (character such as underscore or dash are allowed). This is visible to your end
          users.</li>
        <li>Your system has more servers and disks than in the mid sized cloud example and the time
          to replicate data during a ring rebuild is longer than 16 hours. However, this time is
          system-dependent so you will be unable to figure out the appropriate value for
            <codeph>min-part-power</codeph> until you have more experience with your
            system.<p>Therefore, during the initial deployment of Swift, we suggest not to make
            changes in <b>min-part-time</b>.</p></li>
      </ul></section>
    <section id="selecting-a-partition-power"><b>Selecting a Partition Power</b></section>
    <p>
      <ul>
        <li>
          <p>Partition power is used to distribute the data uniformly across drives in a Swift
            nodes. It also defines the storage cluster capacity. You must set the part partition
            power value based on the total amount of storage you expect your entire ring to use. </p>
          <p>When storing an object, the object storage system hashes the name. This hash results in
            a hit on a partition (so a number of different object names result in the same partition
            number). Generally, the partition is mapped to available disk drives. With a replica
            count of 3, each partition is mapped to three different disk drives. The hashing
            algorithm used hashes over a fixed number of partitions. The partition-power attribute
            determines the number of partitions you have. </p>
        </li>
      </ul>
    </p>
    <p>Select a partition power for a given ring that is appropriate to the number of disk drives
      you allocate to the ring for the following reasons: </p>
    <ul>
      <li>If you use a high partition power and have a few disk drives, each disk drives will have
        1000s of partitions. With too many partitions, audit and other processes in the Object
        Storage system cannot "walk" the partitions in a reasonable time and updates will not occur
        in a timely manner.</li>
      <li>If you use a low partition power and have many disk drives, you will have 10s (or maybe
        only one) partition on a drive. The Object Storage system does not use size when hashing to
        a partition – it hashes the name.
          <!--If sizes and names are randomly distributed, it is normally expected a random, even,  distribution of sizes within a partition. However, this may not happen (i.e., two neighboring partitions may have different sizes). --><p>With
          many partitions on a drive, a large partition is cancelled out by a smaller partition so
          the overall drive usage is similar. However, with very small numbers of partitions, the
          uneven distribution of sizes can be reflected in uneven disk drive utilization (so one
          drive becomes full while a neighboring drive is empty).</p></li>
    </ul>
    <p>An ideal number of partitions per drive is 100. If you know the number of drives, select part
      power approximately to 100 partition per drive. Usually, you install a system with a specific
      number of drives and add drives as your required for storage grows. However, you cannot change
      the value of the partition power. Hence you must select a value that is a compromise between
      current and planned capacity. </p>
  <note type="important">If you are installing a small capacity system and you need to grow to a
      very large capacity but you cannot fit within any of the ranges in the table, please seek help
      from Professional Services to plan your system.</note>
    <p>There are a few additional factors that can help or mitigate the fixed nature of the
      partition power:</p>
    <ul>
      <li>Account and container storage represents a small fraction (typically 1%) of your object
        storage needs. Hence, you can select a smaller partition power (relative to object ring
        partition power) for the account and container rings. </li>
    </ul>
    <p>
      <ul id="ul_shv_s2g_dt">
        <li>For object storage, you can add additional storage policies (i.e., another object ring).
          When you have reached capacity in an existing storage policy, you can add a new storage
          policy with a higher partition power &lt;<b>can we add new storage policy in Beta as we
            claim to support one?</b>> (because you now have more disk drives in your system). This
          means that you can install your system with a small partition power appropriate to a small
          number of initial disk drives. Later when you have many disk drives, the new storage
          policy can have a higher value appropriate to the larger number of drives.</li>
      </ul>
    </p>
    <p> However, when you continue to add storage capacity you should note that existing containers
      continue to use their original storage policy. Hence, the additional objects must be added to
      new containers to take advantage of the new storage policy.</p>
    <p>Use the following table to select an appropriate partition power for each ring. The partition
      power of a ring cannot be changed so it is important to select an appropriate value. This
      table is based on a replica count of 3. But, your replica count is different or you are unable
      to find your system in the table then refer to <xref
        href="#topic_qkg_l4v_bt/calculate-number-of-partition" format="dita">Calculating Numbers of
        Partitions</xref> for information of selecting a partition power. </p>
    <p> The table assumes that when you first deploy the system (<b>by system- are we referring
        Swift here?</b>) you have a small number of drives (the minimum column in the table) and
      later you add drives. </p>
    <p>
      <note>
        <ul id="ul_nlg_zgg_dt">
          <li>You use the total number of drives. For example, if you have 3 servers, each with two
            drives, the total number of drives is 6.</li>
          <li>The lookup should be done separately for each of the account, container and object
            rings. Since account and containers represent approximately 1-2% of object storage, you
            will probably use fewer drives for the account and container rings (i.e., you will have
            fewer Proxy, Account, Container (PAC) servers) – hence you your object rings may have a
            higher partition power.</li>
          <li>The largest anticipated number of drives imposes a limit in how a few drives (the
            minimum) you can have. See <xref href="#topic_qkg_l4v_bt/calculate-number-of-partition"
              format="dita">Calculating Numbers of Partitions</xref>. This means that if you
            anticipate significant growth, your initial system can be small, but under certain
            limit.</li>
          <li>The table assumes that disk drives are the same size. The actual size of a drive is
            not significant.</li>
        </ul>
        <p><b>Partition Power</b><table frame="all" rowsep="1" colsep="1" id="table_fz3_2hg_dt">
            <tgroup cols="3">
              <colspec colname="c1" colnum="1" colwidth="1.0*"/>
              <colspec colname="c2" colnum="2" colwidth="1.0*"/>
              <colspec colname="c3" colnum="3" colwidth="1.0*"/>
              <thead>
                <row>
                  <entry>Number of drive during deployment (minimum)</entry>
                  <entry>Number of drives in largest anticipated system (maximum)</entry>
                  <entry>Recommended Part Power</entry>
                </row>
              </thead>
              <tbody>
                <row>
                  <entry>6</entry>
                  <entry>5,000</entry>
                  <entry>14</entry>
                </row>
                <row>
                  <entry>12</entry>
                  <entry>10,000</entry>
                  <entry>15</entry>
                </row>
                <row>
                  <entry>36</entry>
                  <entry>40,000</entry>
                  <entry>17</entry>
                </row>
                <row>
                  <entry>72</entry>
                  <entry>80,000</entry>
                  <entry>18</entry>
                </row>
                <row>
                  <entry>128</entry>
                  <entry>160,000</entry>
                  <entry>19</entry>
                </row>
                <row>
                  <entry>256</entry>
                  <entry>300,000</entry>
                  <entry>20</entry>
                </row>
                <row>
                  <entry>512</entry>
                  <entry>600,000</entry>
                  <entry>21</entry>
                </row>
                <row>
                  <entry>1024</entry>
                  <entry>1,200,00</entry>
                  <entry>22</entry>
                </row>
                <row>
                  <entry>2048</entry>
                  <entry>2,500,000</entry>
                  <entry>23</entry>
                </row>
                <row>
                  <entry>5120</entry>
                  <entry>5,000,000</entry>
                  <entry>24</entry>
                </row>
              </tbody>
            </tgroup>
          </table></p>
      </note>
    </p>
    <section>
      <p id="calculate-number-of-partition"><b>Calculating Numbers of Partitions</b></p>
      <p>The Object Storage system hashes a given name into a specific <i>partition</i>. For each
        partition, for a replica count of 3, there are three <i>partition directories</i>. The
        partition directories are then evenly scattered over all drives. We can calculate the number
        of partition directories as
        follows:<codeblock>number-partition-directories-per-drive = ( (2 ** partition-power) * replica-count ) / number-of-drives</codeblock></p>
    </section>
    <p>An ideal number of partition directories per drive is 100. However, the system operate
      normally with a wide range of number of partition directories per drive. The table <i><b>
          Partition Power</b></i> is based on the following: </p>
    <ul>
      <li>A replica count of 3</li>
      <li>For a given partition power, the minimum number of drives results in approximately 10,000
        partition directories per drive. With more directories on a drive results in performance
        issues.</li>
      <li>
        <p>For a given partition power, the maximum number of drives results in approximately 10
          partition directories per drive. With fewer directories per drive results in an uneven
          distribution of space utilization.</p>
      </li>
    </ul>
    <p> It is easy to select an appropriate partition power if your system is a fixed size. Select a
      value that gives the closest value to 100 partition directories per drive. If your system
      starts smaller and then grows, the issue is more complicated. The following two techniques may
      help:</p>
    <p>
      <ul id="ul_pkv_f3g_dt">
        <li>Start with a system that is closer to your final anticipated system size – this means
          you can use a high partition power that suites your final system.</li>
      </ul>
    </p>
    <ul>
      <li>You can add additional storage policies as the system grows. These storage policies can
        have a higher partition power because there will be more drives in a larger system. Note
        that this does not help account and container rings – storage policies are only applicable
        to object rings.</li>
    </ul>
    <p> </p>
  </body>
</topic>
