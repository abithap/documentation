<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="ring-specification">
  <title>Ring Specifications</title>
  <body>
    <p>The ring is resposible for mapping data on particular disks.There is a separate ring for
      account databases, container databases and for each object storage policy, but each ring works
      similarly. The swift-ring-builder utility is used to build and manage rings. The
      swift-ring-builder utility uses a builder file to contain ring information and additional data
      required to build future rings. In HOS2.0, you use the cloud model to specify how the rings
      are configured and used. This model is used to automatically invoke the swift-ring-builder
      utility as part of the deploy process. Normally, you will not run the swift-ring-builder
      utility directly.</p>
    <p>The rings are specified in the input model using the <b>ring-specifications</b> key. There is
      an entry for each distinct Swift system â€“ hence the Keystone region name is specified in the
        <b>region-name</b> key of the example cloud model.</p>
    <p>In the cloud model, a ring-specification is mentioned in
        <codeph>data/swift/rings.yml</codeph> file. The sample file of ring-specification is as
      follows:</p>
    <p>
      <codeblock>ring-specifications:
    - region-name: region1
      rings:
        - name: account
          display-name: Account Ring
          min-part-time: 16
          partition-power: 12
          replication-policy:
            replica-count: 3
        - name: container
          display-name: Container Ring
          min-part-time: 16
          partition-power: 12
          replication-policy:
            replica-count: 3
        - name: object-0
          display-name: General
          default: yes
          min-part-time: 16
          partition-power: 12
          replication-policy:
            replica-count: 3</codeblock>
    </p>
    <p>The above sample file shows that the <b>region1</b> has three rings as follows: </p>
    <p>
      <ul id="ul_hl3_q55_dt">
        <li><b>Account ring</b> - You must always specify a ring called <b>account</b>. The account
          ring is used by Swift to store metadata about the projects in your system. In Swift, a
          Keystone project maps to a Swift account. The <codeph>display-name</codeph> is
          informational and not used. </li>
        <li><b>Container ring</b> -You must always specify a ring called "container". The
            <codeph>display-name</codeph> is informational and not used.</li>
        <li><b>Object ring</b> - It is also known as Storage Policy. You must always specify a ring
          called "object-0". It is possible to have multiple object rings, which is known as
            <i>storage policies</i>. The <codeph>display-name</codeph> is the name of the storage
          policy and can be used by users of the Swift system when they create containers. It allows
          them to specify the storage policy that the container uses. In the example, the storage
          policy is called
          <b>General</b>.<!--<li>The swift-ring-builder will enforce a minimum of 16 hours between ring rebuilds.</li><li><b>Replica count</b>: The recommended value for the <codeph>replica-count</codeph> attribute is 3. Three copies of data are kept to provide resiliency and availability.</li>--></li>
        <li>
          <p>The purpose of the <b>min-part-time, partition-power, replication-policy</b>
              and<b>replica-count</b> are described in the following section. </p>
        </li>
      </ul>
    </p>
    <section><b>Ring Parameters</b><simpletable id="simpletable_qyp_lnj_2t">
        <strow>
          <stentry><codeph>replica-count</codeph></stentry>
          <stentry><p>It defines the number of  copies of object created.</p>You require high
            resiliency or availability and want a higher replica count. The
              <codeph>replica-count</codeph> is normally 3 (i.e., Swift will keep three copies of
            accounts and containers). It is <b>recommended</b> not to change the value to lower than
            3.</stentry>
        </strow>
        <strow>
          <stentry><codeph>min-part-time</codeph></stentry>
          <stentry><p>It changes the value used to decide if a given partition can be moved
              again.</p>This is the number of hours that the swift-ring-builder tool will enforce
            between ring rebuilds. On a small system, this can be as low as one hour. The value can
            be different for each ring. <p>The swift-ring-builder will enforce a minimum of 16 hours
              between ring rebuilds. However, this time is system-dependent so you will be unable to
              figure out the appropriate value for <codeph>min-part-power</codeph> until you have
              more experience with your system.</p></stentry>
        </strow>
        <strow>
          <stentry><codeph>partition-power</codeph></stentry>
          <stentry>The appropriate value is related to the number of disk drives you allocate for
            the account and container storage. We recommend that you use the same drives for both
            the account and container rings. Therefore, the <b>partition-power</b> value should be
            the same. For more detailed information on partition-power, refer to <xref
              href="#ring-specification/selecting-partition-power" format="dita">selecting
              partition-power</xref>.</stentry>
        </strow>
        <strow>
          <stentry><codeph>replication-policy</codeph></stentry>
          <stentry>The <b>replication-policy</b> attribute is used to specify that a ring uses
            replicated storage. The duplicate copies of the object are created and stored on
            different disk drives. All replicas are identical. If one is lost or corrupted, the
            system automatically copies one of the remaining replicas to restore the missing
            replica.</stentry>
        </strow>
        <strow>
          <stentry><codeph>default</codeph></stentry>
          <stentry>The default value in the above sample file of ring-specification  is set to yes,
            which means that the storage policy is enabled to store objects. For more details refer
            to <xref href="storage-policies.dita#topic_gm1_4bc_jt"/>.</stentry>
        </strow>
      </simpletable></section>
   <!--  
    <section id="make-changes-ring"><b>Making Changes to the Ring Specifications</b><p>The ring
        specification provided in the example model contains adequate configuration details to
        deploy Swift. However, there may be reasons for changing the model as follows: </p><ul
        id="ul_ef1_y55_dt">
        <li>Your system has more servers and disks than in the mid sized cloud example. Therefore,
          you may require to pick a different value for <codeph>partition-power</codeph>. The
          appropriate value is related to the number of disk drives you allocate for the account and
          container storage. We recommend that you use the same drives for both the account and
          container rings. Hence, the <codeph>partition-power</codeph> value should be the same. For
          more information about selecting an appropriate value, refer to <xref
            href="#topic_efw_k55_dt/selection-partition" format="dita">Selecting a Partition
            Power</xref>.</li>
        <li>You require high resiliency or availability and want a higher replica count. The
            <codeph>replica-count</codeph> is normally 3 (i.e., Swift will keep three copies of
          accounts and containers). It is <b>recommended</b> not to change the value to lower than
          3.</li>
        <li>You require a different name for the Storage Policy. If so, edit the
            <codeph>display-name</codeph> of the object-0 ring. This must be a single word
          (character such as underscore or dash are allowed). This is visible to your end
          users.</li>
        <li>Your system has more servers and disks than in the mid sized cloud example and the time
          to replicate data during a ring rebuild is longer than 16 hours. However, this time is
          system-dependent so you will be unable to figure out the appropriate value for
            <codeph>min-part-power</codeph> until you have more experience with your
            system.<p>Therefore, during the initial deployment of Swift, we suggest not to make
            changes in <b>min-part-time</b>. </p></li>
      </ul></section>-->
    <section id="selecting-partition-power">
      <title>Selecting a Partition Power</title>
      <p>Partition power is used to distribute the data uniformly across drives in a Swift nodes. It
        also defines the storage cluster capacity. You must set the part partition power value based
        on the total amount of storage you expect your entire ring to use. </p>
      <p>When storing an object, the object storage system hashes the name. This hash results in a
        hit on a partition (so a number of different object names result in the same partition
        number). Generally, the partition is mapped to available disk drives. With a replica count
        of 3, each partition is mapped to three different disk drives. The hashing algorithm used
        hashes over a fixed number of partitions. The partition-power attribute determines the
        number of partitions you have. </p>
    </section>
    <p>Select a partition power for a given ring that is appropriate to the number of disk drives
      you allocate to the ring for the following reasons: </p>
    <ul id="ul_i4b_gv5_dt">
      <li>If you use a high partition power and have a few disk drives, each disk drives will have
        1000s of partitions. With too many partitions, audit and other processes in the Object
        Storage system cannot "walk" the partitions in a reasonable time and updates will not occur
        in a timely manner.</li>
      <li>If you use a low partition power and have many disk drives, you will have 10s (or maybe
        only one) partition on a drive. The Object Storage system does not use size when hashing to
        a partition â€“ it hashes the name.
          <!--If sizes and names are randomly distributed, it is normally expected a random, even,  distribution of sizes within a partition. However, this may not happen (i.e., two neighboring partitions may have different sizes). --><p>With
          many partitions on a drive, a large partition is cancelled out by a smaller partition so
          the overall drive usage is similar. However, with very small numbers of partitions, the
          uneven distribution of sizes can be reflected in uneven disk drive utilization (so one
          drive becomes full while a neighboring drive is empty).</p></li>
    </ul>
    <p>An ideal number of partitions per drive is 100. If you know the number of drives, select part
      power approximately to 100 partition per drive. Usually, you install a system with a specific
      number of drives and add drives as your required for storage grows. However, you cannot change
      the value of the partition power. Hence you must select a value that is a compromise between
      current and planned capacity. </p>
    <p><b>Important</b>: If you are installing a small capacity system and you need to grow to a
      very large capacity but you cannot fit within any of the ranges in the table, please seek help
      from Professional Services to plan your system.</p>
    <p>There are a few additional factors that can help or mitigate the fixed nature of the
      partition power, as follows:</p>
    <ul>
      <li>Account and container storage represents a small fraction (typically 1%) of your object
        storage needs. Hence, you can select a smaller partition power (relative to object ring
        partition power) for the account and container rings. </li>
    </ul>
    <p>
      <ul>
        <li>For object storage, you can add additional storage policies (i.e., another object ring).
          When you have reached capacity in an existing storage policy, you can add a new storage
          policy with a higher partition power (because you now have more disk drives in your
          system). This means that you can install your system with a small partition power
          appropriate to a small number of initial disk drives. Later when you have many disk
          drives, the new storage policy can have a higher value appropriate to the larger number of
          drives.</li>
      </ul>
    </p>
    <p> However, when you continue to add storage capacity you should note that existing containers
      continue to use their original storage policy. Hence, the additional objects must be added to
      new containers to take advantage of the new storage policy.</p>
    <p>Use the following table to select an appropriate partition power for each ring. The partition
      power of a ring cannot be changed so it is important to select an appropriate value. This
      table is based on a replica count of 3. If your replica count is different or you are unable
      to find your system in the table then refer to <xref
        href="#ring-specification/calculating-number" format="dita">Calculating Numbers of
        Partitions</xref> for information of selecting a partition power. </p>
    <p> The table assumes that when you first deploy the Swift you have a small number of drives
      (the minimum column in the table) and later you add drives. </p>
    <p>
      <note>
        <ul>
          <li>You use the total number of drives. For example, if you have 3 servers, each with two
            drives, the total number of drives is 6.</li>
          <li>The lookup should be done separately for each of the account, container and object
            rings. Since account and containers represent approximately 1-2% of object storage, you
            will probably use fewer drives for the account and container rings (i.e., you will have
            fewer Proxy, Account, Container (PAC) servers) â€“ hence you your object rings may have a
            higher partition power.</li>
          <li>The largest anticipated number of drives imposes a limit in how a few drives (the
            minimum) you can have. See <xref href="#ring-specification/calculating-number"
              format="dita">Calculating Numbers of Partitions</xref>. This means that if you
            anticipate significant growth, your initial system can be small, but under certain
            limit.</li>
          <li>The table assumes that disk drives are the same size. The actual size of a drive is
            not significant.</li>
        </ul>
      </note>
    </p>
    <section id="partition-power"><b>Partition Power</b><table frame="all" rowsep="1" colsep="1" id="table_fz3_2hg_dt">
        <tgroup cols="3">
          <colspec colname="c1" colnum="1" colwidth="1.0*"/>
          <colspec colname="c2" colnum="2" colwidth="1.0*"/>
          <colspec colname="c3" colnum="3" colwidth="1.0*"/>
          <thead>
            <row>
              <entry>Number of drive during deployment (minimum)</entry>
              <entry>Number of drives in largest anticipated system (maximum)</entry>
              <entry>Recommended Part Power</entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry>6</entry>
              <entry>5,000</entry>
              <entry>14</entry>
            </row>
            <row>
              <entry>12</entry>
              <entry>10,000</entry>
              <entry>15</entry>
            </row>
            <row>
              <entry>36</entry>
              <entry>40,000</entry>
              <entry>17</entry>
            </row>
            <row>
              <entry>72</entry>
              <entry>80,000</entry>
              <entry>18</entry>
            </row>
            <row>
              <entry>128</entry>
              <entry>160,000</entry>
              <entry>19</entry>
            </row>
            <row>
              <entry>256</entry>
              <entry>300,000</entry>
              <entry>20</entry>
            </row>
            <row>
              <entry>512</entry>
              <entry>600,000</entry>
              <entry>21</entry>
            </row>
            <row>
              <entry>1024</entry>
              <entry>1,200,00</entry>
              <entry>22</entry>
            </row>
            <row>
              <entry>2048</entry>
              <entry>2,500,000</entry>
              <entry>23</entry>
            </row>
            <row>
              <entry>5120</entry>
              <entry>5,000,000</entry>
              <entry>24</entry>
            </row>
          </tbody>
        </tgroup>
      </table></section>
    <section id="calculating-number"><b>Calculating Numbers of Partitions</b><p>The Object Storage system hashes a given
        name into a specific <i>partition</i>. For each partition, for a replica count of 3, there
        are three <i>partition directories</i>. The partition directories are then evenly scattered
        over all drives. We can calculate the number of partition directories as
        follows:<codeblock>number-partition-directories-per-drive = ( (2 ** partition-power) * replica-count ) / number-of-drives
</codeblock></p></section>
    <p>An ideal number of partition directories per drive is 100. However, the system operate
      normally with a wide range of number of partition directories per drive. The table <i><b>
          <xref href="#ring-specification/partition-power" format="dita">Partition
        Power</xref></b></i> is based on the following: </p>
    <ul>
      <li>A replica count of 3</li>
      <li>For a given partition power, the minimum number of drives results in approximately 10,000
        partition directories per drive. With more directories on a drive results in performance
        issues.</li>
      <li>
        <p>For a given partition power, the maximum number of drives results in approximately 10
          partition directories per drive. With fewer directories per drive results in an uneven
          distribution of space utilization.</p>
      </li>
    </ul>
    <p> It is easy to select an appropriate partition power if your system is a fixed size. Select a
      value that gives the closest value to 100 partition directories per drive. If your system
      starts smaller and then grows, the issue is more complicated. The following two techniques may
      help:</p>
    <p>
      <ul>
        <li>Start with a system that is closer to your final anticipated system size â€“ this means
          you can use a high partition power that suites your final system.</li>
      </ul>
    </p>
    <ul>
      <li>You can add additional storage policies as the system grows. These storage policies can
        have a higher partition power because there will be more drives in a larger system. Note
        that this does not help account and container rings â€“ storage policies are only applicable
        to object rings.</li>
    </ul>
  </body>
</topic>
