<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="object-storage">
  <title>HP Helion OpenStack Object Storage (Swift)</title>
  <body>
    <p> </p>
    <p> A Object Storage (Swift) is Software Defined Storage (SDS) architecture, layered on top of
      industry standard servers, using native storage devices which are typically disk drives. Swift
      presents an object paradigm using an undelying set of disk drives. The disk drives are managed
      by a data structure called "ring". Also, Swift enable you to store, retrieve, and delete
      objects in a containers through RESTful HTTP API.</p>
    <p>The services in the Swift cluster are either <i>access</i> services or <i>storage</i>
      services. <ul>
        <li> The <i>access</i> services group is made up of the Swift Proxy Servers, which run
          Swift-Proxy, Account and Container Services, HTTP/HTTPS load balancers, and OpenStack®
          Keystone authentication service. </li>
        <li> The <i>storage</i> services group is composed of Swift Object Servers and various
          background services such as replicators. <p>Each of the service group can be scaled
            independently to meet workload and redundancy requirements. The storage service can
            scaled up as necessary. It can also be horizontally scalable to handle an increased
            number of simultaneous connections as well as large number of objects.</p></li>
      </ul></p>
    <p>Based on OpenStack® Object Storage, the HP Helion OpenStack® Object Storage service provides
      a highly available, resilient, and scalable storage pool for unstructured data. It is highly
      durable architecture with no single point of failure. Apart from this, HP Helion OpenStack
      introduces a concept of cloud input model.
      <?oxy_insert_start author="sharmabi" timestamp="20150820T140401+0530"?>In this release (beta1)
      we are providing three types of cloud model.<?oxy_insert_end?></p>
    <?oxy_insert_start author="sharmabi" timestamp="20150820T140434+0530"?>
    <p>
      <ul id="ul_p41_rkv_bt">
        <li>hp-ci/padawan - This cloud model runs in virtual machines. The virtual machines are
          configured  similar to  the <b>one-region-poc-with-vsa</b>. </li>
        <li>one-region-poc-with-vsa- This has a single control plane cluster running all services
          (Nova, Swift, Glance, etc) with an additional 3 resource nodes used as Nova compute
          servers. The number and usage of disk drives is fixed in the model. Therefore, Swift is
          automatically configured for this system and no changes are needed or anticipated for
          Swift-specific services. </li>
        <li>
          <p>mid sized cloud - This example model describes a system using multiple servers and
            networks. It provides an example for the cofiguration of the mid sized cloud. You must
            use this example model as the basis for building a cloud model  for your actual cloud by
            configuring the server and network that you plan to deploy. To deploy swift using this
            model you must make swift -specif changes to the provided example. For more details,
            refer Swift-specific changes in the example mid sized cloud. model.</p>
          <p> </p>
        </li>
      </ul>
    </p>
    <?oxy_insert_end?>
    <p><?oxy_insert_start author="sharmabi" timestamp="20150820T140936+0530"?><!--It allows you to define Swift-specific details like disk and server role of your choice based on the hardware available. 
Disk model- The disk model describes the number of disk present on a particular server and its usage. Some disks are used by the operating system (e.g. root, log, crash filesystems) and others for Swift storage. For a Swift system there are two disk models- Swift proxy servers (which may hold the account and container data) and Swift object servers. 
Server-role model- It maps the disk-model to a particular server type.You can monitor the health of Swift cluster using Monasca. Icinga, which was the supported monitor service in previous releases of Helion OpenStack, is not supported in Helion OpenStack 2.0.--><?oxy_insert_end?><?oxy_delete author="sharmabi" timestamp="20150820T140950+0530" content="It allows you to define Swift-specific details like disk and server role of your choice based on the hardware available. "?></p>
    <?oxy_delete author="sharmabi" timestamp="20150820T140950+0530" content="&lt;p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Disk model&lt;/b&gt;- The disk model describes the number of disk present on a particular server and its usage. Some disks are used by the operating system (e.g. root, log, crash filesystems) and others for Swift storage. For a Swift system there are two disk models- Swift proxy servers (which may hold the account and container data) and Swift object servers. &lt;/li&gt;&lt;li&gt;&lt;b&gt;Server-role model&lt;/b&gt;- It maps the disk-model to a particular server type.&lt;/li&gt;&lt;/ul&gt;&lt;/p&gt;&lt;p&gt;You can monitor the health of Swift cluster using Monasca. &lt;note&gt;Icinga, which was the supported monitor service in previous releases of Helion OpenStack, is not supported in Helion OpenStack 2.0.&lt;/note&gt;&lt;/p&gt;"?>
    <?oxy_insert_start author="sharmabi" timestamp="20150820T142720+0530"?>
    <!--Architecture Overview The following diagram depicts the architectural overview of the Swift deployment into your cloud:insert diagram?????topologyBefore you start your Swift deployment ensure that you are familiar with the following Swift components:Proxy ServerAccounts and ContainersObjectsRingsZonePart PowerStorage PoliciesFor more details on the Swift components, refer .-->
    <?oxy_insert_end?>
    <?oxy_delete author="sharmabi" timestamp="20150820T142727+0530" content="&lt;section id=&quot;arch-overview&quot;&gt;&lt;b&gt;Architecture Overview&lt;/b&gt;&lt;/section&gt;&lt;p&gt; The following diagram depicts the architectural overview of the Swift deployment into your cloud:&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;insert diagram&lt;/b&gt;?????&lt;/p&gt;&lt;p&gt;topology&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/p&gt;&lt;p&gt;Before you start your Swift deployment ensure that you are familiar with the following Swift components:&lt;/p&gt;&lt;p&gt;&lt;ul id=&quot;ul_hdc_qwz_1t&quot;&gt;&lt;li&gt;Proxy Server&lt;/li&gt;&lt;li&gt;Accounts and Containers&lt;/li&gt;&lt;li&gt;Objects&lt;/li&gt;&lt;li&gt;Rings&lt;/li&gt;&lt;li&gt;Zone&lt;/li&gt;&lt;li&gt;Part Power&lt;/li&gt;&lt;li&gt;Storage Policies&lt;p&gt;For more details on the Swift components, refer &lt;xref href=&quot;swift_components.dita#topic_jxs_1xz_1t&quot;/&gt;.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/p&gt;"?>
    <!--          
    <?oxy_insert_end?>
    <section id="storage-plocies"><b>Storage Policies</b><p>Storage policies allow you to
        differentiate how the objects are stored in the cluster. A few of the possible reasons are
        as follows:</p><ul>
        <li><b>Different types or classes of disk drive</b>
        </li>
      </ul><p>You can use different drives to store various type of data. For example: You can use
        7.5K RPM high-capacity drives for one type of data and fast SSD drives for another type of
        data.</p><p>
        <ul>
          <li><b>Different redundancy or availability needs</b></li>
        </ul>
      </p><p>You can define the redundancy and availability based on your requirement. You can use a
        replica count of 3 for "normal" data and a replica count of 4 for "critical" data. </p><p>As
        described in <b>partition power</b> you are growing your capacity beyond the recommended
        partition power. (not clear)</p><p>
        <ul>
          <li><b>Erasure-coded storage for some objects and replicated storage for other objects</b>
            (this is not supported in production)</li>
        </ul>
      </p><p> A storage policies are implemented on per-container basis. You can choose the storage
        policies of your choice. But if you fail to specify the storage policy then the default
        policy is used. Once you create and assign the storage policy to a container that remains
        intact , i.e., you cannot change the policy for that contaier but you can always create
        additional policies and move data from one contianer to another, if you choose to assign a
        different policy to a new container. </p><p> additional policies and move data within
        policies. (one container can have more than one storage policy??) once u crete a storage
        plcy </p><p> The disk drives used by storage policies can overlap or be distinct. If the
        storage policies overlap (i.e., have disks in common between two storage policies), it is
        recommended to use the same set of disk drives for both policies. But in case there is a
        partial overlap in disk drives, as one storage policy receives many objects, the drives that
        are common to both policies must store more objects than drives that are only allocated to
        one storage policy. This can be appropriate for a situation where the "overlapped" disk
        drives are larger than the non-overlapped drives. </p></section>
    <section id="Part-Power"><b>Part Power</b>
      <p>Partition power is used to distribute the data uniformly across drives in a Swift nodes. It
        also defines the storage cluster capacity. You must set the part partition power value based
        on the total amount of storage you expect your entire ring to use. </p><p>When storing an
        object, the object storage system hashes the name. This hash results in a hit on a partition
        (so a number of different object names will result in the same partition number). Generally,
        the partition is mapped to available disk drives. With a replica count of 3, each partition
        is mapped to three different disk drives. The hashing algorithm used hashes over a fixed
        number of partitions. The partition-power attribute determines the number of partitions you
        have. </p></section>
    <section id="Select-part-power">Select a partition power for a given ring that is appropriate to
      the number of disk drives you allocate to the ring for the following reasons:<ul>
        <li>If you use a high partition power and have few disk drives, each disk drives will have
          1000s of partitions. With too many partitions, audit and other processes in the Object
          Storage system cannot "walk" the partitions in a reasonable time and updates will not
          occur in a timely manner.</li>
        <li>If you use a low partition power and have many disk drives, you will have 10s (or maybe
          only one) partition on a drive. <p>With many partitions on a drive, a large partition is
            cancelled out by a smaller partition so the overall drive usage is similar. However,
            with very small numbers of partitions, the uneven distribution of sizes can be reflected
            in disk drive utilization (so one drive becomes full while a neighboring drive is
            empty).</p></li>
      </ul><p>An ideal number of partitions per drive is 100. If you know the number of drives,
        select part power approximatley to 100 partition per drive. Usually, you install a system
        with a specific number of drives and add drives as your storage needs grow. However, you
        cannot change the value of the partition power. Hence you must select a value that is a
        compromise between current and planned capacity. </p><p>The following artition power
        selection table shows a range of drives:<table frame="all" rowsep="1" colsep="1"
          id="table_cyp_1fb_zs">
          <tgroup cols="2">
            <colspec colname="c1" colnum="1" colwidth="1.0*"/>
            <colspec colname="c2" colnum="2" colwidth="1.0*"/>
            <thead>
              <row>
                <entry/>
                <entry/>
              </row>
            </thead>
            <tbody>
              <row>
                <entry/>
                <entry/>
              </row>
              <row>
                <entry/>
                <entry/>
              </row>
              <row>
                <entry/>
                <entry/>
              </row>
            </tbody>
          </tgroup>
        </table><b>Important</b>: if you are installting a small capacity systen and you need to
        grow to a very large capacity but you cannot fit within any of the ranges in the table,
        please seek help from Professional Services to plan your system..</p><p>There are a few
        additional factors that can help or mitigate the fixed nature of the partition power, as
        follows:</p><ul>
        <li>Account and container storage represents a small fraction (typically 1%) of your object
          storage needs. Hence, you can select a smaller partition power (relative to object ring
          partition power) for the account and container rings. </li>
      </ul><p>For object storage, you can add additional storage policies (i.e., another object
        ring). When you have reached capacity in an existing storage policy, you can add a new
        storage policy with a higher partition power (because you now have more disk drives in your
        system). This means that you can install your system with a small partition power
        appropriate to a small number of initial disk drives. Later when you have many disk drives,
        the new storage policy can have a higher value appropriate to the larger number of
        drives.</p><p> However, while this technique allows you to continue to add storage capacity
        you should note that existing containers continue to use their original storage policy.
        Hence, the additional objects must be added to new containers to take advantage of the new
        storage policy.</p></section>
    <section id="non-standard-replica-count"><p><b>Non-Standard Replica Count</b></p><p>The
        recommended value for the <b>replica-count</b> attribute is 3. Three copies of data are kept
        to ensure redundancy and continued access in the event of component failures. There are two
        situations where a different replica count is appropriate:</p><ul>
        <li>A replica count of 2 may be used. We do not recommend using a small replica count
          becuase the probability of object loss is higher with small replica counts.</li>
        <li> You can increase the replica count to give a higher reliability or availability. For
          example, if you have a system divided into 4 swift zones, with a replica count of 4, if
          two zones are failed or unavailable, you still have two copies of all objects in the
          remaining surviving zones. With a replica count of 3, this system would have an average of
          1.5 objects in the surviving zones. (Obviously, you won't have 1.5 copies of a given
          object. Instead, 50% of the objects would have a copy in one or other zone and 50% of
          objects would have a copy in both surviving zones).</li>
      </ul><p>It is possible to change the value of the replica count. In fact, fractional values
        are allowed. You can gradually change to a higher or lower value.</p><p> </p><b>Ring
        Specification</b><p>The ring maps the logical names of data to locations on a particular
        disks. There is a separate ring for account database, conainer database, and individual
        objects, but each rings works similarly. The rings are built and managed manually by a
        utility called the ring-builder. The ring-builder also keeps its own builder file with the
        ring information and additional data required to build future rings.</p><p/></section>
        -->
  </body>
</topic>



<?oxy_options track_changes="on"?>