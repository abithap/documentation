<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="topic_flx_j2b_ws">
  <title>Deploying ESX Cloud </title>
  <body>
    <p>This page describes the procedure to deploy ESX cloud using input model.</p>
    <section>
      <note type="important">
        <p>Before you start your ESX cloud deployment ensure that you read the following
          instructions carefully. <ul id="ul_h4l_m5s_ft">
            <li>Refer to <b><xref href="../installation.dita#topic_flx_j2b_ws/important-notes"
                  >Important Notes</xref></b> in the Installation Guide.</li>
            <li>Prepare your baremetal hardware on all nodes. For instruction, please refer to <xref
                href="../installation.dita#topic_flx_j2b_ws/Prereqs"><b>Before You
              Start</b></xref>.</li>
          </ul></p>
      </note>
    </section>
    <section id="Prereqs">
      <title>Prerequisite</title>
      <p>ESX/vCenter integration is not fully automatic, vCenter administrators are advised of the
        following responsibilities to ensure secure operation:</p>
      <p>
        <ul id="ul_shv_s4b_2t">
          <li>The VMware administrator is responsible for administration of the vCenter servers and
            the ESX nodes using the VMware administration tools. These responsibilities include: <ul
              id="ul_py3_j5l_ft">
              <li>Installing and configuring vCenter Server</li>
              <li>Installing and configuring ESX server and ESX cluster</li>
              <li>Installing and configuring shared datastores</li>
              <li>Establishing network connectivity between the ESX network and the HPE Helion
                management network</li>
            </ul></li>
          <li>The VMware administration staff is responsible for the review of vCenter logs. These
            logs are not automatically included in Helion centralized logging.</li>
          <li>Logging levels for vCenter should be set appropriately to prevent logging of the
            password for the Helion message queue.</li>
          <li>The vCenter cluster and ESX Compute nodes must be appropriately backed up.</li>
          <li>Backup procedures for vCenter should ensure that the file containing the Helion
            configuration as part of Nova and Cinder volume services is backed up and the backups
            are protected appropriately.</li>
          <li>Since the file containing the Helion message queue password could appear in the swap
            area of a vCenter server, appropriate controls should be applied to the vCenter cluster
            to prevent discovery of the password via snooping of the swap area or memory dumps</li>
        </ul>
      </p>
    </section>
    <section><b>Deploy ESX Cloud</b><p>At a high level, here are the steps to configure and deploy
        ESX cloud:</p><p><image href="../../media/esx/esx_deploy.jpg" id="image_kjt_zlm_ft"
      /></p></section>
    <section>
      <title>Procedure to Deploy ESX cloud</title>
    </section>
    <p>The following topics in this section explain how to deploy ESX cloud.<ol id="ol_ctc_gzk_ft">
        <li><b>Set up the Deployer Node</b>
          <p> For the detailed instructions on setting up the deployer node, refer to <xref
              href="../installation.dita#topic_flx_j2b_ws/DeployerInstall">Set up the Deployer
              node</xref> in the installation guide.</p></li>
        <li><b>Configure and Run the Deployer</b><p> For the detailed instructions on configuring
            and running deployer node, refer to <xref
              href="../installation.dita#topic_flx_j2b_ws/HLM_Node_Personalization">Configure and
              Run the Deployer</xref> in the installation guide.</p></li>
        <li><b>Prepare and Deploy Cloud Controllers</b><p>
            <ol id="ol_c1w_pfl_ft">
              <li>See a sample set of configuration files in the
                  <codeph>~/helion/examples/one-region-poc-with-esx</codeph> directory. The
                accompanying README.md file explains the contents of each of the configuration
                files. </li>
              <li>Copy the example configuration files into the required setup directory and edit
                them as required:
                <codeblock>cp -r ~/helion/examples/one-region-poc-with-esx/* ~/helion/my_cloud/definition/</codeblock></li>
              <li>The configuration files for editing are available at
                  <codeph>~/helion/my_cloud/definition/</codeph>. Refer to <b><xref
                    href="../installation.dita#topic_flx_j2b_ws/note">Note</xref></b> for the list
                of configuration files.</li>
              <li>Modify the <codeph>./helion/hlm/ansible/hlm-deploy.yml</codeph> to uncomment the
                line containing <codeph>eon.yml</codeph>.</li>
              <li>Modify the <codeph>neutron.conf.j2</codeph> at
                    <codeph>~/helion/my_cloud/config/<codeph>neutron/neutron.conf.j2</codeph></codeph>with
                the following
                  values:<codeblock>router_distributed = False
modify ml2_conf.ini.j2
with the below values
[ml2]
mechanism_drivers = ovsvapp, openvswitch, l2population
[agent]
enable_distributed_routing = False</codeblock><note>
                  You modify the mechanism_drivers = ovsvapp in the
                  <codeph>neutron.conf.j2</codeph>.</note></li>
              <li>Commit your cloud deploy configuration to the<xref
                  href="../using_git.dita#topic_u3v_1yz_ct"> local git repo</xref>, as follows:
                <codeblock>cd ~/helion/hos/ansible
git add -A
git commit -m "My config or other commit message"</codeblock></li>
              <li>Refer to the installation guide and execute the steps mentioned in sections <xref
                  href="../installation.dita#topic_flx_j2b_ws/CobblerDeploy">Deploy Cobbler</xref>,
                  <xref href="../installation.dita#topic_flx_j2b_ws/NodeProvision"/>, and <xref
                  href="../installation.dita#topic_flx_j2b_ws/run-config-processor">Run the
                  Configuration Processor</xref> to deploy cloud controller.</li>
            </ol>
          </p></li>
        <li><b>Prepare and Deploy ESX Computes and OVSvAPPs</b><p>The following sections describe
            the procedure to install and configure ESX compute and OVSvAPPs on vCenter.<ul
              id="ul_lns_fjl_ft">
              <li><xref href="#topic_flx_j2b_ws/deploy-template" format="dita">Deploy Helion Linux
                  Shell VM Template</xref></li>
              <li><xref href="#topic_flx_j2b_ws/prepare-esx-cloud-deployment" format="dita"
                  >Preparation for ESX Cloud Deployment</xref></li>
            </ul></p></li>
      </ol></p>
    <section><b>Deploy Helion Linux Shell VM Template</b><p>The first step in deploying the ESX
        compute proxy and OVSvAPPs is to create a VM template that will make it easier to deploy the
        ESX compute proxy for each Cluster and OVSvAPPs on each ESX server. </p><p>Perform the
        following steps to deploy a template:<ol id="ol_qdt_ljs_ft">
          <li>Import the <codeph>hlm-shell-vm.ova</codeph> in the vCenter using the vSphere client. </li>
          <li>In the vSphere Client, click <b>File</b> and then click <b>Deploy OVF
            Template</b></li>
          <li>Follow the instructions in the wizard to specify the data center, cluster, and node to
            install. Refer to the VMWare vSphere documentation as needed.</li>
        </ol></p></section>
    <section id="prepare-esx-cloud-deployment"><b>Preparation for ESX Cloud Deployment</b><p>This section describes the procedures to
        prepare and deploy the ESX computes and OVSvAPPs for deployment. <ol id="ol_xpc_zqs_ft">
          <li>Login to the deployer node.</li>
          <li>Source <codeph>service.osrc</codeph>.</li>
          <li><xref href="#topic_flx_j2b_ws/register-vcenter" format="dita">Register a vCenter
              Server</xref></li>
          <li><xref href="#topic_flx_j2b_ws/register-network" format="dita">Register ESX Cloud
              Network Configuration</xref></li>
          <li><xref href="#topic_flx_j2b_ws/import-cluster" format="dita">Import
            Clusters</xref></li>
          <li><xref href="#topic_flx_j2b_ws/activate-cluster" format="dita">Activate
            Clusters</xref></li>
          <li><xref href="#topic_flx_j2b_ws/modify-volume-config" format="dita">Modify the Volume
              Configuration File</xref></li>
          <li><xref href="#topic_flx_j2b_ws/commit-your-cloud" format="dita">Commit your Cloud
              Definition</xref></li>
          <li><xref href="#topic_flx_j2b_ws/deploy-compute-proxy-ovsvapps" format="dita">Deploy ESX
              Compute Proxy and OVSvApps</xref></li>
        </ol></p></section>
    <p><b>Manage vCenters and Clusters</b></p>
    <p>The following section describes the detailed procedure on managing the vCenters and
      clusters.</p>
    <p id="register-vcenter"><b>Register a vCenter Server</b></p>
    <p>vCenter provides centralized management of virtual host and virtual machines from a single
        console.<ol id="ol_xdj_5js_ft">
        <li>Add a vCenter using EON python
            client.<codeblock><codeph># eon vcenter-add --name &lt;vCenter Name> --ip-address &lt;vCenter IP address> --username &lt;vCenter Username> --password &lt;vCenter Password> --port &lt;vCenter Port></codeph></codeblock><p>where:
              <ul id="ul_bp3_yjs_ft">
              <li>vCenter Name - the identical name of the vCenter server.</li>
              <li>vCenter IP address - the IP address of the vCenter server.</li>
              <li>vCenter Username - the admin privilege username for the vCenter.</li>
              <li>vCenter Password - the password for the above username.</li>
              <li>vCenter Port - the vCenter server port. By default it is 443. <note
                  type="important">Please do not change the port unless you are sure about the
                  vCenter Port.</note></li>
            </ul></p><p><b>Sample
            Output:</b><codeblock><codeph># eon vcenter-add --name vc01 --ip-address 10.1.200.41 --username administrator@vsphere.local --password password --port 443</codeph>
+------------+--------------------------------------+
| Property   | Value                                |
+------------+--------------------------------------+
| created_at | 2015-08-20T12:08:09.000000           |
| deleted    | False                                |
| deleted_at | None                                 |
| id         | BC9DED4E-1639-481D-B190-2B54A2BF5674 |
| ip_address | 10.1.200.41                          |
| name       | vc01                                 |
| password   | &lt;password>                          |
| port       | 443                                  |
| type       | vcenter                              |
| updated_at | 2015-08-20T12:08:09.000000           |
| username   | administrator@vsphere.local          |
+------------+--------------------------------------+</codeblock></p></li>
      </ol></p>
    <p  id="register-network"><b>Register ESX Cloud Network Configuration</b></p>
    <p>This involve getting a sample network information template. Fill the details of the template
      and use that template to register cloud network configuration for the vCenter.</p>
    <p>
      <ol>
        <li>Execute the following command to get the network information
            template:<codeblock><codeph># eon get-network-info-template --filename &lt;<b>NETWORK_CONF_FILENAME</b>></codeph></codeblock><p>For
            example:<codeblock># eon get-network-info-template --filename net_conf.json</codeblock></p><p>Sample
            file of <codeph>net_conf.json</codeph> is shown
            below:<codeblock>{
    "network": {
        # Deployer Network details
        "deployer_network": {
            #Deployer Portgroup Name.
            "deployer_pg_name": "hlm-Deployer-PG",

            #VLAN id for Deployer Portgroup
            "deployer_vlan": "1702",

            #Enable DHCP for Deployer N/W
            "enable_deployer_dhcp": "no",

            #CIDR and gateway for deployer network
            "deployer_cidr": "172.170.2.0/24",
            "deployer_gateway_ip": "172.170.2.1",
            "deployer_node_ip": "172.170.1.10"
        },

        #Management Network details
        "management_network": {
            #Mgmt DVS name.
            "mgmt_dvs_name": "hlm-Mgmt",

            #Physical NIC name for Mgmt DVS
            "mgmt_nic_name": "vmnic1",

            #Mgmt Portgroup Name.
            "mgmt_pg_name": "hlm-Mgmt-PG",

            #Interface order: Example eth1
            "mgmt_interface_order": "eth1"
        },

        "data_network": {
            #Tenant network type
            "tenant_network_type": "vlan",

            "data_dvs_name": "hlm-Data",

            "data_nic_name": "vmnic2",

            #Data Portgroup Name.
            "data_pg_name": "hlm-Data-PG",

            #Interface order: Example eth2
            "data_interface_order": "eth2",

            #If more than one mgmt_nic_name are specified then NIC teaming will be enabled by default
            #Active uplink NICs for NIC teaming(If this field is empty, first data_nic_name will be "Active" and the rest will be in "Standby")
            "active_nics": "vmnic2",

            #Load Balancing. Please choose the corresponding number
            #    1 -> Route based on the originating virtual port
            #    2 -> Route based on IP hash
            #    3 -> Route based on source MAC hash
            #    4 -> Route based on physical NIC load
            #    5 -> Use explicit failover order
            "load_balancing": "1",

            #Network Failover Detection. Please choose the corresponding number
            #    1 -> Link Status
            #    2 -> Beacon Probing
            "network_failover_detection": "1",

            #Notify Switches(yes/no)
            "notify_switches": "yes"
        },

        "hpvcn_trunk_network": {
            #Trunk DVS name
            "trunk_dvs_name": "hlm-Trunk",

            #Trunk Portgroup Name.
            "trunk_pg_name": "hlm-Trunk-PG",

            #Interface order: Example eth3
            "trunk_interface_order": "eth3"
        },

        #VLAN Range for Data &amp; Trunk port group. Please provide the range separated by a hyphen(vlan-vlan).
        #Multiple vlan or vlan ranges has to be a comma separated value(*OPTIONAL)
        "vlan_range": "1-4094"
    },

    "template": {
        #Provide the template/appliance name that will be used for cloning
        "template_name": "hlm-shell-vm"
    },

    "vmconfig": {
        #Number of CPUs for OVSvApp/Computeproxy VM
        "cpu": "4",

        #Amount of RAM in MB
        "memory_in_mb": "4096",

        #SSH public key content for OVSvAPP/Computeproxy password less login.
        "ssh_key": "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCbSGs9OeJofAp7oHrztyAWX5LKK8ZSyLjRbmPwDls0qu+obWxRi7vJF9SdRgOB44zoLyRT2i5DC9Vz3sg4zshygLdg9qwtyTKS5N0Qi+R8D5rnbCAPGiU7eTu3jpgVy/xJOuCo6u1TQm2zA8epsSisqjtg6o36gZNvVcOE8XBYr92Dc3wFncjCh+Ej+X2WsKQHiais2fgCME1g4bj2r2E4+8oTiL/g5bhrhl1fSwQZPAMc2WO18Eyum3ItHpD9stxr3OgEpR0sqk2piUasgT5lc4x9NGqa0RZgtbrEVjATBCdF6EOrxEAlfAf6mMQ6/qWzx9SDeD4dyDa4Y+P+Letp stack@hlm"
    },
        # Do you want to skip inactive or maintenance mode hosts ?
    "skip_inactive_hosts": "yes",
        # Provide new host mo ids when you are adding a new host in an activated cluster.
    "new_host_mo_ids": ""
}
</codeblock></p></li>
        <li>Modify the template (json file) as per your
            environment.<codeblock>vi &lt;<b>NETWORK_CONF_FILENAME</b>></codeblock><p>For
            example:<codeblock>vi net_conf.json</codeblock></p></li>
        <li>Use the template to register Cloud Network Configuration. This sets the network
          information for a vCenter which is used to deploy and configure compute proxy and OVSvAPP
          VMs during the cluster activation.
            <codeblock><codeph># eon set-network-info --vcenter-id &lt;vCenter ID> --config-json &lt;<b>NETWORK_CONF_FILENAME</b>></codeph></codeblock><p>For example:
            <codeblock># eon set-network-info --vcenter-id BC9DED4E-1639-481D-B190-2B54A2BF5674 --config-json net_conf.json</codeblock></p><p>
            <note>The vcenter ID is generated when you execute the above (<b>step 2</b>)
              command.</note>
          </p></li>
        <li>Execute the following command to view the list of clusters for the given
            vCenter.<codeblock><codeph># eon cluster-list --vcenter-id &lt;vCenter ID></codeph></codeblock><b>Sample
            Output</b><codeblock># eon cluster-list --vcenter-id BC9DED4E-1639-481D-B190-2B54A2BF5674
+------------+----------+------------+---------------+
| MOID       | Name     | Datacenter | Import Status |
+------------+----------+------------+---------------+
| domain-c21 | Cluster1 | DC1        | not_imported  |
+------------+----------+------------+---------------+</codeblock></li>
      </ol>
    </p>
    <p id="import-cluster"><b>Import Cluster</b>
    </p>
    <p>You can use one or more ESX clusters for ESX Cloud Deployment. When a Import Cluster is
      invoked, required ESX Compute Proxy and OVSvApp nodes are deployed.</p>
    <p>
      <ol>
        <li> Import the cluster for the EON database under the given vCenter. <codeblock><codeph># eon cluster-import --vcenter-id &lt;vCenter ID> --cluster-name &lt;Cluster Name> --cluster-moid &lt;Cluster Moid></codeph></codeblock>where:<p>
            <ul id="ul_r4g_rjs_ft">
              <li>vCenter ID - ID of the vcenter containing the cluster.</li>
              <li>Cluster Name - the name of the cluster that needs to be imported.</li>
              <li>cluster Moid - Moid of the cluster that needs to be imported.</li>
            </ul>
          </p><p><b>Sample
            Output</b><codeblock># eon cluster-import --vcenter-id BC9DED4E-1639-481D-B190-2B54A2BF5674 --cluster-name Cluster1 --cluster-moid domain-c21
+--------------+-----------+
| Property     | Value     |
+--------------+-----------+
| cpu_free     | 83071.73  |
| cpu_total    | 83072     |
| cpu_used     | 0.27      |
| datacenter   | DC1       |
| disk_free    | 1022.79   |
| disk_total   | 1023.75   |
| errors       | []        |
| memory_free  | 496.82    |
| memory_total | 511.76    |
| memory_used  | 14.94     |
| name         | Cluster1  |
| state        | importing |
| switches     | []        |
+--------------+-----------+</codeblock></p><p>One
            vCenter can have multiple clusters. But it allows you to import only one cluster at a
            time.</p></li>
        <li> Execute the following command to view the list of clusters for the given
            vCenter.<codeblock><codeph># eon cluster-list --vcenter-id &lt;vCenter ID></codeph></codeblock><b>Sample
            Output</b><codeblock># eon cluster-list --vcenter-id BC9DED4E-1639-481D-B190-2B54A2BF5674
+------------+----------+------------+---------------+
| MOID       | Name     | Datacenter | Import Status |
+------------+----------+------------+---------------+
| domain-c22 | Cluster2 | DC1        | imported      |
+------------+----------+------------+---------------+</codeblock></li>
      </ol>
    </p>
    <p id="activate-cluster"><b>Activate Clusters</b><note>You can activate the cluster only after
        the import status of the cluster is changed to <b>imported</b>.<p>When you execute the
          active cluster command, the <codeph>server.yml</codeph> of the input model is updated with
          IP Addresses of compute proxy and OVSvApp.
          <!--VMs correspesponding to the clusters are being activated.--></p></note></p>
   
      <p>
        <ol>
          <li>Activate the cluster for the selected
              vCenter.<codeblock><codeph># eon cluster-activate --vcenter-id &lt;vCenter ID> --cluster-moid &lt;Cluster Moid> </codeph></codeblock><p><b>Sample
              Output</b></p><p>
            <codeblock># eon cluster-activate --vcenter-id BC9DED4E-1639-481D-B190-2B54A2BF5674 --cluster-moid domain-c22 
+---------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Property      | Value                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
+---------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| node_info     | {u'computeproxy': {u'pxe-mac-addr': u'00:50:56:b6:ce:1b', u'pxe-ip-addr': u'172.170.2.4', u'name': u'COMPUTEPROXY_Cluster1', u'cluster-moid': u'domain-c21'}, u'network_driver': {u'cluster_dvs_mapping': u'DC1/host/Cluster1:hlm-Trunk', u'Cluster1': [{u'host-moid': u'host-29', u'pxe-ip-addr': u'172.170.2.3', u'esx_hostname': u'10.1.200.33', u'ovsvapp_node': u'ovsvapp-10-1-200-33', u'pxe-mac-addr': u'00:50:56:b6:5e:9a'}, {u'host-moid': u'host-25', u'pxe-ip-addr': u'172.170.2.2', u'esx_hostname': u'10.1.200.66', u'ovsvapp_node': u'ovsvapp-10-1-200-66', u'pxe-mac-addr': u'00:50:56:b6:56:e6'}]}} |
| resource_moid | domain-c21                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
| resource_name | Cluster1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
| state         | activated                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
+---------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+</codeblock>
          </p></li>
        </ol>
      </p>
    <p id="modify-volume-config"><b>Modify  the Volume Configuration File</b></p>
    <p>Once the cluster is activated you must configure the volume.</p>
    <p>Perform the following steps to modify the volume configuration files:</p>
    <p>
      <ol>
        <li>Execute the following
          command:<codeblock>cd /home/stack/helion/hos/ansible/roles/_CND-CMN/templates</codeblock></li>
        <li>Modify the <codeph>cinder.conf.j2</codeph> as
          follows:<codeblock># Start of section for VMDK block storage
#
# If you have configured VMDK Block storage for cinder you must
# uncomment this section, and replace all strings in angle brackets
# with the correct values for vCenter you have configured. You
# must also add the section name to the list of values in the
# 'enabled_backends' variable above. You must provide unique section
# each time you configure a new backend.
#
#[&lt;unique-section-name>]
#vmware_api_retry_count = 10
#vmware_tmp_dir = /tmp
#vmware_image_transfer_timeout_secs = 7200
#vmware_task_poll_interval = 0.5
#vmware_max_objects_retrieval = 100
#vmware_volume_folder = cinder-volumes
#volume_driver = cinder.volume.drivers.vmware.vmdk.VMwareVcVmdkDriver
#vmware_host_ip = &lt;ip_address_of_vcenter>
#vmware_host_username = &lt;vcenter_username>
#vmware_host_password = &lt;password>
#
#volume_backend_name = &lt;vmdk-backend-name>
#
# End of section for VMDK block storage</codeblock></li>
      </ol>
    </p>
    <p id="commit-your-cloud"><b>Commit your Cloud Definition</b></p>
   
      <p>
        <ol>
          <li> Add the cloud deployment definition to the git
          :<codeblock>cd /home/stack/helion/hos/ansible;
git add -A;
git commit -m 'Adding ESX Configurations';</codeblock></li>
          <li>Prepare your environment for deployment:
            <codeblock>ansible-playbook -i hosts/localhost config-processor-run.yml;
ansible-playbook -i hosts/localhost ready-deployment.yml;
cd /home/stack/scratch/ansible/next/hos/ansible;</codeblock></li>
        </ol>
      </p>
    <p id="deploy-compute-proxy-ovsvapps"><b>Deploy ESX Compute Proxy and OVSvApps</b></p>
    <p>Execute the following command to deploy a esx compute and
      OVSvApps:<codeblock>ansible-playbook -i hosts/verb_hosts guard-deployment.yml
ansible-playbook -i hosts/verb_hosts osconfig-run.yml --limit '*<b>esx-ovsvapp:*esx-compute</b>' 
ansible-playbook -i hosts/verb_hosts hlm-deploy.yml --limit NOV-ESX:NEU-OVSVAPP</codeblock></p>
    <note>The variable <b>esx-ovsvapp</b> and <b>esx-compute</b> must be taken from the <b>name</b>
      key in the <codeph>resource-nodes</codeph> section in the
        <codeph>/data/control_plane.yml</codeph> file
        (<codeph>/home/stack/helion/my_cloud/definition/data/control_plane.yml</codeph>). </note>
  </body>
</topic>
