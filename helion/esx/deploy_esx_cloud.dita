<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="topic_flx_j2b_ws">
  <title>Deploying ESX Cloud </title>
  <body>
    <p>This page describes the procedure to deploy ESX cloud using input model.</p>
    <section>
      <title>Important</title>
      <p>Before you start your ESX cloud deployment ensure that you read the following instructions
        carefully. </p>
      <ul id="ul_x1p_ps2_ft">
        <li>Refer to <b><xref href="../installation.dita#topic_flx_j2b_ws/important-notes">Important
              Notes</xref></b> in the Installation Guide.</li>
        <li>Prepare your baremetal hardware on all nodes. For instruction, please refer to  <xref
            href="../installation.dita#topic_flx_j2b_ws/Prereqs"><b>Before You
          Start</b></xref>.</li>
      </ul>
    </section>
    <section id="Prereqs">
      <title>Prerequisite</title>
      <p>ESX/vCenter integration is not fully automatic, vCenter administrators are advised of the
        following responsibilities to ensure secure operation:</p>
      <p>
        <ul id="ul_shv_s4b_2t">
          <li>The VMware administrator is responsible for administration of the vCenter servers and
            the ESX nodes using the VMware administration tools. These responsibilities include: <ul
              id="ul_py3_j5l_ft">
              <li>Installing and configuring vCenter Server</li>
              <li>Installing and configuring ESX server and ESX cluster</li>
              <li>Installing and configuring shared datastores</li>
              <li>Establishing network connectivity between the ESX network and the HP Helion
                management network</li>
            </ul></li>
          <li>The VMware administration staff is responsible for the review of vCenter logs. These
            logs are not automatically included in Helion centralized logging.</li>
          <li>Logging levels for vCenter should be set appropriately to prevent logging of the
            password for the Helion message queue.</li>
          <li>The vCenter cluster and ESX Compute nodes must be appropriately backed up.</li>
          <li>Backup procedures for vCenter should ensure that the file containing the Helion
            configuration as part of Nova and Cinder volume services is backed up and the backups
            are protected appropriately.</li>
          <li>Since the file containing the Helion message queue password could appear in the swap
            area of a vCenter server, appropriate controls should be applied to the vCenter cluster
            to prevent discovery of the password via snooping of the swap area or memory dumps</li>
        </ul>
      </p>
    </section>
    <section><b>Deploy ESX Cloud</b><p>At a high level, here are the steps to configure and deploy
        ESX cloud:</p><p><image href="../../media/esx/esx_deploy.jpg" id="image_kjt_zlm_ft"
      /></p></section>
    <section>
      <title>Procedure to Deploy ESX cloud</title>
    </section>
    <p>The following topics in this section explain how to deploy ESX cloud.<ol id="ol_ctc_gzk_ft">
        <li><b>Set up the Deployer Node</b>
          <p> For the detailed instructions on setting up the deployer node, refer to <xref
              href="../installation.dita#topic_flx_j2b_ws/DeployerInstall">Set up the Deployer
              node</xref> in the installation guide.</p></li>
        <li><b>Configure and Run the Deployer</b><p> For the detailed instructions on configuring
            and running deployer node, refer to <xref
              href="../installation.dita#topic_flx_j2b_ws/HLM_Node_Personalization">Configure and
              Run the Deployer</xref> in the installation guide.</p></li>
        <li><b>Prepare and Deploy Cloud Controllers</b><p>
            <ol id="ol_c1w_pfl_ft">
              <li>See a sample set of configuration files in the
                  <codeph>~/helion/examples/one-region-poc-with-esx</codeph> directory. The
                accompanying README.md file explains the contents of each of the configuration
                files. </li>
              <li>Copy the example configuration files into the required setup directory and edit
                them as required:
                <codeblock>cp -r ~/helion/examples/one-region-poc-with-esx/* ~/helion/my_cloud/definition/</codeblock></li>
              <li>The configuration files for editing are available at
                  <codeph>~/helion/my_cloud/definition/</codeph>. Refer to <b><xref
                    href="../installation.dita#topic_flx_j2b_ws/note">Note</xref></b> for the list
                configuration files.</li>
              <li>Modify the <codeph>./helion/hlm/ansible/hlm-deploy.yml</codeph> to uncomment the
                line containing <codeph>eon.yml</codeph>.</li>
              <li>Modify the <codeph>neutron.conf.j2</codeph> with the following
                  values:<codeblock>router_distributed = False
modify ml2_conf.ini.j2
with the below values
[ml2]
mechanism_drivers = ovsvapp, openvswitch, l2population
[agent]
enable_distributed_routing = False</codeblock><note>
                  You modify the mechanism_drvivers  = ovsvapp in the
                    <codeph>neutron.conf.j2</codeph>.</note></li>
              <li>Refer to the installation guide and perform the <b>c</b> from <xref
                  href="../installation.dita#topic_flx_j2b_ws/Configuration">Configure your
                  Environment</xref>.</li>
              <li>Refer to the installation guide and execute the steps mentioned in <xref
                  href="../installation.dita#topic_flx_j2b_ws/CobblerDeploy">Deploy Cobbler</xref>,
                  <xref href="../installation.dita#topic_flx_j2b_ws/NodeProvision"/>,  and <xref
                  href="../installation.dita#topic_flx_j2b_ws/run-config-processor">Run the
                  Configuration Processor</xref> to deploy cloud controller.</li>
            </ol>
          </p></li>
        <li><b>Prepare and Deploy ESX Computes and OVSvAPP</b><p>The following sections describe the
            procedure to install and configure ESX compute and OVSvAPP on vCenter.<ul
              id="ul_lns_fjl_ft">
              <li><xref href="#topic_flx_j2b_ws/deploy-template" format="dita">Deploy Helion Linux
                  Shell VM Template</xref></li>
              <li><xref href="#topic_flx_j2b_ws/deploy-service" format="dita">Preparation for ESX
                  Cloud Deployment</xref><p><b>Deploy Helion Linux Shell VM Template</b></p><p>The
                  first step in deploying the ESX compute proxy and OVSvApps is to create a VM
                  template that will make it easier to deploy the ESX compute proxy for each Cluster
                  and OVSvApps on each ESX server. </p><p>Perform the following steps to deploy a
                    template:<ol id="ol_qdt_ljs_ft">
                    <li>Import the <codeph>hlm-shell-vm.ova</codeph> in the vCenter using the
                      vSphere client. </li>
                    <li>In the vSphere Client, click <b>File</b> and then click <b>Deploy OVF
                        Template</b></li>
                    <li>Follow the instructions in the wizard to specify the data center, cluster,
                      and node to install. Refer to the VMWare vSphere documentation as
                          needed.<p><b>Manage vCenters and Clusters</b></p><p>This section describes
                        the procedures to prepare the ESX cloud for deployment. </p></li>
                  </ol></p></li>
            </ul></p></li>
      </ol></p>
    <p>
      <note> Source <codeph>service.osrc</codeph> before you perform the following
            procedure.<p><b>Register a vCenter Server</b></p><p>vCenter provides centralized
          management of virtual host and virtual machines from a single console.<ol
            id="ol_xdj_5js_ft">
            <li>Add a vCenter using EON
                CLI.<codeblock><codeph>eon vcenter-add --name &lt;vCenter Name> --ip-address &lt;vCenter IP address> --username &lt;vCenter Username> --password &lt;vCenter Password> --port &lt;vCenter Port></codeph></codeblock><p>where:
                  <ul id="ul_bp3_yjs_ft">
                  <li>vCenter Name - the name of the vCenter server where the service is
                    deployed.</li>
                  <li>vCenter IP address - the IP address of the vCenter server where the service is
                    deployed.</li>
                  <li>vCenter Username - the username for the vCenter administrator.</li>
                  <li>vCenter Password - the password for the vCenter administrator.</li>
                  <li>vCenter Port - the vCenter server port.</li>
                </ul></p><p><b>Sample
                Output:</b><codeblock>+------------+--------------------------------------+
| Property   | Value                                |
+------------+--------------------------------------+
| created_at | 2015-08-20T12:08:09.000000           |
| deleted    | False                                |
| deleted_at | None                                 |
| id         | BC9DED4E-1639-481D-B190-2B54A2BF5674 |
| ip_address | 10.1.200.41                          |
| name       | vc01                                 |
| password   | &lt;SANITIZED>                          |
| port       | 443                                  |
| type       | vcenter                              |
| updated_at | 2015-08-20T12:08:09.000000           |
| username   | administrator@vsphere.local          |
+------------+--------------------------------------+</codeblock></p></li>
          </ol></p></note>
    </p>
    <p><b>Register ESX Cloud Network Configuration</b></p>
    <p>This involve getting a sample network information template. Fill the details of the template
      and use that template to register cloud network configuration for the vCenter.</p>
    <p>
      <ol id="ol_esl_ncc_2t">
        <li>Execute the following command to get the network information
            template:<codeblock><codeph>eon get-network-info-template --filename &lt;<b>NETWORK_CONF_FILENAME</b>></codeph></codeblock><p>For
            example:<codeblock>eon get-network-info-template --filename net_conf.json</codeblock></p><p>Sample
            file of <codeph>net_conf.json</codeph> is shown
            below:<codeblock>{
    "network": {
        # Deployer Network details
        "deployer_network": {
            #Deployer Portgroup Name.
            "deployer_pg_name": "hlm-Deployer-PG",

            #VLAN id for Deployer Portgroup
            "deployer_vlan": "1702",

            #Enable DHCP for Deployer N/W
            "enable_deployer_dhcp": "no",

            #CIDR and gateway for deployer network
            "deployer_cidr": "172.170.2.0/24",
            "deployer_gateway_ip": "172.170.2.1",
            "deployer_node_ip": "172.170.1.10"
        },

        #Management Network details
        "management_network": {
            #Mgmt DVS name.
            "mgmt_dvs_name": "hlm-Mgmt",

            #Physical NIC name for Mgmt DVS
            "mgmt_nic_name": "vmnic1",

            #Mgmt Portgroup Name.
            "mgmt_pg_name": "hlm-Mgmt-PG",

            #Interface order: Example eth1
            "mgmt_interface_order": "eth1"
        },

        "data_network": {
            #Tenant network type
            "tenant_network_type": "vlan",

            "data_dvs_name": "hlm-Data",

            "data_nic_name": "vmnic2",

            #Data Portgroup Name.
            "data_pg_name": "hlm-Data-PG",

            #Interface order: Example eth2
            "data_interface_order": "eth2",

            #If more than one mgmt_nic_name are specified then NIC teaming will be enabled by default
            #Active uplink NICs for NIC teaming(If this field is empty, first data_nic_name will be "Active" and the rest will be in "Standby")
            "active_nics": "vmnic2",

            #Load Balancing. Please choose the corresponding number
            #    1 -> Route based on the originating virtual port
            #    2 -> Route based on IP hash
            #    3 -> Route based on source MAC hash
            #    4 -> Route based on physical NIC load
            #    5 -> Use explicit failover order
            "load_balancing": "1",

            #Network Failover Detection. Please choose the corresponding number
            #    1 -> Link Status
            #    2 -> Beacon Probing
            "network_failover_detection": "1",

            #Notify Switches(yes/no)
            "notify_switches": "yes"
        },

        "hpvcn_trunk_network": {
            #Trunk DVS name
            "trunk_dvs_name": "hlm-Trunk",

            #Trunk Portgroup Name.
            "trunk_pg_name": "hlm-Trunk-PG",

            #Interface order: Example eth3
            "trunk_interface_order": "eth3"
        },

        #VLAN Range for Data &amp; Trunk port group. Please provide the range separated by a hyphen(vlan-vlan).
        #Multiple vlan or vlan ranges has to be a comma separated value(*OPTIONAL)
        "vlan_range": "1-4094"
    },

    "template": {
        #Provide the template/appliance name that will be used for cloning
        "template_name": "hlm-shell-vm"
    },

    "vmconfig": {
        #Number of CPUs for OVSvApp/Computeproxy VM
        "cpu": "4",

        #Amount of RAM in MB
        "memory_in_mb": "4096",

        #SSH public key content for OVSvAPP/Computeproxy password less login.
        "ssh_key": "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCbSGs9OeJofAp7oHrztyAWX5LKK8ZSyLjRbmPwDls0qu+obWxRi7vJF9SdRgOB44zoLyRT2i5DC9Vz3sg4zshygLdg9qwtyTKS5N0Qi+R8D5rnbCAPGiU7eTu3jpgVy/xJOuCo6u1TQm2zA8epsSisqjtg6o36gZNvVcOE8XBYr92Dc3wFncjCh+Ej+X2WsKQHiais2fgCME1g4bj2r2E4+8oTiL/g5bhrhl1fSwQZPAMc2WO18Eyum3ItHpD9stxr3OgEpR0sqk2piUasgT5lc4x9NGqa0RZgtbrEVjATBCdF6EOrxEAlfAf6mMQ6/qWzx9SDeD4dyDa4Y+P+Letp stack@hlm"
    },
        # Do you want to skip inactive or maintenance mode hosts ?
    "skip_inactive_hosts": "yes",
        # Provide new host mo ids when you are adding a new host in an activated cluster.
    "new_host_mo_ids": ""
}
</codeblock></p></li>
        <li>Modify the template (json file) as per your
            environment.<codeblock>vi &lt;<b>NETWORK_CONF_FILENAME</b>></codeblock><p>For
            example:<codeblock>vi net_conf.json</codeblock></p></li>
        <li>Use the template to register Cloud Network Configuration. This sets the network
          information for a vCenter which is used to deploy and configure compute proxy and OVSvAPP
          VMs during the cluster activation.
            <codeblock><codeph>eon set-network-info --vcenter-id &lt;vCenter ID> --config-json &lt;<b>NETWORK_CONF_FILENAME</b>></codeph></codeblock><p>For
            example:
            <codeblock>eon set-network-info --vcenter-id &lt;vCenter ID> --config-json net_conf.json</codeblock></p><p>
            <note>The vcenter ID is generated when you execute the above (<b>step 2</b>)
              command.</note>
          </p></li>
        <li>Execute the following command to view the list of clusters for the given
            vCenter.<codeblock><codeph>eon cluster-list --vcenter-id &lt;vCenter ID></codeph></codeblock><b>Sample
            Output</b><codeblock>+------------+----------+------------+---------------+
| MOID       | Name     | Datacenter | Import Status |
+------------+----------+------------+---------------+
| domain-c21 | Cluster1 | DC1        | not_imported  |
+------------+----------+------------+---------------+</codeblock></li>
      </ol>
    </p>
    <p><b>Import Cluster</b>
    </p>
    <p>You can use one or more ESX clusters for ESX Cloud Deployment. When a Import Cluster is
      invoked, required ESX Compute Proxy and OVSvApp nodes are deployed.</p>
    <p>
      <ol id="ol_lyt_cfc_2t">
        <li> Import the cluster for the EON database under the given vCenter. <codeblock><codeph>eon cluster-import --vcenter-id &lt;vCenter ID> --cluster-name &lt;Cluster Name> --cluster-moid &lt;Cluster Moid></codeph></codeblock>where:<p>
            <ul id="ul_r4g_rjs_ft">
              <li>vCenter ID - ID of the vcenter containing the cluster.</li>
              <li>Cluster Name - the name of the cluster that needs to be imported.</li>
              <li>cluster Moid - Moid of the cluster that needs to be imported.</li>
            </ul>
          </p><p><b>Sample
            Output</b><codeblock>+--------------+-----------+
| Property     | Value     |
+--------------+-----------+
| cpu_free     | 83071.73  |
| cpu_total    | 83072     |
| cpu_used     | 0.27      |
| datacenter   | DC1       |
| disk_free    | 1022.79   |
| disk_total   | 1023.75   |
| errors       | []        |
| memory_free  | 496.82    |
| memory_total | 511.76    |
| memory_used  | 14.94     |
| name         | Cluster1  |
| state        | importing |
| switches     | []        |
+--------------+-----------+</codeblock></p><p>One
            vCenter can have multiple clusters. But it allows you to import only one cluster at a
            time.</p></li>
        <li> Execute the following command to view the list of clusters for the given
            vCenter.<codeblock><codeph>eon cluster-list --vcenter-id &lt;vCenter ID></codeph></codeblock><b>Sample
            Output</b><codeblock>+------------+----------+------------+---------------+
| MOID       | Name     | Datacenter | Import Status |
+------------+----------+------------+---------------+
| domain-c22 | Cluster2 | DC1        | imported  |
+------------+----------+------------+---------------+</codeblock></li>
      </ol>
    </p>
    <section><b>Activate Clusters</b><note>You can activate the cluster only afte the import status
        of the cluster is changed to <b>imported</b>.</note><p>When you execute the active cluster
        command, the <codeph>server.yml</codeph> of the input model is updated with IP Addresses of
        compute proxy and OVSvApp.
        <!--VMs correspesponding to the clusters are being activated.--></p></section>
    <section>
      <p>
        <ol id="ol_nsc_tb3_2t">
          <li>Activate the cluster for the selected
                vCenter.<codeblock><codeph>eon cluster-activate --vcenter-id &lt;vCenter ID> --cluster-moid &lt;Cluster Moid> </codeph></codeblock><p><b>Sample
                Output</b></p><p>
              <codeblock>+---------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Property      | Value                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
+---------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| node_info     | {u'computeproxy': {u'pxe-mac-addr': u'00:50:56:b6:ce:1b', u'pxe-ip-addr': u'172.170.2.4', u'name': u'COMPUTEPROXY_Cluster1', u'cluster-moid': u'domain-c21'}, u'network_driver': {u'cluster_dvs_mapping': u'DC1/host/Cluster1:hlm-Trunk', u'Cluster1': [{u'host-moid': u'host-29', u'pxe-ip-addr': u'172.170.2.3', u'esx_hostname': u'10.1.200.33', u'ovsvapp_node': u'ovsvapp-10-1-200-33', u'pxe-mac-addr': u'00:50:56:b6:5e:9a'}, {u'host-moid': u'host-25', u'pxe-ip-addr': u'172.170.2.2', u'esx_hostname': u'10.1.200.66', u'ovsvapp_node': u'ovsvapp-10-1-200-66', u'pxe-mac-addr': u'00:50:56:b6:56:e6'}]}} |
| resource_moid | domain-c21                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
| resource_name | Cluster1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
| state         | activated                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
+---------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+</codeblock>
            </p></li>
        </ol>
      </p>
    </section>
    <section id="modify-volume-config"><b>Modify the Volume Configuration File</b></section>
    <p>Once the cluster is activated you must configure the volume.</p>
    <p>Perform the following steps to modify the volume configuration files:</p>
    <p>
      <ol>
        <li>Execute the following
          command:<codeblock>cd /home/stack/helion/hos/ansible/roles/_CND-CMN/templates</codeblock></li>
        <li>Modify the <codeph>cinder.conf.j2</codeph> as follows:</li>
      </ol>
      <codeblock> # Start of section for VMDK block storage
#
# If you have configured VMDK Block storage for cinder you must
# uncomment this section, and replace all strings in angle brackets
# with the correct values for vCenter you have configured. You
# must also add the section name to the list of values in the
# 'enabled_backends' variable above. You must provide unique section
# each time you configure a new backend.
#
#[&lt;unique-section-name>]
#vmware_api_retry_count = 10
#vmware_tmp_dir = /tmp
#vmware_image_transfer_timeout_secs = 7200
#vmware_task_poll_interval = 0.5
#vmware_max_objects_retrieval = 100
#vmware_volume_folder = cinder-volumes
#volume_driver = cinder.volume.drivers.vmware.vmdk.V
MwareVcVmdkDriver
#vmware_host_ip = &lt;ip_address_of_vcenter>
#vmware_host_username = &lt;vcenter_username>
#vmware_host_password = &lt;password>
#
#volume_backend_name: &lt;vmdk-backend-name>
#
# End of section for VMDK block storage</codeblock>
    </p>
    <section id="commit-change"><b>Commit your Cloud Definition</b></section>
    <section>
      <p>
        <ol id="ol_vk2_f1c_2t">
          <li> Add the cloud deployment definition to the git
            :<codeblock>cd /home/stack/helion/hos/ansible;
git add -A;
git commit -m 'My config';</codeblock></li>
          <li>Prepare your environment for deployment:
            <codeblock>ansible-playbook -i hosts/localhost config-processor-run.yml;
ansible-playbook -i hosts/localhost ready-deployment.yml;
cd /home/stack/scratch/ansible/next/hos/ansible;</codeblock></li>
        </ol>
      </p>
    </section>
    <section id="deploy-cloud"><b>Deploy ESX Compute Proxy and OVSvApps</b></section>
    <p>Execute the following command to deploy a esx compute and OVSvApps:<note>The variable
          <b>esx-ovsvapp</b> and <b>esx-compute</b> must be taken from the <b>name</b> key in the
          <codeph>resource-nodes</codeph> section in the <codeph>/data/control_plane.yml</codeph>
        file (/home/stack/helion/my_cloud/definition/data/control_plane.yml).
      </note><codeblock>ansible-playbook -i hosts/verb_hosts guard-deployment.yml
ansible-playbook -i hosts/verb_hosts osconfig-run.yml --limit '*<b>esx-ovsvapp:*esx-compute</b>' 
ansible-playbook -i hosts/verb_hosts hlm-deploy.yml --limit NOV-ESX:NEU-OVSVAPP</codeblock></p>
  </body>
</topic>
