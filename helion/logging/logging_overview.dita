<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic
  PUBLIC "-//OASIS//DTD DITA Topic//EN" "http://docs.oasis-open.org/dita/v1.1/OS/dtd/topic.dtd" >
<topic xml:lang="en-us" id="logging">
  <title>HP Helion <tm tmtype="reg">OpenStack</tm> 2.0: Centralized Logging Overview</title>
  <body>
    <p>Because a typical HP Helion OpenStack cloud consists of multiple servers, locating a specific
      log from a specific server can be difficult.</p>
    <p>The HP Helion OpenStack Centralized Logging feature collects logs on a central system, rather
      than leaving the logs scattered across the network. The administrator can use a single Kibana
      interface to view log information in charts, graphs, tables, histograms, and other forms.</p>
    <p>Centralized logging helps the administrator triage and troubleshoot the distributed HP Helion
      OpenStack cloud deployment from a single location. The admin is not required to access the
      several remote servers (SSH) to view the individual log files.</p>
    <p>In addition to each of the HP Helion services, Centralized Logging also processes logs for
      the following HP Helion OpenStack features:</p>
    <ul>
      <li>HAProxy</li>
      <li>syslog</li>
      <li>keepalived</li>
    </ul>
    <p>This document describes the Centralized Logging feature and contains the following
      sections:</p>
    <ul>
      <li>
        <xref type="section" href="#topic5937/install">Installation</xref>
      </li>
      <li>
        <xref type="section" href="#topic5937/components">Centralized Logging components</xref>
      </li>
      <li>
        <xref type="section" href="#topic5937/types">Centralized Logging log types</xref>
      </li>
      <li>
        <xref type="section" href="#topic5937/kibana">Kibana configuration</xref>
        <ul>
          <li>
            <xref type="section" href="#topic5937/interface">Logging into Kibana</xref>
          </li>
        </ul>
      </li>
      <li>
        <xref type="section" href="#topic5937/info">For more information</xref>
      </li>
    </ul>
    <section id="install">
      <title>Installation</title>
      <p>The Centralized Logging feature is automatically installed as part of the HP Helion
        OpenStack installation.</p>
      <p>No specific configuration is required to use Centralized Logging. However, you can tune or
        configure the individual components as needed for your environment.</p>
    </section>
    <section id="components">
      <title>Centralized Logging components</title>
      <p>Centralized logging consists of several components, detailed below:</p>
      <ul>
        <li>
          <p>
            <b>Beaver</b> is a python daemon that takes information in log files and sends the
            content to RabbitMQ.</p>
        </li>
        <li>
          <p>
            <b>RabbitMQ</b> is a message broker for collection of logging data across nodes.</p>
        </li>
        <li>
          <p>
            <b>logstash</b> is a log processing system for receiving, processing and outputting
            logs. logstash retrieves logs from RabbitMQ, processes and enriches the data, then
            stores the data in Elasticsearch.</p>
        </li>
        <li>
          <p>
            <b>Elasticsearch</b> is data store offering fast indexing and querying.</p>
        </li>
        <li>
          <p>
            <b>Kibana</b> is a client-side JavaScript application to visualize the data in
            Elasticsearch through a web browser. Kibana enables you to create charts and graphs
            using the log data.</p>
        </li>
      </ul>
      <p>These components are configured to work out-of-the-box and the admin should be able to view
        log data using the default configurations.</p>
      <p>At a high level, the Helion services forward logs to Beaver. Then, Beaver forwards JSON
        messages to RabbitMQ on the controller0 (management controller) node. Logstash connects to
        RabbitMQ to read queued messages and process the messages according to the Logstash
        configuration file. Logstash then forwards the processed log files in Elasticsearch. Users
        can use the Kibana interface to view and analyze the information, as shown in the following
        figure:</p>
      <p>
        <image href="../images/centralized_logging_diagram.png" placement="break"  width="4in"/>
      </p>
      <note>Logging requires 4GB of disk space to make sure that all logging messages are
        retained.</note>
    </section>
    <section id="types">
      <title>Centralized Logging log types</title>
      <p>The following table lists the types of logs collected by Centralized Logging and provides
        information on how the logs are maintained.</p>
      <table>
        <tgroup cols="6">
          <colspec colname="col1"/>
          <colspec colname="col2"/>
          <colspec colname="col3"/>
          <colspec colname="col4"/>
          <colspec colname="col5"/>
          <colspec colname="col6"/>
          <thead>
            <row>
              <entry>Data name</entry>
              <entry>Confidentiality</entry>
              <entry>Integrity</entry>
              <entry>Availability</entry>
              <entry>Backup?</entry>
              <entry>Description</entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry>Log records</entry>
              <entry>Restricted</entry>
              <entry>High</entry>
              <entry>Medium</entry>
              <entry>No</entry>
              <entry>Log records have a limited life, and are not archived. The log file on the
                local filesystem provides a fallback source of logging data (up to 20GB or 45 days)
                if the logging system fails.</entry>
            </row>
            <row>
              <entry>Log metadata</entry>
              <entry>Restricted</entry>
              <entry>High</entry>
              <entry>Medium</entry>
              <entry>No</entry>
              <entry>Elasticsearch indexes logged data to allow flexible searching.</entry>
            </row>
            <row>
              <entry>Credentials</entry>
              <entry>Confidential</entry>
              <entry>High</entry>
              <entry>Medium</entry>
              <entry>No</entry>
              <entry>Credentials for access to Elasticsearch and RabbitMQ are stored in
                configuration files owned by root with mode 0600.</entry>
            </row>
          </tbody>
        </tgroup>
      </table>
      <p>Log rotation will happen daily or when the current logfile reaches 2GB, whichever happens
        sooner. The number of rotations held will be balanced to attempt to cap logs from all
        services at 200GB.</p>
    </section>
    <section id="configuration">
      <title>Configuration</title>
      <p>To configure logging, modify the <codeph>roles/logging-common/defaults/main.yml</codeph>
        file as needed then reconfigure. To enable or disable logging for a service, for example,
        set it in this file and reconfigure:</p>
      <codeblock>
logr_services:
  rabbitmq:
    files:
    - centralized_logging:
    enabled: true</codeblock>
    </section>
    <section id="kibana">
      <title>Kibana configuration</title>
      <p>You can use the Kibana dashboards to view log data. Kibana is a tool developed to create
        charts, graphs, tables, and histograms based on logs send to Elasticsearch by logstash.</p>
      <p>While creating Kibana dashboards is beyond the scope of this document, it is important to
        know that you can use the default Kibana dashboards or create custom dashboards. The
        dashboards are JSON files that you can modify or create new dashboards based on existing
        dashboards.</p>
      <note>Kibana is client-side software. To operate properly, the browser must be able to access
        port 5601 on the control plane.</note>
    </section>
    <section id="interface">
      <title>Logging into Kibana</title>
      <p>There are two ways to access Kibana:</p>
      <ul>
        <li>Through the <b>Logging Dashboard</b> option in the Operations Console</li>
        <li>Direct link via port 5601 on the control plane IP</li>
      </ul>
      <p>Details:</p>
      <p><b>Operations Console method</b> - Launch a web browser on the control plane host to the
        following IP address, using the control plane IP address from the end of the install:</p>
      <codeblock>http://&lt;control plane IP&gt;:9095</codeblock>
      <p>Navigate to the <b>Logging Dashboard</b> via the menu.</p>
      <p><b>Direct Access method</b> - Launch a web browser on the control plane host to the
        following IP address, using the control plane IP address from the end of the install:</p>
      <codeblock>http://&lt;control plane IP&gt;:5601</codeblock>
      <b>Login Credentials</b>
      <p>Log in with these credentials:</p>
      <ul>
        <li>Username: kibana</li>
        <li>Password: 1f3pNxQmXXELJn/MJ4jOHixePSXlu4eZzhi1gJjr7mI=</li>
      </ul>
    </section>
    <section id="troubleshooting"><title>Using Centralized Logging to Troubleshoot Issues</title>
      <p>You can troubleshoot service-specific issues by reviewing the logs. After logging into
        Kibana, follow these steps:</p>
      <ol>
        <li>Select an index and click the green <b>Create</b> button on the <b>Configure an Index
            Pattern</b></li>
        <li>Click the <b>Discover</b> tab in the top left and you should be able to perform a search
          for the service and component for their logs</li>
      </ol>
    </section>
    <section id="monitoring"><title>Monitoring</title>
      <p>To help keep abreast of potential logging issues and resolve issues before they affect
        logging, you may wish to monitor the Centralized Logging Alarms. To do so:</p>
      <ol>
        <li>Log in to the Operations Console GUI</li>
        <li>Navigate to the Monitoring | Alarm Definitions page</li>
        <li>Find the alarm definitions that are applied to the various hosts. See the <xref
            href="../monasca/alarms.dita">Alarm Definitions List</xref> for the Centralized Logging
          Alarm Definitions.</li>
        <li>Navigate to the Monitoring | Alarm page</li>
        <li>Find the alarm definitions applied to the various hosts. These should match the alarm
          definitions in the <xref href="../monasca/alarms.dita">Alarms Definitions list</xref>
          below.</li>
        <li>See if the alarm is green (good) or is in a bad state. If any are in a bad state, see
          the possible actions to perform in the <xref href="../monasca/alarms.dita">Alarms
            Definitions list</xref>.</li>
      </ol>
      <p>You can use this filtering technique in the "Alarms" page to look for the following:</p>
      <ol>
        <li>To look for Processes that may be down, filter for "Process" then make sure the process
          are up: <ol>
            <li>Elasticsearch</li>
            <li>Logstash</li>
            <li>RabbitMQ</li>
            <li>Beaver</li>
            <li>Apache</li>
          </ol></li>
      </ol>
      <p>To look for sufficient Disk space, filter for "Disk"</p>
      <p>To look for sufficient RAM Memory, filter for "Memory"</p>
    </section>
    <section id="troubleshooting_issues"><title>Troubleshooting Logging Issues</title>
      <p>If centralized logging is not working, go through the following steps to
        troubleshooting:</p>
      <ul>
        <li><b>Processes</b> - Ensure that all the centralized logging processes are up.<ul>
            <li>Elasticsearch</li>
            <li>Logstash</li>
            <li>RabbitMQ</li>
            <li>Beaver</li>
            <li>Apache</li>
          </ul></li>
        <li><b>Check Disk Space</b> - Ensure that you have not run out of disk space.</li>
        <li><b>RAM</b> - Ensure that you have sufficient RAM and that it is allocated properly.</li>
      </ul>
      <b>Logging Best Practices</b>
      <p>By default a full backup includes all logs. Logs can be large, which can slow down the
        backup process. During this time, the control plane is offline.</p>
      <p>To balance having useful logs with optimizing system performance, you should routinely
        clean up logs from <codeph>logstash</codeph> to only keep logs of interest. You should only
        do a full backup if you need to preserve logs. In most cases, the exclude option is
        appropriate.</p>
      <b>Logs can fill the control plane node</b>
      <p>Logs are stored in one index per day in the Elasticsearch database but are not rotated.</p>
      <p>Because logs are not rotated, the logs can accumulate and consume significant space on the
        control plane.</p>
      <p>Use the following command to view information on log storage:</p>
      <codeblock>curl http://localhost:9200/_cat/indices?v</codeblock>
      <p>The following is an example of output from this command:</p>
      <codeblock>
health index pri rep docs.count docs.deleted store.size pri.store.size
yellow logstash-2015.02.10 5 1 33758082 0 16.5gb 16.5gb
yellow logstash-2015.02.11 5 1 27471823 0 13.6gb 13.6gb</codeblock>
      <p>You can delete older data using following command:</p>
      <codeblock>curl -X DELETE http://localhost:9200/[index name]</codeblock>
      <b>The number of logstash processes might not be sufficient</b>
      <p>On larger installations, logstash may not be able to process logs as they are produced. In
        this case unprocessed logs are stored in RabbitMQ queue and this queue is increasing.</p>
      <p>To check size of queue, use the following command:</p>
      <codeblock>rabbitmqctl list_queues | grep logs</codeblock>
      <p>The output of the command is similar to the following:</p>
      <codeblock>logstash-queue 0</codeblock>
      <p>If the queue is more than zero, wait a few minutes and run command again. If the size has
        increased, logs are not being processed quickly enough and the number of logstash processes
        should be increased.</p>
      <p>To increase logstash processes:</p>
      <ol>
        <li>Stop logstash using command <codeph>service logstash stop</codeph></li>
        <li>In the <codeph>/etc/default/logstash</codeph> file, find the
            <codeph>LS_NUM_INSTANCES</codeph> line and change the value to a larger number.</li>
        <li>Restart logstash using the command <codeph>service logstash start</codeph></li>
      </ol>
      <p>In approximately 15 minutes logstash starts processing. Re-check the queue size to make
        sure the queue size is reducing.</p>
    </section>
    <section id="info">
      <title>For more information</title>
      <p>For information the centralized logging components, use the following links:</p>
      <ul>
        <li><xref href="http://logstash.net/" scope="external" format="html">Logstash</xref></li>
        <li><xref href="http://www.elasticsearch.org/" scope="external" format="html"
            >Elasticsearch</xref></li>
        <li><xref href="http://www.rabbitmq.com/" scope="external" format="html"
          >RabbitMQ</xref></li>
        <li><xref href="http://www.elasticsearch.org/guide/en/kibana/current/_dashboard_schema.html"
            scope="external" format="html">Kibana Dashboard</xref></li>
      </ul>
    </section>
  </body>
</topic>
