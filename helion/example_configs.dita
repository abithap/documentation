<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="example_configs">
  <title>HP Helion <tm tmtype="reg">OpenStack</tm> 2.0: Example Configurations</title>
  <body>
    <section id="about"><title>Configuration Options</title>
      <ul>
        <li>Entry Scale Cloud - uses KVM Hypervisor for Compute hosts and VSA Block Storage
          backend</li>
        <li>Entry Scale Cloud variations: <ul>
            <li>Using a Ceph Block Storage backend</li>
            <li>Using a 3PAR Block Storage backend</li>
            <li>Using ESX as your Compute host hypervisor</li>
          </ul></li>
        <li>Entry Scale Storage Depot - deploys exclusively as an Object Storage service</li>
      </ul>
    </section>
    <section><title>Entry Scale Cloud with KVM Hypervisor on Compute Hosts and VSA Block Storage
        Backend</title>
      <p>Choosing this model will result in a cloud with that has the following characteristics:</p>
      <table frame="all" rowsep="1" colsep="1" id="entry_scale_cloud_kvm">
        <tgroup cols="2">
          <colspec colname="c1" colnum="1"/>
          <colspec colname="c2" colnum="2"/>
          <thead>
            <row>
              <entry>Item</entry>
              <entry>Description</entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry>Control Planes</entry>
              <entry>A single control plane consisting of three servers that co-host all of the
                required services. Optionally, the deployer can reside on a control plane node,
                reducing the number of servers by one. Separate instructions are available for that
                option. The defaults for this model are described below.</entry>
            </row>
            <row>
              <entry>Resource Pools</entry>
              <entry>One compute node, one VSA node</entry>
            </row>
            <row>
              <entry>Deployer</entry>
              <entry>The deployer handles the installation and initial configuration of your cloud.
                It can live on a dedicated node or you can utilize one of your controller nodes for
                this purpose. If you are using the GUI deployer, behind the scenes, it will enter
                your values into <b>data/servers.yml</b>. If not, you must enter them
                there.</entry>
            </row>
            <row>
              <entry>Networking</entry>
              <entry><p>The example requires the following networks: <ul>
                    <li> IMPI/iLO network, connected to the deployer and the IPMI/iLO ports of all
                      servers. </li>
                    <li> A network connected to a dedicated NIC on each server used to install the
                      operating system and install and configure Helion. </li>
                    <li> A pair of bonded NICs which are used used by the following networks: <ul>
                        <li> External API - This is the network that users will use to make requests
                          to the cloud </li>
                        <li> External VM - This is the network that will be used to provide access
                          to VMs (via floating IP addresses) </li>
                        <li> Guest - This is the network that will carry traffic between VMs on
                          private networks within the cloud </li>
                        <li> Cloud Management - This is the network that will be used for all
                          internal traffic between the cloud services </li>
                      </ul>
                    </li>
                  </ul>
                </p>
                <p>Note that the EXTERNAL_API network must be reachable from the EXTERNAL_VM network
                  if you want VMs to be able to make API calls to the cloud. An example set of
                  networks are defined in <codeph>data/networks.yml</codeph>. If you are using the
                  GUI installer, it will populate this file with your values; otherwise, you will
                  need to enter your values here.</p>
                <p>The example uses eth2 for the install network and eth3 and eth4 for the bonded
                  network. If you need to modify these for your environment they are defined in
                    <codeph>data/net_interfaces.yml.</codeph>.</p></entry>
            </row>
            <row>
              <entry>Local Storage</entry>
              <entry>
                <p>Each server should present a single operating system disc, protected by a RAID
                  controller. In addition the example configures one additional disk depending on
                  the role of the server: Controllers: /dev/sdb is configured to be used by
                  Swift.</p>
                <p>Additional discs can be configured for any of these roles by adding them in the
                  appropriate section. If you are using the GUI installer, it will edit the
                  appropriate configuration files; otherwise, you must enter your values in
                    <codeph>data/disks_.yml</codeph>.</p>
              </entry>
            </row>
          </tbody>
        </tgroup>
      </table>
      <p>The following diagram depicts a deployment scenario using the KVM hypervisor:</p>
      <p><image href="../media/networking/hos_kvm_diag.png"/></p>
    </section>

    <section id="single_esx"><title>Single Region Cloud (on ESX)</title>
      <p>Choosing this model will result in a cloud that has the following characteristics:</p>
      <p><table frame="all" rowsep="1" colsep="1" id="entry_scale_cloud_esx">
          <tgroup cols="2">
            <colspec colname="c1" colnum="1"/>
            <colspec colname="c2" colnum="2"/>
            <thead>
              <row>
                <entry/>
                <entry/>
              </row>
            </thead>
            <tbody>
              <row>
                <entry>Control Planes</entry>
                <entry>A single control plane consisting of three servers that co-host all of the
                  required services. Optionally, the deployer can reside on a control plane node,
                  reducing the number of servers by one. Separate instructions are available for
                  that option. The defaults for this model are described below.</entry>
              </row>
              <row>
                <entry>Resource Pools</entry>
                <entry>One compute proxy Node ( one per cluster in vCenter) two OvsVapp Nodes ( one
                  per ESX host in the cluster ) Additional resource nodes can be added to the
                  configuration. Minimal Swift Resources are provided by the control plane.</entry>
              </row>
              <row>
                <entry>Deployer</entry>
                <entry>
                  <p>The deployer handles the installation and initial configuration of your cloud.
                    It can live on a dedicated node or you can utilize one of your controller nodes
                    for this purpose. The minimum server count for this example is therefore 5-6
                    servers (Deployer [optional] + Control Plane (x3) + 1 vCenter with 1 cluster)*
                    The set of servers will be defined with the values you enter here.</p>
                  <p>If you are using the GUI deployer, behind the scenes, it will enter your values
                    into <b>data/servers.yml</b>. If not, you must enter them there.</p>
                </entry>
              </row>
              <row>
                <entry>Networking</entry>
                <entry><p>The example requires the following networks: <ul>
                      <li>IMPI/iLO network, connected to the deployer and the IPMI/iLO ports of all
                        servers. </li>
                      <li> A network connected to a dedicated NIC on each server used to install the
                        operating system and install and configure Helion. </li>
                      <li> A pair of bonded NICs which are used used by the following networks: <ul>
                          <li> External API - This is the network that users will use to make
                            requests to the cloud </li>
                          <li> External VM - This is the network that will be used to provide access
                            to VMs (via floating IP addresses) </li>
                          <li> Guest - This is the network that will carry traffic between VMs on
                            private networks within the cloud </li>
                          <li> Cloud Management - This is the network that will be used for all
                            internal traffic between the cloud services </li>
                        </ul></li>
                    </ul></p>
                  <p>Note that the EXTERNAL_API network must be reachable from the EXTERNAL_VM
                    network if you want VMs to be able to make API calls to the cloud.</p>
                  <p>An example set of networks are defined in
                    <codeph>data/networks.yml</codeph>.</p>
                  <p>If you use the GUI installer, it will populate this file with your values;
                    otherwise, you must enter your values there.</p>
                  <p>The example uses eth2 for the install network and eth3 and eth4 for the bonded
                    network. If you need to modify these for your environment they are defined in
                      <codeph>data/net_interfaces.yml</codeph>.</p></entry>
              </row>
              <row>
                <entry>Local Storage</entry>
                <entry><p>Each server should present a single operating system disc, protected by a
                    RAID controller. In addition the example configures one additional disk
                    depending on the role of the server: <ul>
                      <li>Controllers: /dev/sdb is configured to be used by Swift</li>
                    </ul></p>
                  <p>Additional discs can be configured for any of these roles by adding them in the
                    appropriate section. If you are using the GUI installer, it will edit the
                    appropriate configuration files; otherwise, you must enter them in
                      <codeph>data/disks_.yml</codeph>.</p></entry>
              </row>
            </tbody>
          </tgroup>
        </table></p>
      <p>The following diagram depicts a deployment scenario using the ESX hypervisor:</p>
      <p><image href="../media/networking/hos_esx_diag2.png"/></p>
    </section>




    <!--<p>In Helion <tm tmtype="reg">OpenStack</tm> 2.0 Beta 2 we offer the Entry Scale Cloud model
      with two variations depending on the hypervisor you wish to use: KVM or ESX.</p>
    <p>The following list summarizes this configuration and it's purpose.</p>
    <section><title>Single region with VSA on KVM</title>
      <p> This model can  scale up to include up to 100 compute nodes. Minmum node counts per role
        are described here:</p>
      <simpletable id="hardware">
        <strow>
          <stentry><b>Control Plane</b></stentry>
          <stentry>3 nodes, hosts all management and ancillary services in a highly available
            cluster</stentry>
        </strow>
        <strow>
          <stentry><b>Object Store</b></stentry>
          <stentry>All Swift services are contained in the control plane</stentry>
        </strow>
        <strow>
          <stentry><b>Compute Nodes</b></stentry>
          <stentry>Min 1; Max 100</stentry>
        </strow>
        <strow>
          <stentry><b>Block Storage (optional)</b></stentry>
          <stentry>3 nodes (recommended to ensure high availability) for our backend
            options</stentry>
        </strow>
        <strow>
          <stentry><b>Physical Network</b></stentry>
          <stentry>1 pair of bonded NICs</stentry>
        </strow>
        <strow>
          <stentry><b>Network Separation</b>
          </stentry>
          <stentry>EXTERNAL_API, EXTERNAL_VM, GUEST (vxlan), MGMT on separate VLANs</stentry>
        </strow>
      </simpletable>
      <p>You can learn more about the hardware requirements here: <xref href="hardware.dita">Minimum
          Hardware Requirements</xref></p> Note that this model assumes a dedicated deployer node,
      but you have the option of  running the deployer from a controller node.<p>The network
        configuration supports separation of networks for:</p>
      <ul id="network">
        <li>Management - This network will be used for all internal traffic between the cloud
          services. It must be an untagged network.</li>
        <li>External API - This is the network that users will use to make requests to the cloud. It
          must be a tagged network.</li>
        <li>External VM - This network will be used to provide access to VMs (via floating IP
          addresses). It must be a tagged network.</li>
        <li>Guest - This network will carry traffic between VMs on private networks within the
          cloud. It must be a tagged network.</li>
      </ul>
      <note>The requirement to have a dedicated network for operating system install and system deployment has
        been removed. To re-use the deployment network as part of the operational network the
        following conditions must be adhered to:</note>
      <ol id="single_network">
        <li>The network IP addresses used for the operating system install (i.e.
            <codeph>servers.yml</codeph> must belong to a
          Network that is used for your cloud infrastructure (e.g. in networks.yml).</li>
        <li>The network that re-uses those addresses must be un-tagged.</li>
      </ol> In the example <b>Single region cloud with VSA</b> the management network is: <ul
        id="mgmt_net">
        <li>Mgmt 10.2.1.0/24 VLAN: 100 (untagged)</li>
      </ul>
      <p>In the <codeph>servers.yml</codeph> file, servers are listed as:
        <codeblock>
baremetal_servers:
  - node_name: controller1
    node_type: CCN-001-001
    pxe_mac_addr: b2:72:8d:ac:7c:6f
    pxe_interface: eth2
    pxe_ip_addr: 192.168.10.3
    ilo_ip: 192.168.9.3</codeblock></p>
      <p>In the <codeph>servers.yml</codeph> file, servers will show as:
        <codeblock>
  - id: controller1
    role: ROLE-CCN
    ip-addr: 192.168.10.3</codeblock></p>
      <p>The values in the <codeph>servers.yml</codeph> and <codeph>servers.yml</codeph>
        definition files need to members of the CIDR used for the untagged network (in this case the
        MGMT network).</p>
      <p>This cloud model also supports the physical bonding of network interfaces (NIC bonding),
        see <xref href="input_model.dita#input_model/4.7_InterfaceModels" type="section">Interface
          Models</xref> for more details on that feature.</p>
    </section>-->
    
    <!-- INFO ABOUT MID-SIZE MODEL
        <p>The mid-size model is intended as a template for a moderate sized cloud. The Control plane is
      made up of multiple server clusters to provide sufficient computational, network and IOPS
      capacity for a mid-size production style cloud.</p>
    <p>The Helion mid-size model is architected as follows: </p>
    <p><b>Control Plane</b><ul id="ul_dq4_jbc_bt">
        <li>Core cluster: Runs Core OpenStack Services, (e.g. keystone, nova api, gance api, netron
          api, horizon, heat api). Default configuration is three nodes of role type ROLE-CONTROLLER </li>
        <li>Metering and Monitoring cluster: Runs the OpenStack Services for metering and monitoring
          (e.g. celiometer, monasca and logging). Default configuration is three nodes of role type
          ROLE-CONTROLLER</li>
        <li>Database and Message Queue Cluster: Runs clustered MySQL and RabbitMQ services to
          support the Helion cloud infrastructure. Default configuration is three nodes of role type
          ROLE-DBMQ. Three nodes are required for high availability.</li>
        <li>Swift PAC cluster: Runs the Swift Proxy, Account and Container services. Default
          configuration is three nodes or role type ROLE-SWPAC.</li>
      </ul>
    </p>
    <section><title>Resource Nodes</title></section>
    <p>Three resource pools are defined:<ul id="ul_mqk_mbc_bt">
        <li>Compute: Runs nova compute and associated services.Runs on nodes of role type
          ROLE-COMPUTE. The example lists 3 nodes, one node is required at a minimum.</li>
        <li>VSA: Runs the StoreVirtual VSA appliance.Runs on nodes of role type ROLE-VSA.One node
          minimum is required for demo purposes, three for high availability.The example lists 1
          node.</li>
        <li>Object: Runs the Swiftobject service.Runs on nodes of role type ROLE-SWPAC.The minimum
          node count should match your sift replica count.The example lists 3 nodes.</li>
      </ul></p>
    <section>
      <title>Deployer Node</title>
      <p>A dedicated deployer node is required. (Beta 1, this will be removed in a later Beta). The
        minimum node count required to run the model un-modified is 20 nodes. This can be reduced by
        consolidating servies on the control plane clusters. </p>
    </section>
    <section><title>Networking</title>
      <p>The example requires the following networks: <ul>
          <li>IMPI/iLO network, connected to the deployer and the IPMI/iLO ports of all servers</li>
          <li>A network connected to a dedicated NIC on each server used to install the operating
            system and install and configure Helion. In a future Beta release this network can be
            subsumed into system networks (below).</li>
          <li>A pair of bonded NICs which are used used by the following networks: <ul>
              <li>External API - This is the network that users will use to make requests to the
                cloud</li>
              <li>External VM - This is the network that will be used to provide access to VMs (via
                floating IP addresses)</li>
              <li>Guest - This is the network that will carry traffic between VMs on private
                networks within the cloud</li>
              <li>Cloud Management - This is the network that will be used for all internal traffic
                between the cloud services. In the example this is shown as untagged. It can be
                tagged if needed.</li>
              <li>iSCSI - This network is sued to host all iSCSI (Cinder back end) traffic between
                Nova compute and the VSA servers</li>
              <li>SWIFT - This network is used for internal Swift comunications between the Swift
                servers</li>
            </ul>
            <p>Note that the EXTERNAL\_API network must be reachable from the EXTERNAL\_VM network
              if you want VMS to be able to make API calls to the cloud.</p><p>An example set of
              networks are defined in ***data/networks.yml***. You will need to modify this file to
              reflect your environment.</p><p>The example uses eth2 for the install network and eth3
              and eth4 for the bonded network. If you need to modify these for your environment they
              are defined in ***data/net_interfaces.yml***</p>
          </li>
        </ul></p></section>
    <section><title>Adapting the mid-size model to fit your environment</title>
      <p>The minimum set of changes you need to make to adapt the model for your environment are:<ul>
          <li>Update baremetalConfig.yml to list the details of your baremetal servers. You need to
            perform this step if you are using the HLM supplied Cobber playbooks to install hLinux
            on your servers.</li>
          <li>Update servers.yml,: This step is not necessary if you are using the HLM supplied
            Cobber playbooks to install hLinux on your servers (From Beta 1 onwards ?). As this file
            will be automatically generated. However if you are using some other method to provision
            your servers then this file must be provided/updated.</li>
          <li>Update the networks.yml file to replace network CIDRs and VLANs with site specific
            values.</li>
          <li>Update the disk models to reflect any server disk configuration changes you want to
            make. Disk models are provided as follows:<ul>
              <li>DISK SET CONTROLLER: Minimum 1 disk</li>
              <li>DISK SET DBMQ: Minimum 3 disks</li>
              <li>DISK SET COMPUTE: Minimum 2 disks</li>
              <li>DISK SET SWPAC: Minimum 3 disks</li>
              <li>DISK SET SWOBJ: Minimum 3 disks</li>
              <li>DISK SET VSA: Minimum 3 disks</li>
            </ul>
          </li>
          <li>Update the net interfaces.yml file to match the server NICs used in your
            configuration. This file has a separate interface model definition for each of the following:<ul>
              <li>INTERFACE SET CONTROLLER</li>
              <li>INTERFACE SET VSA</li>
              <li>INTERFACE SET DBMQ</li>
              <li>INTERFACE SET SWPAC</li>
              <li>INTERFACE SET SWOBJ</li>
              <li>INTERFACE SET COMPUTE</li>
            </ul></li>
        </ul></p>
        -->
  </body>
</topic>
