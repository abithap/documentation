<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="config_ceph">
  <title>HPE Helion <tm tmtype="reg">OpenStack</tm> 2.0: Configuring for Ceph Block Storage
    Backend</title>
  <body>
    <p>This page describes how to configure your Ceph backend for the Helion Entry Scale Cloud with
      KVM model.</p>
    <section id="prereq"><title>Pre-requisites</title>
      <p>
        <ul>
          <li>The deployer node must be setup before deploying Ceph. For more details on the
            installation of deployer node, refer  <xref href="install_entryscale_kvm.dita"
              >here</xref>.</li>
        </ul>
      </p>
    </section>
    <p>New concept of Ceph </p>
    <section id="configuration">
      <title>Configure Your Environment</title>
      <p>The example configuration files for a Ceph backend can be found in
          <codeph>~/helion/examples/entry-scale-kvm-ceph/</codeph></p>
      <p>Edit your <codeph>disks_osd.yml</codeph> file to ensure that the additional disks are
        available on the servers:</p>
      <codeblock>disk-models:
  - name: DISK_SET_OSD
    # two disk node; remainder of disk 1 and all of disk 2 combined in single VG
    # VG is used to create three logical vols for /var, /var/log, and /var/crash
    device-groups:
      - name: ceph-osd-data-and-journal
        devices:
          - name: /dev/sdb
        consumer:
           name: ceph
           attrs:
             usage: data
             journal_disk: /dev/sdc
      - name: ceph-osd-data-and-shared-journal-set-1
        devices:
          - name: /dev/sdd
        consumer:
           name: ceph
           attrs:
             usage: data
             journal_disk: /dev/sdf
      - name: ceph-osd-data-and-shared-journal-set-2
        devices:
          - name: /dev/sde
        consumer:
           name: ceph
           attrs:
             usage: data
             journal_disk: /dev/sdf</codeblock>
      <p>The above sample file contains three OSD nodes and two journal disk.</p>
      <p>The disk model has the following fields:</p>
      <p>
        <simpletable>
          <strow>
            <stentry><b>device-groups</b></stentry>
            <stentry>There can be several device groups. This allows different sets of disks to be
              used for different purposes.</stentry>
          </strow>
          <strow>
            <stentry><b>name</b></stentry>
            <stentry>This is an arbitrary name for the device group. The name must be
              unique.</stentry>
          </strow>
          <strow>
            <stentry><b>devices</b></stentry>
            <stentry>This is a list of devices allocated to the device group. A
                <codeph>name</codeph> field containing <codeph>/dev/sdb</codeph>,
                <codeph>/dev/sdd</codeph>, and <codeph>/dev/sde</codeph> indicates that the device
              group is used by Ceph.</stentry>
          </strow>
          <strow>
            <stentry><b>consumer</b></stentry>
            <stentry>This specifies the service that uses the device group. A <codeph>name</codeph>
              field containing <b>ceph</b> indicates that the device group is used by
              Ceph.</stentry>
          </strow>
          <strow>
            <stentry><b>attrs</b></stentry>
            <stentry>This is the list of attributes.</stentry>
          </strow>
          <strow>
            <stentry><b>usage</b></stentry>
            <stentry>There can be several use of devices for a particular service. In the above
              sample, <codeph>usage</codeph> field contains <b>data</b> which indicates that the
              device is used for data storage.</stentry>
          </strow>
          <strow>
            <stentry><b>journal_disk</b></stentry>
            <stentry>It is to used to capture journal data. You can share the journal disk between
              two nodes.</stentry>
          </strow>
        </simpletable>
      </p>
      <p>Editable parameters for Ceph are available in the following locations:</p>
      <ul>
        <li><codeph>~/helion/my_cloud/config/ceph/settings.yml</codeph>
          <p>In the <codeph>settings.yml</codeph> file, you can edit the following parameters: <ul>
              <li>fsid</li>
              <li>ceph_cluster</li>
              <li>osd_settle_time</li>
              <li>OSD_journal_size</li>
            </ul></p><?oxy_custom_start type="oxy_content_highlight" color="255,255,0"?>Any
          additional Ceph parameters: <?oxy_custom_end?></li>
        <li>
          <p><codeph>~/helion/my_cloud/config/ceph/user_model.yml</codeph></p>
          <p>The <codeph>user_model.yml</codeph> has the editable values for the different pools
            created by HPE Helion OpenStack.</p>
        </li>
      </ul>
      <p>Commit your configuration to a <xref href="using_git.dita">local repository</xref>: <codeblock>cd ~/helion/hos/ansible
git add -A
git commit -m "&lt;commit message>"</codeblock>
        <note>Enter your commit message &lt;commit message></note></p>
      <note type="important">This step needs to be repeated any time you make changes to your
        configuration files before you move onto the following steps. See <xref
          href="using_git.dita">Using Git for Configuration Management</xref> for more
        information.</note>
      <p>Then you need to run the following commands to complete your configuration. These commands
        also verify your configuration is correct.</p>
    </section>
    <section id="deploy_cloud"><title>Deploy the Cloud (refer to vsa- third step not
        required.)</title>
      <p>Ceph Monitor service is deployed on the Controller Nodes and OSD's are deployed as separate
        nodes (Resource Nodes).</p>
      <p><b>Run Ceph Client Packages</b></p>
      <p>Execute the following command to install the ceph client packages on controller nodes and
        create users and ceph pools on the resource nodes:</p>
      <p>
        <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts ceph-client-prepare.yml</codeblock>
      </p>
    </section>
  </body>
</topic>
