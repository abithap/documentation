<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="supported_configs">
  <title>HP Helion <tm tmtype="reg">OpenStack</tm> 2.0: Example Configurations</title>
  <body>
    <!--<p>In Helion <tm tmtype="reg">OpenStack</tm> 2.0 Beta 2 we offer the Entry Scale Cloud model
      with two variations depending on the hypervisor you wish to use: KVM or ESX.</p>
    <p>The following list summarizes this configuration and it's purpose.</p>
    <section><title>Single region with VSA on KVM</title>
      <p> This model can  scale up to include up to 100 compute nodes. Minmum node counts per role
        are described here:</p>
      <simpletable id="hardware">
        <strow>
          <stentry><b>Control Plane</b></stentry>
          <stentry>3 nodes, hosts all management and ancillary services in a highly available
            cluster</stentry>
        </strow>
        <strow>
          <stentry><b>Object Store</b></stentry>
          <stentry>All Swift services are contained in the control plane</stentry>
        </strow>
        <strow>
          <stentry><b>Compute Nodes</b></stentry>
          <stentry>Min 1; Max 100</stentry>
        </strow>
        <strow>
          <stentry><b>Block Storage (optional)</b></stentry>
          <stentry>3 nodes (recommended to ensure high availability) for our backend
            options</stentry>
        </strow>
        <strow>
          <stentry><b>Physical Network</b></stentry>
          <stentry>1 pair of bonded NICs</stentry>
        </strow>
        <strow>
          <stentry><b>Network Separation</b>
          </stentry>
          <stentry>EXTERNAL_API, EXTERNAL_VM, GUEST (vxlan), MGMT on separate VLANs</stentry>
        </strow>
      </simpletable>
      <p>You can learn more about the hardware requirements here: <xref href="hardware.dita">Minimum
          Hardware Requirements</xref></p> Note that this model assumes a dedicated deployer node,
      but you have the option of  running the deployer from a controller node.<p>The network
        configuration supports separation of networks for:</p>
      <ul id="network">
        <li>Management - This network will be used for all internal traffic between the cloud
          services. It must be an untagged network.</li>
        <li>External API - This is the network that users will use to make requests to the cloud. It
          must be a tagged network.</li>
        <li>External VM - This network will be used to provide access to VMs (via floating IP
          addresses). It must be a tagged network.</li>
        <li>Guest - This network will carry traffic between VMs on private networks within the
          cloud. It must be a tagged network.</li>
      </ul>
      <note>The requirement to have a dedicated network for operating system install and system deployment has
        been removed. To re-use the deployment network as part of the operational network the
        following conditions must be adhered to:</note>
      <ol id="single_network">
        <li>The network IP addresses used for the operating system install (i.e.
            <codeph>baremetalConfig.yml</codeph> and <codeph>servers.yml</codeph>) must belong to a
          Network that is used for your cloud infrastructure (e.g. in networks.yml).</li>
        <li>The network that re-uses those addresses must be un-tagged.</li>
      </ol> In the example <b>Single region cloud with VSA</b> the management network is: <ul
        id="mgmt_net">
        <li>Mgmt 10.2.1.0/24 VLAN: 100 (untagged)</li>
      </ul>
      <p>In the <codeph>baremetalConfig.yml</codeph> file, servers are listed as:
        <codeblock>
baremetal_servers:
  - node_name: controller1
    node_type: CCN-001-001
    pxe_mac_addr: b2:72:8d:ac:7c:6f
    pxe_interface: eth2
    pxe_ip_addr: 192.168.10.3
    ilo_ip: 192.168.9.3</codeblock></p>
      <p>In the <codeph>servers.yml</codeph> file, servers will show as:
        <codeblock>
  - id: controller1
    role: ROLE-CCN
    ip-addr: 192.168.10.3</codeblock></p>
      <p>The values in the <codeph>baremetalConfig.yml</codeph> and <codeph>servers.yml</codeph>
        definition files need to members of the CIDR used for the untagged network (in this case the
        MGMT network).</p>
      <p>This cloud model also supports the physical bonding of network interfaces (NIC bonding),
        see <xref href="input_model.dita#input_model/4.7_InterfaceModels" type="section">Interface
          Models</xref> for more details on that feature.</p>
    </section>-->
    <section><title>Entry scale cloud with VSA (on KVM)</title>
      <p>Choosing this model will result in a cloud with that has the following
      characteristics:</p></section>
    <section id="controlplane"><title>Control Planes</title>
      <p>A single control plane consisting of three servers that co-host all of the required
        services. Optionally, the deployer can reside on a control plane node, reducing the number
        of servers by one. Separate instructions are available for that option. The defaults for
        this model are described below.</p>
    </section>
    <section id="resourcepools"><title>Resource Pools</title>
      <p>One compute node, one VSA node</p>
    </section>
    <section id="deployer"><title>Deployer</title>
      <p>The deployer handles the installation and initial configuration of your cloud. It can live
        on a dedicated node or you can utilize one of your controller nodes for this purpose. If you
        are using the GUI deployer, behind the scenes, it will enter your values into
          <b>data/baremetalConfig.yml</b>. If not, you must enter them there.</p>
    </section>
    <section id="networking"><title>Networking </title>
      <p>The example requires the following networks: <ul>
          <li> IMPI/iLO network, connected to the deployer and the IPMI/iLO ports of all servers. </li>
          <li> A network connected to a dedicated NIC on each server used to install the operating system and
            install and configure Helion. </li>
          <li> A pair of bonded NICs which are used used by the following networks: <ul>
              <li> External API - This is the network that users will use to make requests to the
                cloud </li>
              <li> External VM - This is the network that will be used to provide access to VMs (via
                floating IP addresses) </li>
              <li> Guest - This is the network that will carry traffic between VMs on private
                networks within the cloud </li>
              <li> Cloud Management - This is the network that will be used for all internal traffic
                between the cloud services </li>
            </ul>
          </li>
        </ul>
      </p>
      <p>Note that the EXTERNAL_API network must be reachable from the EXTERNAL_VM network if you
        want VMs to be able to make API calls to the cloud. An example set of networks are defined
        in <codeph>data/networks.yml</codeph>. If you are using the GUI installer, it will populate
        this file with your values; otherwise, you will need to enter your values here.</p>
      <p>The example uses eth2 for the install network and eth3 and eth4 for the bonded network. If
        you need to modify these for your environment they are defined in
          <codeph>data/net_interfaces.yml.</codeph>.</p>
    </section>
    <section id="localstorage"><title>Local Storage</title>
      <p>Each server should present a single operating system disc, protected by a RAID controller. In addition
        the example configures one additional disk depending on the role of the server: Controllers:
        /dev/sdb is configured to be used by Swift.</p>
      <p>Additional discs can be configured for any of these roles by adding them in the appropriate
        section. If you are using the GUI installer, it will edit the appropriate configuration
        files; otherwise, you must enter your values in <codeph>data/disks_.yml</codeph>.</p>
    </section>
    <section id="single_esx"><title>Single Region Cloud (on ESX)</title>
      <p>Choosing this model will result in a cloud that has the following characteristics:</p>
    </section>
    <section id="esx_control"><title>Control Planes</title>
      <p>A single control plane consisting of three servers that co-host all of the required
        services. Optionally, the deployer can reside on a control plane node, reducing the number
        of servers by one. Separate instructions are available for that option. The defaults for
        this model are described below.</p>
    </section>
    <section id="esx_resource"><title>Resource Pools</title>
      <p>One compute proxy Node ( one per cluster in vCenter) two OvsVapp Nodes ( one per ESX host
        in the cluster ) Additional resource nodes can be added to the configuration. Minimal Swift
        Resources are provided by the control plane.</p>
    </section>
    <section id="esx_deploy"><title>Deployer</title>
      <p>The deployer handles the installation and initial configuration of your cloud. It can live
        on a dedicated node or you can utilize one of your controller nodes for this purpose. The
        minimum server count for this example is therefore 5-6 servers (Deployer [optional] +
        Control Plane (x3) + 1 vCenter with 1 cluster)* The set of servers will be defined with the
        values you enter here.</p>
      <p>If you are using the GUI deployer, behind the scenes, it will enter your values into
          <b>data/baremetalConfig.yml</b>. If not, you must enter them there.</p>
    </section>
    <section id="esx_network"><title>Networking</title>
      <p>The example requires the following networks: <ul>
          <li>IMPI/iLO network, connected to the deployer and the IPMI/iLO ports of all servers. </li>
          <li> A network connected to a dedicated NIC on each server used to install the operating system and
            install and configure Helion. </li>
          <li> A pair of bonded NICs which are used used by the following networks: <ul>
              <li> External API - This is the network that users will use to make requests to the
                cloud </li>
              <li> External VM - This is the network that will be used to provide access to VMs (via
                floating IP addresses) </li>
              <li> Guest - This is the network that will carry traffic between VMs on private
                networks within the cloud </li>
              <li> Cloud Management - This is the network that will be used for all internal traffic
                between the cloud services </li>
            </ul></li>
        </ul></p>
      <p>Note that the EXTERNAL_API network must be reachable from the EXTERNAL_VM network if you
        want VMs to be able to make API calls to the cloud.</p>
      <p>An example set of networks are defined in <codeph>data/networks.yml</codeph>.</p>
      <p>If you use the GUI installer, it will populate this file with your values; otherwise, you
        must enter your values there.</p>
      <p>The example uses eth2 for the install network and eth3 and eth4 for the bonded network. If
        you need to modify these for your environment they are defined in
          <codeph>data/net_interfaces.yml</codeph>.</p>
    </section>
    <section id="esx_local"><title>Local Storage</title>
      <p>Each server should present a single operating system disc, protected by a RAID controller. In addition
        the example configures one additional disk depending on the role of the server: <ul>
          <li>Controllers: /dev/sdb is configured to be used by Swift</li>
        </ul></p>
      <p>Additional discs can be configured for any of these roles by adding them in the appropriate
        section. If you are using the GUI installer, it will edit the appropriate configuration
        files; otherwise, you must enter them in <codeph>data/disks_.yml</codeph>.</p>
    </section>
    <section id="diagrams">
      <title>Configuration Illustrations</title>
      <p>The following diagram depicts a deployment scenario using the KVM hypervisor:</p>
      <p><image href="../media/networking/hos_kvm_diag.png"/></p>
      <p>The following diagram depicts a deployment scenario using the ESX hypervisor:</p>
      <p><image href="../media/esx/Beta1-ESX-Network-Diagram%20(2).jpg" id="image_phk_1wz_4t"/></p>
    </section>
  </body>
</topic>
