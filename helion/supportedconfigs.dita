<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="supported_configs">
  <title>Beta 2: Supported Configurations</title>
  <body>
    <!--<p>In Helion <tm tmtype="reg">OpenStack</tm> 2.0 Beta 2 we offer the Entry Scale Cloud model
      with two variations depending on the hypervisor you wish to use: KVM or ESX.</p>
    <p>The following list summarizes this configuration and it's purpose.</p>
    <section><title>Single region with VSA on KVM</title>
      <p> This model can  scale up to include up to 100 compute nodes. Minmum node counts per role
        are described here:</p>
      <simpletable id="hardware">
        <strow>
          <stentry><b>Control Plane</b></stentry>
          <stentry>3 nodes, hosts all management and ancillary services in a highly available
            cluster</stentry>
        </strow>
        <strow>
          <stentry><b>Object Store</b></stentry>
          <stentry>All Swift services are contained in the control plane</stentry>
        </strow>
        <strow>
          <stentry><b>Compute Nodes</b></stentry>
          <stentry>Min 1; Max 100</stentry>
        </strow>
        <strow>
          <stentry><b>Block Storage (optional)</b></stentry>
          <stentry>3 nodes (recommended to ensure high availability) for our backend
            options</stentry>
        </strow>
        <strow>
          <stentry><b>Physical Network</b></stentry>
          <stentry>1 pair of bonded NICs</stentry>
        </strow>
        <strow>
          <stentry><b>Network Separation</b>
          </stentry>
          <stentry>EXTERNAL_API, EXTERNAL_VM, GUEST (vxlan), MGMT on separate VLANs</stentry>
        </strow>
      </simpletable>
      <p>You can learn more about the hardware requirements here: <xref href="hardware.dita">Minimum
          Hardware Requirements</xref></p> Note that this model assumes a dedicated deployer node,
      but you have the option of  running the deployer from a controller node.<p>The network
        configuration supports separation of networks for:</p>
      <ul id="network">
        <li>Management - This network will be used for all internal traffic between the cloud
          services. It must be an untagged network.</li>
        <li>External API - This is the network that users will use to make requests to the cloud. It
          must be a tagged network.</li>
        <li>External VM - This network will be used to provide access to VMs (via floating IP
          addresses). It must be a tagged network.</li>
        <li>Guest - This network will carry traffic between VMs on private networks within the
          cloud. It must be a tagged network.</li>
      </ul>
      <note>The requirement to have a dedicated network for OS-install and system deployment has
        been removed. To re-use the deployment network as part of the operational network the
        following conditions must be adhered to:</note>
      <ol id="single_network">
        <li>The network IP addresses used for the operating system install (i.e.
            <codeph>baremetalConfig.yml</codeph> and <codeph>servers.yml</codeph>) must belong to a
          Network that is used for your cloud infrastructure (e.g. in networks.yml).</li>
        <li>The network that re-uses those addresses must be un-tagged.</li>
      </ol> In the example <b>Single region cloud with VSA</b> the management network is: <ul
        id="mgmt_net">
        <li>Mgmt 10.2.1.0/24 VLAN: 100 (untagged)</li>
      </ul>
      <p>In the <codeph>baremetalConfig.yml</codeph> file, servers are listed as:
        <codeblock>
baremetal_servers:
  - node_name: controller1
    node_type: CCN-001-001
    pxe_mac_addr: b2:72:8d:ac:7c:6f
    pxe_interface: eth2
    pxe_ip_addr: 192.168.10.3
    ilo_ip: 192.168.9.3</codeblock></p>
      <p>In the <codeph>servers.yml</codeph> file, servers will show as:
        <codeblock>
  - id: controller1
    role: ROLE-CCN
    ip-addr: 192.168.10.3</codeblock></p>
      <p>The values in the <codeph>baremetalConfig.yml</codeph> and <codeph>servers.yml</codeph>
        definition files need to members of the CIDR used for the untagged network (in this case the
        MGMT network).</p>
      <p>This cloud model also supports the physical bonding of network interfaces (NIC bonding),
        see <xref href="input_model.dita#input_model/4.7_InterfaceModels" type="section">Interface
          Models</xref> for more details on that feature.</p>
    </section>-->
    
    <section><title>Single Region Cloud with VSA (on KVM)</title> Choosing this model will result in
      a cloud with that has the following characteristics: </section>
    <section><b>Control Planes </b></section>
    <section>A single control plane consisting of three servers that co-host all of the required
      services. Optionally, the deployer can reside on a control plane node, reducing the number of
      servers by one. Separate instructions are available for that option. The defaults for this
      model are described below. </section>
    
      <section><b>Resource Pools</b> </section>
    
    <section>One compute proxy Node ( 1 per cluster in vCenter) Two OvsVapp Nodes ( 1 per ESX host
      in the cluster ) Additional resource nodes can be added to the configuration. Minimal Swift
      Resources are provided by the control plane </section>
    <section><b>Deployer Node </b></section>
    <section>A dedicated deployer node is used in this model. The minimum server count for this
      example is therefore 6 servers (Deployer + Control Plane (x3) + 1 vCenter with 1 cluster)* The
      set of servers will be defined with the values you enter here. If you are using the GUI
      deployer, behind the scenes, it will enter your values into <b>data/barmetalConfig.yml</b>. If
      not, you must enter them there.</section>
    <section><b>Networking </b></section>
    <section>The example requires the following networks: <ul id="ul_cjb_gkv_kt">
        <li> IMPI/iLO network, connected to the deployer and the IPMI/iLO ports of all servers. </li>
        <li> A network connected to a dedicated NIC on each server used to install the OS and
          install and configure Helion. </li>
        <li> A pair of bonded NICs which are used used by the following networks: <ul
            id="ul_rkb_gkv_kt">
            <li> External API - This is the network that users will use to make requests to the
              cloud </li>
            <li> External VM - This is the network that will be used to provide access to VMs (via
              floating IP addresses) </li>
            <li> Guest - This is the network that will carry traffic between VMs on private networks
              within the cloud </li>
            <li> Cloud Management - This is the network that will be used for all internal traffic
              between the cloud services </li>
          </ul>
        </li>
      </ul> Note that the EXTERNAL_API network must be reachable from the EXTERNAL_VM network if you
      want VMS to be able to make API calls to the cloud. An example set of networks are defined in
      data/networks.yml. If you are using the GUI installer, it will populate this file with your
      values, but you can refer to it later for reference. </section>
    <section>The example uses eth2 for the install network and eth3 and eth4 for the bonded network.
      If you need to modify these for your environment they are defined in<b>
        data/net_interfaces.yml</b>. </section>
    
    <section><b>Local Storage </b></section>
    
    <section>Each server should present a single OS disc, protected by a RAID controller. In
      addition the example configures one additional disk depending on the role of the server:
      Controllers: /dev/sdb is configured to be used by Swift Additional discs can be configured for
      any of these roles by adding them in the appropriate section. As this installer runs, it will
      edit the appropriate configuration files; however, if you would like to see the file or edit
      it later, it is data/disks_.yml. </section>
    
    
    <section><title>Single Region Cloud (on ESX)</title> Choosing this model will result in a cloud
      that has the following characteristics: </section>
    <section><b>Control Planes</b> </section>
    <section>A single control plane consisting of three servers that co-host all of the required
      services. Optionally, the deployer can reside on a control plane node, reducing the number of
      servers by one. Separate instructions are available for that option. The defaults for this
      model are described below. </section>
    <section><b>Resource Pools</b> One compute proxy Node ( 1 per cluster in vCenter) Two OvsVapp Nodes ( 1
      per ESX host in the cluster ) Additional resource nodes can be added to the configuration.
      Minimal Swift Resources are provided by the control plane </section>
    <section><b>Deployer Node</b> </section>
    <section>A dedicated deployer node is used in this model. The minimum server count for this
      example is therefore 6 servers (Deployer + Control Plane (x3) + 1 vCenter with 1 cluster)* The
      set of servers will be defined with the values you enter here. If you are using the GUI
      deployer, behind the scenes, it will enter your values into <b>data/barmetalConfig.yml</b>. If
      not, you must enter them there.</section>
    <section><b>Networking</b> </section>
    <section>The example requires the following networks: <ul id="ul_lh3_cgv_kt">
        <li>IMPI/iLO network, connected to the deployer and the IPMI/iLO ports of all servers. </li>
        <li> A network connected to a dedicated NIC on each server used to install the OS and
          install and configure Helion. </li>
        <li> A pair of bonded NICs which are used used by the following networks: <ul
            id="ul_x33_cgv_kt">
            <li> External API - This is the network that users will use to make requests to the
              cloud </li>
            <li> External VM - This is the network that will be used to provide access to VMs (via
              floating IP addresses) </li>
            <li> Guest - This is the network that will carry traffic between VMs on private networks
              within the cloud </li>
            <li> Cloud Management - This is the network that will be used for all internal traffic
              between the cloud services </li>
          </ul></li>
      </ul> Note that the EXTERNAL_API network must be reachable from the EXTERNAL_VM network if you
      want VMS to be able to make API calls to the cloud. </section>
    <section>An example set of networks are defined in data/networks.yml. </section>
    <section>If you use the GUI installer, it  will populate this file with your values, but you can
      refer to it later for reference. </section>
    <section>The example uses eth2 for the install network and eth3 and eth4 for the bonded network.
      If you need to modify these for your environment they are defined in data/net_interfaces.yml. </section>
    <section><b>Local Storage</b> </section>
    <section>Each server should present a single OS disc, protected by a RAID controller. In
      addition the example configures one additional disk depending on the role of the server: 
    <ul><li>Controllers: /dev/sdb is configured to be used by Swift</li></ul> Additional discs can be
      configured for any of these roles by adding them in the appropriate section. As this installer
      runs, it will edit the appropriate configuration files; however, if you would like to see the
      file or edit it later, it is data/disks_.yml. </section>
    <section id="diagrams">
      <title>Configuration Illustrations</title>
      <p>The following diagram depicts a deployment scenario using the KVM hypervisor:</p>
      <p><image href="../media/networking/hos_kvm_diag.png"/></p>
      <p>The following diagram depicts a deployment scenario using the ESX hypervisor:</p>
      <p><image href="../media/networking/hos_esx_diag.png"/></p>
    </section>
  </body>
</topic>
