<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "http://docs.oasis-open.org/dita/v1.1/OS/dtd/topic.dtd" >
<topic xml:lang="en-us" id="topic12031">
  <title>Beta 2: Release Notes</title>
  <body>
    <p>This document provides an overview of the features contained within HP Helion <tm
        tmtype="reg">OpenStack</tm> 2.0 Beta 2, including known issues and workarounds for this
      release:</p>
    <section id="Features"><title>Features in HP Helion OpenStack 2.0 Beta 2</title>
      <p><b>New GUI Installer</b></p>
      <p>HP Helion OpenStack 2.0 Beta 2 introduces a GUI installer for configuring and installing
        OpenStack and OpenStack services. It provides an easy way to populate the configuration
        files needed to define the <xref href="input_model.dita">Input Model</xref>, and uses them
        to configure and install your cloud.</p><p>Note that in Beta 2, the GUI does not have final
        help text and has placeholder text on screens.</p>
      <p><b>Dedicated Deployer Node No Longer Required</b></p>
      <p>You can now use either a dedicated deployer node or setup the deployer on your first
        controller node. If you choose to use your first controller node as your deployer, ensure
        that your <codeph>baremetalConfig.yml</codeph> file contains the <codeph>is_deployer:
          true</codeph> notation in the options. If you are using a dedicated deployer node you can
        omit this. Here is an example snippet of a <codeph>baremetalConfig.yml</codeph> file where a
        user is using their first controller node as their deployer:
        <codeblock>
node_name: "ccn-0001"
role: "ROLE-CONTROLLER"
pxe_mac_addr: "b2:72:8d:ac:7c:6f"
pxe_interface: "eth2"
pxe_ip_addr: "192.168.10.3"
ilo_ip: "192.168.9.3"
ilo_user: "admin"
ilo_password: "password"
<b>is_deployer: true</b></codeblock></p>
      <p><b>The Installer only supports English language</b></p>
      <p>The deployer installation process prompts you to select a language, however only the
        English language as been tested. Therefore, at this time that is the only supported
        language.</p>
      <p><b>Local Git repository for config tracking</b></p>
      <p>In HP Helion OpenStack 2.0, a local git repository is used to track configuration changes
        and for the config processor to gather configuration settings from. The operations work as
        explained below:</p>
      <ul>
        <li>Operations under <codeph>~/helion</codeph> are under the aegis of git. You’ll need to
          commit any config into git (on the "site" branch) before the Configuration Processor can
          run it. The script that runs the config processor now guards against uncommitted
          changes.</li>
        <li>Deployment operations are run from a scratch directory that's assembled from various
          parts (the ansible output of the Configuration Processor and the playbooks). The directory
          is created using <codeph>ready-deployment.yml</codeph>.</li>
        <li>All deployment operations should be run from
            <codeph>~/scratch/ansible/next/hos/ansible</codeph>.</li>
      </ul>The Configuration Processor uses the config files stored in the repo to apply
      configuration settings. An explanation can be found <xref
        href="using_git.dita#topic_u3v_1yz_ct">in this topic</xref>. <p><b>New UEFI support</b></p>
      <p>Beta 2 introduces support for UEFI (which replaces the BIOS on newer servers), while
        continuing to support Legacy BIOS. Select the required mode on each node through its BIOS
        settings before beginning the install process. The installer will detect and use the mode
        you have selected for each node.</p>
      <p><b>Updated HP Helion OpenStack Services</b></p>
      <p>We have included the core set of OpenStack services from the <xref
          href="https://wiki.openstack.org/wiki/ReleaseNotes/Kilo" scope="external" format="html"
          >Kilo release</xref> with the exception of limitations notated in this document.</p>
      <p><b>More Cinder Backend Options Available</b></p>
      <p>You can now choose between Ceph, VSA, and 3PAR for Cinder backends.</p>
      <p>See: <xref href="cinder/cinder_overview.dita">Cinder Overview</xref> for more details</p>
      <p><b>Neutron LBaaS and FWaaS Extensions</b></p>
      <p>We have implemented networking extensions for load balancing and firewalls. Here is a
        description of each of these:</p>
      <ul>
        <li>
          <p>LBaaS (Load-Balancing-as-a-Service) is a Neutron extension that introduces a load
            balancing feature set. Helion OpenStack 2.0 includes the V2 version which allows
            implementation uses haproxy but third-party Load Balancers can be integrated as well. In
            case that a third-party driver doesn’t exist for LBaaS V2 installing LBaaS V1 instead of
            V2 is supported.</p>
          <ul>
            <li>For more information, see: <xref href="neutron/lbaas.dita">Load Balancer (LBaaS)
                Configuration</xref></li>
          </ul>
        </li>
        <li>
          <p>FWaaS (FireWall-as-a-Service) is a Neutron extension that introduces a firewall feature
            set. The included reference implementation is using iptables to block traffic but
            third-party Firewalls can be integrated with this extension into Helion OpenStack as
            well.</p>
          <ul>
            <li>For more information, see: <xref href="neutron/fwaas.dita">Firewall (FWaaS)
                Configuration</xref></li>
          </ul>
        </li>
        <li>
          <p>VPNaaS (Virtual-Private-Network-as-a-Service) is a Neutron extension that introduces a
            vpn feature set. It allows for encrypted traffic between two routers in (potentially)
            different datacenters hence supporting secure multi Availability Zone architectures for
            customers.</p>
          <ul>
            <li>For more information, see: <xref href="neutron/vpnaas.dita">VPNaaS
                Configuration</xref></li>
          </ul>
        </li>
        <li>
          <p>Multiple networks is a feature in Helion OpenStack® 2.0 that enables bridging of
            multiple external physical networks to neutron network. Using this functionality a
            tenant can dictate the path taken by the data in the physical network outside the cloud.
            It can also be used for multi-homing to two physical networks.</p>
          <ul>
            <li>For more information, see: <xref href="neutron/multinetwork.dita">Multiple External
                Network Configuration</xref></li>
          </ul>
        </li>
      </ul>
      <p><b>New Network Configuration Options</b></p><ul>
        <li>Network interface (NIC) bonding with multiple modes will allow you to set up a highly
          available and performant network infrastructure.</li>
        <li>Multi-network support will allow you to bridge multiple external physical networks to a
          Neutron network. This will enable you to dictate the path taken by the data in the
          physical network outside of your cloud deployment.</li>
        <li>Network separation (both physical and VLAN) will allow you to segregate traffic by type.
          For example, traffic can be separated into management, tenant, external, and services
          neworks.</li>
      </ul>
      <p><b>NIC Mapping Feature</b></p>
      <p>The new NIC mappings feature in HP Helion OpenStack Beta 2 allows the cloud administrator
        to map network interface names to PCI bus addresses. This is important because for
        well-understood reasons, Linux can sometimes name the network interfaces in a pseudo-random
        order. So in a cluster of identical machines, the <b>eth3</b> device can appear as
          <b>eth4</b> on one or more machines in the cluster. Once this erroneous name is determined
        by the operating system, it will use persistence rules to ensure that the NIC device is
        called <b>eth4</b> thereafter. This makes cluster configuration using Helion quite difficult
        if not impossible, if most machines understand the Management Network to be <b>eth3</b>, but
        some subset of machines understand that network to be on <b>eth4</b>( and whatever was on
        eth4 is similarly swapped to <b>eth3</b>).</p>
      <p>To resolve this, and to ensure that all NIC devices have consistent naming across the
        entire cloud, Beta 2 includes NIC Mapping facilities. This is a mechanism in the Input Model
        to define a specific NIC interface for a given PCI address for a given class of machine.
        Generally, when all of the network interfaces are defined in the <xref
          href="input_model.dita#input_model/4.8_NICMappings" type="section">Input Model</xref>, it
        is advisable to use the traditional naming, ie <b>ethN</b>. However, if one or more
        interfaces are not defined in the Input Model (perhaps because the Ansible network is not
        defined, or because there is a customer-specific network which is outside the scope of
        Helion), then it is recommended that a different naming convention is used, to prevent the
        definitions from colliding with existing configuration data. For example, the NIC mapping
        definitions could use <b>hos0</b>through <b>hosN</b> rather than <b>eth0</b> through
          <b>ethN</b>. Bearing in mind that the new naming must be consistent across the entire
        Input Model.</p>
      <p><b>Monasca-based Monitoring</b></p>
      <p>Monasca is a multi-tenant, scalable, fault-tolerant, monitoring service. It uses a REST API
        for high-speed metrics processing and querying, and has a streaming alarm and notification
        engine. In the Helion OpenStack 2.0 release, the OpenStack monitoring standard, Monasca, is
        supported as the Helion OpenStack monitoring solution with the following exceptions (which
        are not implemented):</p>
      <ul>
        <li>Transform Engine</li>
        <li>Events Engine </li>
        <li>Anomaly and Prediction Engine</li>
      </ul>
      <p>Monitoring is integrated for the following services: Logging, Neutron, Swift, Ceilometer,
        Heat, Rabbit, MySQL, HA Proxy, Glance, Cinder, Monitoring of Monasca, Nova, HP Linux for HP
        Helion OpenStack.</p><p>Learn more about Monasca here: <xref href="monasca/monasca.dita"
          >Monasca Overview</xref></p>
      <p><b>Centralized Logging</b></p>
      <p>Logging for the services below is enabled in the default configuration. You will have the
        ability to disable per-service logging as needed.</p>
      <p>Logging is integrated for the following services: Nova, Glance, Swift, Neutron, Ceilometer,
        Monasca, Horizon, Keystone, Cinder, Heat, OpsConsole, DNSaaS, LBaaS, FWaaS, and Trove.</p>
      <p>Learn more about Centralized Logging here: <xref href="logging/centralized_logging.dita"
          >Centralized Logging Overview</xref></p>
      <!--
<p><b>Backup and Restore Capabilities for the Control Plane</b> &#154; Allows you to backup and recover your Helion OpenStack</tm> environment with RPO=0.</p>
-->
    </section>
    <section id="KnownIssues">
      <title>Limitations in this Release</title>
      <p><b>Helion Examples</b></p>
      <p>The <codeph>~/helion/examples</codeph> directory contains a folder named "mid-size." This
        configuration is known not to work with the Beta 2 version of HP Helion OpenStack and should
        not be deployed.</p>
      <p>See <xref href="supportedconfigs.dita">Supported Configuration</xref> for the current list
        of supported configurations.</p>
      <p><b>TLS is not supported</b></p>
      <p>TLS is not supported for any traffic in Beta 2.</p>
      <p><b>HP Helion Development Platform</b></p>
      <p>Helion Development Platform is not supported on Beta 2.</p>
      <p><b>Installing Swift</b></p>
      <p>Swift installs with the following limitations: </p>
      <ul>
        <li>Only one Swift zone is supported.</li>
      </ul>
      <p><b>Helion Cloud Monitoring (Monasca) Limitations</b></p>
      <p>Basic Monitoring is complete for Beta 2 with the following limitations:</p>
      <ul>
        <li>Vertica is the default metrics database option in the install. If you want to use
          InfluxDB then read <xref href="monasca/monasca.dita#monitoring/config" type="section"
            >Configuring Monasca</xref> for more details.</li>
        <li>InfluxDB is not configured behind a VIP. It must directly access one node in the
          cluster.</li>
        <li>Backup/restore of Monasca configuration and metrics data is not available.</li>
      </ul>
      <p><b>Operations Console (Ops Console)</b></p>
      <p>The Operations Console is available on port 9095 on the first controller node IP. There are
        some limitations to note and they are listed below by menu option in the console:</p>
      <ul>
        <li>Dashboard <ul>
            <li>There are two notification sections for the Storage services. The one in the upper
              left is the Compute service mislabeled as Storage.</li>
            <li>The New Alarms section uses the UTC timezone to determine timeframes regardless of
              what timezone is in the settings.</li>
            <li>The New Alarms section will only display alarms that have a service dimension
              defined.</li>
            <li>Platform Services (HDP Summary) alarm panels are not yet implemented but appear in
              the menu and thus will not populate if selected.</li>
          </ul>
        </li>
        <li>Logging Dashboard <ul>
            <li>There is no auto-logon function, users must use the credentials in the <xref
                href="logging/centralized_logging.dita#logging/interface">Logging Overview</xref> to
              access the UI.</li>
          </ul></li>
        <li>Alarms <ul>
            <li>The tile view of alarms with lengthy dimensions does not render cleanly.</li>
          </ul></li>
        <li>Compute Instances <ul>
            <li>Compute instance utilization metrics are not available in Beta 2</li>
          </ul>
        </li>
        <li>Networking - Alarm Summary <ul>
            <li>Alarms may be limited to only those with the dimension "service=networking".</li>
          </ul></li>
        <li>Storage - Alarm Summary <ul>
            <li>Alarms with the dimension "service=compute" sometimes appear in this list in
              addition to the "service=block-storage" and "service=object-storage" ones.</li>
          </ul></li>
        <li>HDP - Alarm Summary <ul>
            <li>This section is not implemented in Beta 2</li>
          </ul></li>
        <li>Masthead Items <ul>
            <li>The Help and EULA links will not work properly. You can view the Help files here:
                <xref href="monasca/opsconsole/overview.dita">Operations Console
              Overview</xref></li>
            <li>User information panel fails to load making it so a user cannot update their email
              address or password.</li>
          </ul></li>
      </ul>
      <p><b>Centralized Logging Limitations</b></p>
      <p>Logging support is complete for Beta 2 with the following limitations:</p>
      <ul>
        <li>Alarm definition and generation for Kibana and curator are not integrated.</li>
        <li>Logging-monitor 'stop' playbook is not integrated.</li>
      </ul>
      <b>Issue: Unable to Launch instance from volume on Horizon dashboard</b>
      <p> In Beta 2 you will not be able to launch an instance from a volume in Horizon. To work
        around this issue you have two options: </p><ul>
        <li>From Horizon Dashboard | Instances page: Launch an Instance and select source as
          Bootable volume.</li>
        <li>Use the OpenStack or Nova client  to create the instance.</li>
      </ul>
      <!--<p><b>Networks</b></p>
      <p>
        <ul id="ul_rlg_dbg_ys">
          <li>Multiple NICs on a compute node and each NIC is connected to a different external
            network. Today we don't support multiple external networks at L2 (ML2 OVS mech driver)
            nor L3 (DVR or Centralized).</li>
        </ul>
      </p>-->
      <!--<p>Need use cases/stories </p>
    <p>* ability to span L3</p>
    <p>* multiple provider networks/separation of traffic</p>
    <p>* two pools for fip to span networks/different fip pools</p>
    <p>* talk to multiple networks</p>
    <p> </p>
    <p>[06/02/15]</p>
    <p>We discussed today that all of the use cases should be fulfilled by HOS2.0.</p>
    <p> </p>-->
      <!--  <p><b>Cinder</b></p><ul id="ul_vld_l1g_ys">
        <li>
          <p>Core cinder services are all being installed and configured via the installer. </p>
        </li>
        <li>
          <p>Cinder installation with VSA as a backend is being worked through at the moment, manual
            steps to be defined later may be required to complete installation.</p>
        </li>
      </ul>-->
      <!--
      <p><b>Cinder Limitations</b></p>
      <ul>
        <li>3Par storage is not supported in Beta 2</li>
      </ul>-->
    </section>
  </body>
</topic>
