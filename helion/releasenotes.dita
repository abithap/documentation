<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "http://docs.oasis-open.org/dita/v1.1/OS/dtd/topic.dtd" >
<topic xml:lang="en-us" id="topic12031">
  <title>Beta 1: Release Notes</title>
  <body>
    <p>This document provides an overview of the features contained within HP Helion <tm
        tmtype="reg">OpenStack</tm> 2.0 Beta 1, including known issues and workarounds for this
      release:</p>
    <section id="Features"><title>Features in HP Helion OpenStack 2.0 Beta 1</title><p><b>New UEFI
          support</b> &#151; Beta 1 introduces support for UEFI (which replaces the BIOS on newer
        servers), while continuing to support Legacy BIOS. Select the required mode on each node
        through its BIOS settings before beginning the install process. HOS 2.0 will detect and use
        the mode you have selected for each node.</p>
      <p><b>Updated HP Helion OpenStack Services</b> &#151; We have included the core set of
        OpenStack services from the <xref href="https://wiki.openstack.org/wiki/ReleaseNotes/Kilo"
          scope="external" format="html">Kilo release</xref> with the exception of limitations
        notated in this document.</p><p><b>Neutron LBaaS and FWaaS Extensions</b> &#151; We have
        implemented networking extensions for load balancing and firewalls. Here is a description of
        each of these:</p><ul>
        <li>LBaaS (Load-Balancing-as-a-Service) is a Neutron extension that introduces a load
          balancing feature set. Helion OpenStack 2.0 includes the V2 version which allows
          implementation uses haproxy but third-party Load Balancers can be integrated as well. In
          case that a third-party driver doesn’t exist for LBaaS V2 installing LBaaS V1 instead of
          V2 is supported. <ul>
            <li>For more information, see: <xref
                href="https://wiki.openstack.org/wiki/Neutron/LBaaS" scope="external" format="html"
                >https://wiki.openstack.org/wiki/Neutron/LBaaS</xref></li>
          </ul>
        </li>
        <li>FWaaS (FireWall-as-a-Service) is a Neutron extension that introduces a firewall feature
          set. The included reference implementation is using iptables to block traffic but
          third-party Firewalls can be integrated with this extension into Helion OpenStack as well. <ul>
            <li>For more information, see: <xref
                href="https://wiki.openstack.org/wiki/Neutron/FWaaS" scope="external" format="html"
                >https://wiki.openstack.org/wiki/Neutron/FWaaS</xref></li>
          </ul>
        </li>
        <!-- removed per wiki
<li>VPNaaS (Virtual-Private-Network-as-a-Service) is a Neutron extension that introduces a vpn feature set. It allows for encrypted traffic between two routers in (potentially) different datacenters hence supporting secure multi Availability Zone architectures for customers. 
<ul>
<li>For more information, see: <xref href="https://wiki.openstack.org/wiki/Neutron/VPNaaS" scope="external" format="html">https://wiki.openstack.org/wiki/Neutron/VPNaaS</xref></li>
</ul>
</li>
-->
      </ul><p><b>New Network Configuration Options</b></p><ul>
        <li>Network interface bonding with multiple modes will allow you to set up a highly
          available and performant network infrastructure.</li>
        <li>Multi-network support will allow you to bridge multiple external physical networks to a
          Neutron network. This will enable you to dictate the path taken by the data in the
          physical network outside of your cloud deployment.</li>
        <li>Network separation (both physical and VLAN) will allow you to segregate traffic by type.
          For example, traffic can be separated into management, tenant, external, and services
          neworks.</li>
      </ul><p><b>Monasca-based Monitoring</b> &#151; Monasca is a multi-tenant, scalable,
        fault-tolerant, monitoring service. It uses a REST API for high-speed metrics processing and
        querying, and has a streaming alarm and notification engine. In the Helion OpenStack 2.0
        release, the OpenStack monitoring standard, Monasca, is supported as the Helion OpenStack
        monitoring solution with the following exceptions (which are not implemented):</p><ul>
        <li>Transform Engine</li>
        <li>Events Engine </li>
        <li>Anomaly and Prediction Engine</li>
      </ul><p>Monitoring is integrated for the following services: Logging, Neutron, Swift,
        Ceilometer, Heat, Rabbit, MySQL, HA Proxy, Glance, Cinder, Monitoring of Monasca, Nova, HP
        Linux for HP Helion OpenStack.</p><p>Learn more about Monasca here: <xref
          href="monasca/monasca.dita">Monasca Overview</xref></p><p><b>Centralized Logging</b>
        &#151; Logging for the services below is enabled in the default configuration. You will have
        the ability to disable per-service logging as needed.</p><p>Logging is integrated for the
        following services: Nova, Glance, Swift, Neutron, Ceilometer, Monasca, Horizon, Keystone,
        Cinder, Heat, OpsConsole, DNSaaS, LBaaS, FWaaS, and Trove.</p><p>Learn more about
        Centralized Logging here: <xref href="logging/centralized_logging.dita">Centralized Logging
          Overview</xref></p><p><b>New Installer</b></p><ul>
        <li>You supply the cloud configuration you want based on our <xref
            href="supportedconfigs.dita">Supported Configurations</xref> and the installer will set
          up your environment.</li>
        <li>Manage the different phases of your cloud deployment with ease.</li>
      </ul><p><b>Local Git repository for config tracking</b></p><p>In HP Helion OpenStack 2.0
        versions, a local git repository is used to track configuration changes and for the config
        processor to gather configuration settings from. The operations work as explained below:</p><ul>
        <li>Operatiosn under ~/helion are under the aegis of git. You’ll need to commit any config
          into git (on the "site" branch) before the CP can run over it. The script that runs the
          config processor now guards against uncommitted changes. </li>
        <li>Deployment operations are run from a scratch directory that's assembled from various
          parts (the ansible output of the CP and the playbooks). The directory is created using
          "ready-deployment.yml". </li>
        <li>All deployment operations should be run from ~/scratch/ansible/next/hos/ansible.</li>
      </ul>The configuration processor uses the config files stored in the repo to apply
      configuration settings. An explanation can be found <xref
        href="using_git.dita#topic_u3v_1yz_ct">in this topic</xref>.
          <!--
<p><b>Backup and Restore Capabilities for the Control Plane</b> &#154; Allows you to backup and recover your Helion OpenStack</tm> environment with RPO=0.</p>
--><p><b>NIC
          Mapping Feature</b></p><p>The new NIC mappings feature in HP Helion OpenStack Beta 1
        allows the cloud administrator to map network interface names to PCI bus addresses. This is
        important because for well-understood reasons, Linux can sometimes name the network
        interfaces in a pseudo-random order. So in a cluster of identical machines, the <b>eth3</b>
        device can appear as <b>eth4</b> on one or more machines in the cluster. Once this erroneous
        name is determined by the operating system, it will use persistence rules to ensure that the
        NIC device is called <b>eth4</b> thereafter. This makes cluster configuration using Helion
        quite difficult if not impossible, if most machines understand the Management Network to be
          <b>eth3</b>, but some subset of machines understand that network to be on<b> eth4 </b>(and
        whatever was on eth4 is similarly swapped to <b>eth3</b>). </p><p> To resolve this, and to
        ensure that all NIC devices have consistent naming across the entire cloud, Beta 1 includes
        NIC Mapping facilities. This is a mechanism in the Input Model to define a specific NIC
        interface for a given PCI address for a given class of machine. Generally, when all of the
        network interfaces are defined in the Input Model, it is advisable to use the traditional
        naming, ie <b>ethN</b>. However, if one or more interfaces are not defined in the Input
        Model (perhaps because the Ansible network is not defined, or because there is a
        customer-specific network which is outside the scope of Helion), then it is recommended that
        a different naming convention is used, to prevent the definitions from colliding with
        existing configuration data. For example, the NIC mapping definitions could use <b>hos0
        </b>through <b>hosN</b> rather than <b>eth0</b> through <b>ethN</b>. Bearing in mind that
        the new naming must be consistent across the entire Input Model.</p></section>
    <section id="KnownIssues">
      <title>Limitations in this Release</title>
      <p><b>TLS is not supported</b></p>
      <p>TLS is not supported for any traffic in Beta1.</p>
      <p><b>HP Helion Development Platform</b></p>
      <p>Helion Development Platform is not supported on Beta 1.</p>
      <p><b>Installing Swift</b></p>
      <p>Swift installs with the following limitations: </p>
      <ul>
        <li>There is no LVM (logical volumes) support. Therefore, dedicated disks are needed for
          Swift on the 3 controller nodes.</li>
        <li>In Beta 1 you cannot add or remove Swift servers.</li>
        <li>Only one Swift zone is supported.</li>
        <li>Only one storage policy, object-0, is supported in this release.</li>
      </ul>
      <!-- <p>
      <b>Deployment of VSA cloud on a separate Node as per the life-cycle management
      features</b></p>
    <p>Completed activity:</p>
    <p> vsa-deploy play to deploy VSA on standalone node</p>
    <p> cmc-deploy play do deploy CMC on controller node [0] </p>
    <p> one-region-poc-with-vsa file with dev environment </p>
    <p>Impediment:</p>
    <p>The one-region-poc-with-vsa will be provided by the HLM team which impacts not only VSA but
      other services as well. This will be beyond be Beta0 timeframe.</p>
    <p> </p>-->
      <p><b>Helion Cloud Monitoring (Monasca) Limitations</b></p>
      <p>Basic Monitoring is complete for Beta 1 with the following limitations:</p>
      <ul>
        <li>Access to Monasca is limited to the REST API and Command Line Client (CLI) only. GUI
          access will be implemented in later releases. When using the CLI, the prefix for CLI
          commands is "monasca". Use "monasca help" for details.</li>
        <li>Vertica is not available in the install. You must choose the InfluxDB option
          instead.</li>
        <li>InfluxDB is not configured behind a VIP. It must directly access one node in the
          cluster.</li>
        <li>Backup/restore of Monasca configuration and metrics data is not available.</li>
      </ul>
      <p><b>Operations Console (Ops Console)</b></p>
      <p>The Operations Console is available on port 9095 on the Control Plane. There are some
        limitations to note and they are listed below by menu:</p>
      <ul>
        <li>Dashboard <ul>
            <li>There are two notification sections for the Storage services. The one in the upper
              left is the Compute service mislabeled as Storage.</li>
            <li>The New Alarms section uses the UTC timezone to determine timeframes regardless of
              what timezone set in the settings.</li>
            <li>The New Alarms section will only display alarms that have a service dimension
              defined.</li>
            <li>Platform Services (HDP Summary) alarm panels are not yet implemented but appear in
              the menu and thus will not populate if selected.</li>
          </ul></li>
        <li>Logging Dashboard <ul>
            <li>There is no auto-logon function, users must use the credentials in the <xref
                href="logging/centralized_logging.dita#logging/interface">Logging Overview</xref> to
              access the UI.</li>
          </ul></li>
        <li>Alarms <ul>
            <li>The tile view of alarms with lengthy dimensions does not render cleanly.</li>
          </ul></li>
        <li>Networking - Alarm Summary <ul>
            <li>Alarms may be limited to only those with the dimension "service=networking".</li>
          </ul></li>
        <li>Storage - Alarm Summary <ul>
            <li>Alarms with the dimension "service=compute" sometimes appear in this list in
              addition to the "service=block-storage" and "service=object-storage" ones.</li>
          </ul></li>
        <li>HDP - Alarm Summary <ul>
            <li>This section is not implemented in Beta 1</li>
          </ul></li>
        <li>Masthead Items <ul>
            <li>The Help and EULA links will not work properly. You can view the Help files here:
                <xref href="monasca/opsconsole/overview.dita">Operations Console
              Overview</xref></li>
            <li>User information panel fails to load making it so a user cannot update their email
              address or password.</li>
          </ul></li>
      </ul>
      <p><b>Centralized Logging Limitations</b></p>
      <p>Logging support is complete for Beta 1 with the following limitations:</p>
      <ul>
        <li>Alarm definition and generation for Kibana and curator are not integrated.</li>
        <li>Logging-monitor 'stop' playbook is not integrated.</li>
      </ul>
      <p><b>Neutron Feature Limitations</b></p>
      <ul>
        <li>VPNaaS (Virtual-Private-Network-as-a-Service) is not enabled in this release.</li>
      </ul>
      <!--<p><b>Networks</b></p>
      <p>
        <ul id="ul_rlg_dbg_ys">
          <li>Multiple NICs on a compute node and each NIC is connected to a different external
            network. Today we don't support multiple external networks at L2 (ML2 OVS mech driver)
            nor L3 (DVR or Centralized).</li>
        </ul>
      </p>-->
      <!--<p>Need use cases/stories </p>
    <p>* ability to span L3</p>
    <p>* multiple provider networks/separation of traffic</p>
    <p>* two pools for fip to span networks/different fip pools</p>
    <p>* talk to multiple networks</p>
    <p> </p>
    <p>[06/02/15]</p>
    <p>We discussed today that all of the use cases should be fulfilled by HOS2.0.</p>
    <p> </p>-->
      <!--  <p><b>Cinder</b></p><ul id="ul_vld_l1g_ys">
        <li>
          <p>Core cinder services are all being installed and configured via the installer. </p>
        </li>
        <li>
          <p>Cinder installation with VSA as a backend is being worked through at the moment, manual
            steps to be defined later may be required to complete installation.</p>
        </li>
      </ul>-->
     
      <p><b>Network Interface Bonding changes needed during installation</b></p>
      <ul>
        <li>The bond mode in the network interface definition needs to be specified by name
          (active-backup) rather than by number (1).</li>
        <li>This workaround is included in the installation instructions.</li>
      </ul>
      <p><b>Cinder Limitations</b></p>
      <ul>
        <li>3Par storage is not supported in Beta 1</li>
      </ul>
    </section>
    <section id="troubleshooting"><title>Troubleshooting</title>
      <p><b>Wait for SSH phase in bm-reimage.yml hangs </b></p>
      <p>This issue has been observed during deployment to systems configured with Qlogic based
        BCM578XX network adapters utilizing the bnx2x driver and is currently under investigation.
        The symptom manifests following a cold boot during deployment at the "wait for SSH" phase in
        bm-reimage.yml which will result in a hang and eventually timeout, causing the baremetal
        install to fail. The presence of this particular issue can further confirmed by connecting
        to the remote console of the server via iLO or checking the dmesg file for the presence of
        bnx2_panic_dump messages, similar to the following:
        <codeblock>bnx2x: [bnx2x_prev_unload_common:10433(eth%d)]Failed to empty BRB, hope for the best  ...
bnx2x: [bnx2x_stats_update:1268(eth0)]storm stats were not updated for 3 times
bnx2x: [bnx2x_stats_update:1269(eth0)]driver assert
bnx2x: [bnx2x_panic_dump:929(eth0)]begin crash dump -----------------  .....
bnx2x: [bnx2x_panic_dump:1163(eth0)]end crash dump ----------------- 
</codeblock>
        This issue is resolved by rebooting the server. </p>
      <p><b>Freezer installation fails if an independent network is used for the External_API.</b></p>
      <p> Currently the Freezer installation fails if an independent network is used for the
        External_API. If you intend to deploy the External API in Beta 1 on an independent network,
        the following changes need to be made: </p>
      <p>In <b>roles/freezer-agent/defaults/main.yml</b> add the following line:
        <codeblock>backup_freezer_api_url: "{{ FRE_API | item('advertises.vips.private[0].url', default=' ') }}"</codeblock>
        In <b>roles/freezer-agent/templates/backup.osrc.j2</b> add the following line:
        <codeblock>export OS_FREEZER_URL={{ backup_freezer_api_url }}</codeblock>
      </p>
    </section>
  </body>
</topic>
