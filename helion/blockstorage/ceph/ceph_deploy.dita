<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="install_kvm_ceph">
  <title>HP Helion <tm tmtype="reg">OpenStack</tm> 2.0: Installation for KVM Hypervisor and
    Ceph</title>
  <body>
    <p>This page describes configuration, installation, and the integration of Ceph Block Storage
      with HP Helion<tm tmtype="reg">OpenStack</tm>2.0. After the installation of Ceph you can
      perform the cinder operation as documented in this page. It also provides the procedure to
      perform cinder operation after Ceph deployment.</p>
    <p><!--Ceph has two components, i..e., monitor and OSD. These components require different set of hardware specifications as follows:<ul id="ul_pgh_qjv_kt"><li>For monitor node, storage is not required but it is CPU sensitive. </li><li>For OSD nodes, a significant disk media, RAM and reasonable CPU are required.<p>Therefore, with this requirement we recommend the following:<ul id="ul_rl1_jkv_kt"><li>SL4540 for OSD</li><li>SL230 series of servers for monitor if monitors are not deployed on controller node which is our default configuration. If monitor is deployed on controller node, the server used for controlller node is good neough to support monitor. </li></ul></p></li></ul>--></p>
    <!--<p>Ceph has two component with differing need of hardware specifications: monitor and OSD. Monitors does not need storage but CPU sensitive. OSD needs significant disk media, RAM and reasonable CPU. Keeping this aspect in mind, we do recommend following: SL4540 for OSD DL380 or SL230 series of servers for monitor if monitors are not deployed on controller node which is our default configuration. If monirot is deployed on controller node, the server used for controlller node is good neough to support monitor </p>-->
    <section>
      <title id="preq">Prerequisite</title>
      <p>The deployer node must be setup before deploying Ceph. For more details on the installation
        of deployer node, refer to <xref href="../../install_entryscale_kvm_vsa.dita#install_kvm"
          >installation guide.</xref></p>
    </section>
    <section>
      <title id="install-procedure">Installation Procedure</title>
      <p>Perform the following procedures to configure, install, and the integrate the Ceph Block
        Storage with HP Helion OpenStack 2.0.</p>
    </section>
    <section>
      <p>
        <ol id="ol_evc_k11_gt">
          <li>Login to the deployer node.</li>
          <li>Change the directory as shown below:<codeblock>cd ~/helion</codeblock></li>
          <li>Edit <codeph>cinder.conf.j2</codeph> file at
              <codeph>my_cloud/config/cinder/cinder.conf.j2</codeph> to add Ceph backend:</li>
          <li>Edit the file <codeph>disks_osd.yml</codeph> and enter the details for the additional
            disks meant for OSD data and journal
              filesystems.<codeblock>vi <codeph>disks_osd.yml</codeph></codeblock><note>Ensure that
              disks designated to be used for the OSD data and journal storage must be in a clean
              state (i.e. any existing partitions must be deleted).</note></li>
          <li>Editable parameters for Ceph are available in the following locations:<ul
              id="ul_jbn_nj5_kt">
              <li><codeph>~/helion/my_cloud/config/ceph/settings.yml</codeph><p>In the
                    <codeph>settings.yml</codeph> file, you can edit the following parameters:<ul
                    id="ul_ocp_ty5_kt">
                    <li>fsid</li>
                    <li>ceph clusters</li>
                    <li>osd_settle_time</li>
                    <li>OSD_journal_size</li>
                  </ul></p></li>
              <li>
                <p><codeph>~/helion/my_cloud/config/ceph/user_model.yml</codeph></p>
                <p>The <codeph>user_model.yml</codeph> has the editable values for the different
                  pools created by HP Helion OpenStack.</p>
              </li>
            </ul></li>
          <li> Commit your configuration to a <xref href="../../using_git.dita">local
              repository</xref>:<codeblock>cd ~/helion/hos/ansible
git add -A
git commit -m "&lt;commit message>"</codeblock><note>
              Enter your commit message &lt;commit message></note></li>
          <li>Run the configuration
            processor:<codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
          <li>Run the following command to create a deployment
            directory.<codeblock>cd ~/helion/hos/ansible
<codeph>ansible-playbook -i hosts/localhost ready-deployment.yml</codeph></codeblock></li>
          <li>Modify <codeph>~/helion/hlm/ansible/hlm-deploy.yml</codeph> and uncomment the line
            containing <codeph>ceph-deploy.yml</codeph> to enable deployment of ceph.</li>
          <li>Run the following ansible
            playbook:<codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts site.yml</codeblock></li>
        </ol>
      </p>
    </section>
    <p>Ceph Monitor service is deployed on the Controller Nodes and OSD's are deployed as separate
      nodes (Resource Nodes).</p>
    <p id="run-ceph-client-package"><b>Run Ceph Client Packages</b></p>
    <p>Ceph can be used as backend for nova-compute, glance, cinder-volume and cinder-backup. The
      nodes running these services should have a client installed on it. Use
        <codeph>ceph-client-prepare.yml</codeph> to deploy client on respective nodes.</p>
    <p><!--Execute the following command to install the Ceph client packages on controller nodes. --></p>
    <p>
      <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts ceph-client-prepare.yml</codeblock>
    </p>
    <p>This will also create Ceph users and Ceph pools on the resource nodes.</p>
  </body>
</topic>
