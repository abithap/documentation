<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="topic_e32_tm2_rt">
  <title>HP Helion <tm tmtype="reg">OpenStack</tm> 2.0: Removing a VSA Node For Maintenance</title>
  <body>
    <!--Needs Work; fnf edit on 10/27; missing some references/links; need to clarify step 2 in the second procedure.-->
    <p>Use this process to remove a storage system/VSA node from a cluster/management group for
      maintenance using the HP StoreVirtual Management Console (CMC). After maintenance, the VSA
      node is added back to the cluster.</p> <p><note type="caution"> <p>To make the volumes available even
        after the VSA storage node goes offline, you must ensure that the VSA network RAID
        configuration/data protection aspects are configured properly prior to removing a VSA node.</p>
          <ul id="ul_a2n_kp2_rt">
          <li>To configure RAID, see the "Using disk RAID with Network RAID in a cluster," "Planning
            the RAID configuration," and "Data protection" sections of the HP StoreVirtual Storage
            Online help (available in the <b>StoreVirtual Management Console</b>). You must
            configure RAID before adding the storage system to the management group. Refer the
            online help of <b>HP StoreVirtual CMC for configuring the network RAID for storage
              systems.</b></li>
          <li>You must set the data protection level while creating the volume type for the VSA
            volumes in OpenStack Cinder. For instructions, see the OpenStack configuration guide
              <xref
              href="http://docs.openstack.org/kilo/config-reference/content/HP-LeftHand-StoreVirtual-driver.html"
              format="html" scope="external">HP LeftHand/StoreVirtual driver</xref>.</li>
        </ul></note></p>
    <section>
      <title>To Remove a VSA Storage System for Maintenance</title>
      <p>Perform the following steps to remove a VSA storage system for maintenance:</p><ol
          id="ol_rpx_wp2_rt">
          <li>Ensure that the volume is available if the storage system/VSA node to be removed goes
            offline. For instructions, see the "Determining volume and snapshot availability"
            section in the <b>HP StoreVirtual Storage Online help</b>.</li>
          <li>If the repair option is available for the VSA storage system, choose the appropriate
            option in repair. For instructions, see the "Using Repair Storage System" section in the
              <b>HP StoreVirtual Storage Online help in CMC</b>.<p>OR</p><p>If the repair option is
              unavailable, perform either of the below steps to remove the VSA storage system for
              maintenance:</p><p>Perform any of the below operations to ensure that the cluster
              functions normally and the volumes continue to be available even after the storage
              system is removed/replaced. (For instructions see "Maintaining storage systems in
              clusters" section in the <b>HP StoreVirtual Storage Online help in CMC</b>).</p><p>
              <ul id="ul_dmj_sq2_rt">
                <li>Add and remove storage systems for an existing cluster (the new storage system
                  from the list of available VSA storage systems as shown in CMC). </li>
                <li>Exchange a storage system in a cluster. </li>
                <li>Remove a storage system from a cluster (provided the remaining storage system is
                  sufficient for the data availability, a minimum of three nodes should be available
                  after the remove operation). Once the maintenance is done, the storage system may
                  be added back to the cluster. For instructions, see the "Adding a storage system
                  to a cluster" section.</li>
              </ul>
            </p></li>
        </ol>
    </section>
    <p>
      <note type="important">While removing a VSA storage system permanently, the operator needs to
        ensure there is an odd number of managers running to avoid split brain syndrome. For
        example, if there are five nodes, and one node is getting removed, the operator needs to
        stop the manager running on any of the remaining four nodes</note>
    </p>
    <section>
      <title><b>To exclude the VSA node that's offline for further reconfiguration/updates until
          it's back online</b></title>
      <p>Even after the removable of a VSA node, it should continue to remain in the
          <codeph>data/servers.yml</codeph> file. You should also create a
          <codeph>servers.yml</codeph> file on the deployer/lifecycle-manager node with a list of
        VSA nodes that are currently offline so that these nodes get excluded from any further
        reconfiguration or upgrades. This prevents a VSA node that is offline from being included in
        deployments.</p>
      <p>Perform the following steps to remove the VSA node for maintenance.<ol id="ol_xft_3hp_5t">
        <li>Login to the Deployer/lifecycle-manager node.</li>
        <li>Verify that the node that needs to be kept under maintenance from
              <codeph>hosts/verb_hosts</codeph>
            file:<codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts</codeblock></li>
        <li>Create a file (for example, @offline-vsa) and enter the node information that needs to
            be placed under maintenance. For example, if you want to place
              <codeph>padawan-ccp-vsa001</codeph> under maintenance, then you must use the host name
            along with ! and * (i.e., <codeph>!padawan-ccp-vsa001*</codeph>).</li>
        <li>Execute the following
              command:<codeblock>ansible-playbook -i hosts/verb_hosts osconfig-run.yml --limit &lt;hostname></codeblock><p>In
              the following example, <b>offline-vsa</b> file is created with the VSA host name:
              <codeblock>ansible-playbook -i hosts/verb_hosts osconfig-run.yml --limit @offline-vsa</codeblock></p><p>The
              node which is included in the offline-vsa is placed under maintenance.</p></li>
        <li>Run the following command:
              <codeblock>ansible-playbook -i hosts/verb_hosts site.yml --limit &lt;hostname></codeblock><p>For
              example:</p><codeblock>ansible-playbook -i hosts/verb_hosts site.yml --limit @offline-vsa</codeblock><p>When
              performing Helion lifecycle management, deployments specify the name of this file in a
                <codeph>--limit</codeph> option to prevent the deployment process trying to access
              the VSA nodes that are offline.</p></li>
      </ol></p>
    </section>
  </body>
</topic>
