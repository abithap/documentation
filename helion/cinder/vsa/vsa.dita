<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="topic_gmc_dgk_jt">
  <title> VSA Configuration (Optional)</title>
  <body>
    <p>This page provides detailed instructions on deployment of HP StoreVirtual Appliance (VSA)
      Storage Nodes, and their configuration as Cinder backend.</p>
    <section>
      <title id="preq">Prerequisite</title>
      <p>
        <ul id="ul_pfl_bbz_ht">
          <li>HP Helion Cloud must be successfully deployed. It should be up and running.</li>
          <li>Remember to note down the IP address of VSA VM and VSA Cluster from
              <codeph>~/scratch/ansible/last/my_cloud/stage/info/net_info.yml</codeph> file, which
            is generated by configuration
            processor.<!-- (It is recommended that you write down the IP address so that it can be easily accessible).--></li>
        </ul>
      </p>
      <p>You can deploy VSA with adoptive optmization (AO) and without adoptive optmization. The
        process to deploy VSA with AO and without AO remains similar except the change in the disk
        input model. For more detailed information, refer to <xref
          href="#topic_gmc_dgk_jt/deploy-vsa-with-ao-without-ao" format="dita">VSA with AO and
          without AO</xref>.</p>
    </section>
    <section>
      <title>Installing VSA and CMC</title>
      <p>Perform the following steps to deploy VSA and CMC:<ol id="ol_wgh_djk_jt">
          <li>Login to deployer node.</li>
          <li>Execute the following command to install VSA:
              <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts vsa-deploy.yml</codeblock><p>Once
              VSA is successfully deployed then you can execute the ansible playbook to install CMC.
              CMC is required to configure the HP StoreVirtual VSA nodes .</p></li>
          <li>Execute the following command to install
            CMC:<codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts cmc-deploy.yml</codeblock></li>
        </ol><b> Install (any) X Display Tool to Launch Centralized Management Console (CMC)</b></p>
      <p>You must configure X display tool to launch CMC. User can select <b>any</b> X display tool.
        In this section we are using <b>Xming</b> tool as an example to launch CMC. The following
        example provides the steps to install Xming and launch CMC.</p>
      <p><!--You must configure X display to launch CMC. Therefore, CMC is required to configure the HP StoreVirtual VSA nodes. In this section we are referring to Xming as a X display tool. The display tool can vary from one user to another.--></p>
      <p><!--The follwowing steps provide an example on the configuration of Xming to launch CMC.--></p>
      <p>
        <!--In order to configure the HP StoreVirtual VSA nodes, you must first install the CMC. The following example provides the steps for installaing CMC. The Xming is a tool we are referring in the example below-->
        <ol id="ol_alr_mdl_jt">
          <li>Install <b>Xming</b> on Windows Box from where you can access the deployer node.</li>
          <li>Select <b>Enable X11 forwarding</b> checkbox on the putty session for deployer node. </li>
          <li>SSH to first control plane node.<codeblock>ssh -X</codeblock></li>
          <li>Execute the following command:
            <codeblock>/opt/HP/StoreVirtual/UI/jre/bin/java -jar /opt/HP/StoreVirtual/UI/UI.jar</codeblock>CMC
            will be launch successfully.</li>
        </ol>
      </p>
    </section>
    <section>
      <title>Creating a StoreVirtual Cluster and adding it to a new Management GroupCluster using
        CMC</title>
      <!--<p><note type="important"><?oxy_custom_start type="oxy_content_highlight" color="255,255,0"?>Ensure that CMC is installed successfully and HP StoreVirtual VSA (one or more) is deployed in the same management network where CMC is installed.<?oxy_custom_end?></note></p>-->
      <p>Perform the following steps to create the cluster.<ol id="ol_yln_fhl_jt">
          <li>Open CMC.<p>By default, the CMC is configured to discover the StoreVirtual nodes in
              the subnet in which it is installed. You can manually add the nodes also.</p></li>
          <li>In the CMC UI, click <b>Find</b> and then click <b>Find Systems</b> from the left
            panel. Find Systems dialogue box is displayed.</li>
          <li>You can choose <b>Add</b> or <b>Find</b> option to search the system. Find option
            starts searching for the nodes in the same subnet as that of CMC. Add option displays an
              <b>Enter IP</b> pop-up box to enter the IP of the StoreVirtual system.</li>
          <li>In the CMC UI, click <b>Tasks</b> then point to <b>Management Group</b> and click<b>
              New Management Group</b>.</li>
          <li>In the <b>New Management Group Name</b> box, enter a name for the management group.
            For example: <codeph>hlm003-vsa-mg</codeph></li>
          <li>Click <b>Next</b> to display the Add Administrative User page.</li>
        <li>Enter the required information (username/password) for the Administrative User.<p>For
              example:</p><ul id="ul_cdk_xjl_jt">
              <li>username- stack </li>
              <li>password-stack</li>
            </ul></li>
          <li>Click <b>Next</b> to display the Management Group Time page. </li>
          <li>Add NTP server information and click <b>Next</b> to display the Domain Name Server
            Configuration page.</li>
          <li>Skip the DNS and SMTP sections.</li>
          <li>Click <b>Next</b> to display a Wizard in the Create a Cluster page.</li>
          <li>Select the cluster type as <i>Standard Cluster</i> from the displayed options and
            click <b>Next</b>.</li>
          <li>In the <b>Cluster Name</b> box, enter the name of the cluster and click <b>Next</b>.
            For example: <codeph>hlm003-vsa-cluster</codeph>.</li>
          <li>In the Add VIP and Subnet Mask pop-up box, enter the virtual IP and Subnet Mask of the
            cluster in the respective boxes and click <b>OK</b>. The details are displayed in a
            tabular format in the page. <note>The VIP IP can be found as the cluster IP value in the
                <codeph>~/scratch/ansible/next/my_cloud/stage/info/net_info.yml</codeph>
              file.</note></li>
          <li>Click <b>Next</b> to go to the next page.</li>
          <li>Select the checkbox displayed against <b>Skip Volume Creation</b>.</li>
          <li>Click <b>Next</b>.The Management Group and Cluster is created and displays in the Home
            page of CMC.<p>Wait till the cluster is displayed in the Home Page of CMC.</p></li>
        </ol></p>
    </section>
    <section>
      <title>Configure VSA as a Cinder Backend</title>
      <p>You must modify <codeph>cinder.conf.j2</codeph> to update VSA cluster.</p>
      <p>Perform the following steps to update the VSA cluster:<ol id="ol_dj2_qcz_ht">
          <li>Login to deployer node.</li>
          <li>Change the directory as shown below:<codeblock>cd ~/helion</codeblock></li>
          <li>Edit <codeph>cinder.conf.j2</codeph> file at
              <codeph>my_cloud/config/cinder/cinder.conf.j2</codeph> to add VSA data as
            follows:<codeblock>enabled_backends=vsa-1</codeblock></li>
          <li>Copy VSA section, uncomment it, and modify the values in the
              <codeph>cinder.conf.j2</codeph> file as shown in the following
              example:<codeblock id="vssa-config">[vsa-1]
hplefthand_password: &lt;password given during cluster creation>
hplefthand_clustername: &lt;VSA cluster name>
hplefthand_api_url: https://&lt;store virtual virtual ip>:8081/lhos
hplefthand_username: &lt;username given during cluster creation>
hplefthand_iscsi_chap_enabled: true
volume_backend_name: &lt;volume backend name>
volume_driver: cinder.volume.drivers.san.hp.hp_lefthand_iscsi.HPLeftHandISCSIDriver
hplefthand_debug: false</codeblock><p><!--where,<ul id="ul_swt_2mb_lt"><li>hplefthand_password :  Password given during cluster creation.</li><li>hplefthand_clustername: Name of the VSA cluster.</li><li>hplefthand_api_url: Virtual IP of Store Virtual.</li><li>hplefthand_username: Username given during cluster creation.</li><li>hplefthand_iscsi_chap_enabled : </li><li>volume_backend_name: Name of the backend.</li><li>volume_driver</li><li>hplefthand_debug</li></ul>--></p><note>
              Please enter the parameter values which are specific to your environment. </note></li>
          <li>Commit your configuration to a <xref href="../../using_git.dita#topic_u3v_1yz_ct"
              >local
              repository</xref>:<codeblock>cd ~/helion/hos/ansible
git add -A
git commit -m "your commit message"</codeblock><note>
              Enter your commit message in quotes </note></li>
          <li>Run the configuration
            processor:<codeblock>ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
          <li>Run the following command to create a deployment
            directory.<codeblock><codeph>ansible-playbook -i hosts/localhost ready-deployment.yml</codeph></codeblock></li>
        </ol></p>
    </section>
    <p id="run-ceph-client-package"><b>Run Cinder Reconfigure Packages</b></p>
    <p>Execute the following command to configure VSA. </p>
    <p>
      <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts cinder-reconfigure.yml</codeblock>
    </p>
    <p>You must verify the VSA configuration as backend by creating the volume and volume type. Once
      you are able to create the volume, you can delete the created volume. For more information,
      refer to <xref href="../verify_backend_configuration.dita#topic_q5q_trz_jt">Verify Backend
        Configuration</xref>.</p>
    <note type="important"> You can create more than one VSA cluster of same or different type by
      specifying the configuration in cloud model. For more details, refer <xref
        href="vsa_create_multiple_clusters.dita#topic_q3v_krq_jt">Modifying Cloud Model to Create
        Multiple Clusters</xref>.</note>
    <section id="deploy-vsa-with-ao-without-ao"><b>VSA with AO and without AO</b><p>VSA is deployed
        with adoptive optmization (AO) and without AO. AO allows in-built storage tiring for VSA.
        While deploying VSA with or without AO you must ensure to use appropriate disk input
        model.</p><p>VSA with AO will have extra disk section with the marked usage of
        adoptive-optimization as mentioned in the following
        example:<codeblock>Additional disks can be added if available
device_groups:
  - name: vsa-data
    consumer:
      name: vsa
      usage: data
    devices:
      - name: /dev/sdc
     - name: /dev/sdd
      - name: /dev/sde
     - name: /dev/sdf

  - name: vsa-cache
    consumer:
      name: vsa
      usage: <b>adaptive-optimization</b>
    devices:
      - name: /dev/sdb</codeblock></p><p>VSA
        without AO consist of data disks as mentioned in the following
        example:<codeblock>Additional disks can be added if available
device_groups:
  - name: vsa-data
    consumer:
      name: vsa
      usage: data
    devices:
      - name: /dev/sdc
     - name: /dev/sdd
      - name: /dev/sde
     - name: /dev/sdf</codeblock></p><p>It
        is recommended to use SSD disk for AO. The minimum VSA deployment with AO is 2 (apart from
        OS disk) and 1 for VSA without AO. However, the maximum supported disk is 7.
        <!--, VSA will have 1-6 disks as data disks and 1 disk as VSA with AO configuration. --></p><!--<p>VSA deployment can done with AO or without AO. AO stands for adoptive optmization which allows in-built storage tiring for VSA. From the deployment perspective, deployment of VSA with or without AO is not having much difference excepy in disk input model.For example: a VSA with AO will have to have extra disk section with the marked usage of adoptive-optimization mentioned below. Additional disks can be added if availiable device_groups:   - name: vsa-data     consumer:       name: vsa       usage: data     devices:       - name: /dev/sdc - name: /dev/sdd - name: /dev/sde - name: /dev/sdf   - name: vsa-cache     consumer:       name: vsa       usage: adaptive-optimization     devices:       - name: /dev/sdb For VSA without AO, we do need to describe only data disks as mentioned below: Additional disks can be added if availiable device_groups:   - name: vsa-data     consumer:       name: vsa       usage: data     devices:       - name: /dev/sdc - name: /dev/sdd - name: /dev/sde - name: /dev/sdf idealy, it is recommended to use SSD disk for adoptive-optimization purpose. So, a typical VSA will have 1-6 disks as data disks and 1 disks as VSA with AO configuration. From above it is clear that minimum disk for VSA deployment with AO is 2 (apart from OS disk) and 1 for VSA without AO </p>--></section>
  </body>
</topic>
