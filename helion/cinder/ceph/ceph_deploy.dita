<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="topic_jrc_5lz_ft">
  <title id="install-ceph"> Ceph Configuration </title>
  <body>
    <p>This page describes configuration, installation, and the integration of Ceph Block Storage
      with HP Helion<tm tmtype="reg">OpenStack</tm>2.0. After the installation of Ceph you can
      perform the cinder operation as documented in this page. It also provides the procedure to
      perform cinder operation after Ceph deployment.</p>
    <p><!--Ceph has two components, i..e., monitor and OSD. These components require different set of hardware specifications as follows:<ul id="ul_pgh_qjv_kt"><li>For monitor node, storage is not required but it is CPU sensitive. </li><li>For OSD nodes, a significant disk media, RAM and reasonable CPU are required.<p>Therefore, with this requirement we recommend the following:<ul id="ul_rl1_jkv_kt"><li>SL4540 for OSD</li><li>SL230 series of servers for monitor if monitors are not deployed on controller node which is our default configuration. If monitor is deployed on controller node, the server used for controlller node is good neough to support monitor. </li></ul></p></li></ul>--></p>
    <!--<p>Ceph has two component with differing need of hardware specifications: monitor and OSD. Monitors does not need storage but CPU sensitive. OSD needs significant disk media, RAM and reasonable CPU. Keeping this aspect in mind, we do recommend following: SL4540 for OSD DL380 or SL230 series of servers for monitor if monitors are not deployed on controller node which is our default configuration. If monirot is deployed on controller node, the server used for controlller node is good neough to support monitor </p>-->
    <section>
      <title id="preq">Prerequisite</title>
      <p>The deployer node must be setup before deploying Ceph. For more details on the installation
        of deployer node, refer to <xref href="../../bare_installation_kvm.dita#install_kvm"
          >installation guide.</xref></p>
    </section>
    <section>
      <title id="install-procedure">Installation Procedure</title>
      <p>Perform the following procedures to configure, install, and the integrate the Ceph Block
        Storage with HP Helion OpenStack 2.0.</p>
    </section>
    <section>
      <p>
        <ol id="ol_evc_k11_gt">
          <li>Login to the deployer node.</li>
          <li>Create the Ceph configuration files (<b>manually</b>) at
              <codeph>~/helion-input/my_cloud/definition</codeph>. The example configuration files
            are available at <xref href="configuration_files.dita#config_files">Ceph
              Configuration
            files</xref>.<!--<p>The configuration files for editing will be available at <codeph>~/helion/my_cloud/definition/data</codeph>.</p>--></li>
          <li>Copy the configuration files in the respective folder as shown in the screen shot
              below.<p/><image href="../../../media/ceph/screenshot-path.png" id="image_arv_dcx_gt"
              /><p>You must copy the <b>yml</b> files in the appropriate folder. For example: Copy
              the <codeph>ring.yml</codeph> file in <codeph>config/swift/</codeph>.</p></li>
          <li>Edit the configuration files, based on your environment, to deploy Ceph. </li>
          <li>Edit the file <codeph>disks_osd.yml</codeph> and enter the details for the additional
            disks meant for OSD data and journal
            filesystems.<codeblock>vi <codeph>disks_osd.yml</codeph></codeblock></li>
          <li>Editable parameters for Ceph are available in the following locations:<ul
              id="ul_jbn_nj5_kt">
              <li><codeph>/home/stack/helion/my_cloud/config/ceph/settings.yml</codeph><p>In the
                    <codeph>settings.yml</codeph> file, you can edit the following parameters:<ul
                    id="ul_ocp_ty5_kt">
                    <li>fsid</li>
                    <li>ceph clusters</li>
                    <li>osd_settle_time</li>
                    <li>OSD_journal_size</li>
                  </ul></p></li>
              <li>
                <p><codeph>/home/stack/helion/my_cloud/config/ceph/usermodel.yml</codeph></p>
                <p>The <codeph>usermodel.yml</codeph> has the editable values for the different
                  pools created by HP Helion OpenStack.</p>
              </li>
            </ul></li>
          <li> Commit your configuration to a <xref href="../../using_git.dita#topic_u3v_1yz_ct"
              >local
              repository</xref>:<codeblock>cd ~/helion/hos/ansible
git add -A
git commit -m "&lt;commit message>"</codeblock><note>
              Enter your commit message &lt;commit message></note></li>
          <li>Run the configuration
            processor:<codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
          <li>Change the directory to <codeph>~/scratch/ansible/next/hos/ansible</codeph> and run
            the following command to create a deployment
            directory.<codeblock><codeph>ansible-playbook -i hosts/localhost ready-deployment.yml</codeph></codeblock></li>
          <li>Modify <codeph>~/helion/hlm/ansible/hlm-deploy.yml</codeph> and uncomment the line
            containing <codeph>ceph-deploy.yml</codeph> to enable deployment of ceph.</li>
          <li>Run the following ansible
            playbook:<codeblock>ansible-playbook -i hosts/verb_hosts site.yml</codeblock></li>
        </ol>
      </p>
    </section>
    <p>Ceph Monitor service is deployed on the Controller Nodes and OSD's are deployed as separate
      nodes (Resource Nodes).</p>
    <p id="run-ceph-client-package"><b>Run Ceph Client Packages</b></p>
    <p>Ceph can be used as backend for nova-compute, glance, cinder-volume, and cinder-backup. The
      nodes running these services should have a client installed on it. Use
        <codeph>ceph-client-prepare.yml</codeph> to deploy client on respective nodes.</p>
    <p><!--Execute the following command to install the Ceph client packages on controller nodes. --></p>
    <p>
      <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts ceph-client-prepare.yml</codeblock>
    </p>
    <p>This will also create Ceph users and Ceph pools on the resource nodes.</p>
    <p id="configure-ceph-as-cinder-backend"><b>Configure Ceph as a Cinder backend</b></p>
    <p>Perform the following procedure on the Deployer node to configure Ceph as a Cinder backend:<ol>
        <li>Edit <codeph>~/helion/hos/ansible/roles/_CND-CMN/templates/cinder.conf.j2</codeph> to
          add ceph configuration data as shown
          below:<codeblock>enabled_backends=ceph1</codeblock></li>
        <li>Copy the following
            configurations:<codeblock>[ceph1]
rbd_max_clone_depth = 5
rbd_flatten_volume_from_snapshot = False
rbd_uuid = 457eb676-33da-42ec-9a8c-9293d545c337
rbd_user = cinder
rbd_pool = volumes
rbd_ceph_conf = /etc/ceph/ceph.conf
volume_driver = cinder.volume.drivers.rbd.RBDDriver
volume_backend_name = ceph</codeblock><note>The
              <b>rbd_uuid </b>is same as available at
              <codeph>/home/stack/helion/my_cloud/config/ceph/settings.yml</codeph>.<?oxy_custom_start type="oxy_content_highlight" color="255,255,0"?></note></li>
        <?oxy_custom_end?>
        <li> Copy <codeph>ceph.client.cinder.keyring</codeph> to the controller nodes:<ol
            id="ol_ihn_2l5_kt">
            <li>Login to controller node as a root user and execute the following
                  command.<codeblock>ceph auth get-or-create client.cinder | tee /etc/ceph/ceph.client.cinder.keyring</codeblock><p><b>OR</b></p><p>You
                can copy the keyring from the deployer
                node<codeblock>scp /etc/ceph/ceph.client.cinder.keyring</codeblock> to
                  <codeph>/etc/ceph</codeph> folder on all the controller nodes.</p></li>
          </ol></li>
        <li>To enable cinder backup to Ceph, modify <codeph>cinder.conf.j2</codeph> at
            <codeph>/home/stack/helion/my_cloud/config/cinder</codeph> with the following values:
          <codeblock>backup_driver = cinder.backup.drivers.ceph
backup_ceph_conf = /etc/ceph/ceph.conf
backup_ceph_user = cinder-backup
backup_ceph_chunk_size = 134217728
backup_ceph_pool = backups
backup_ceph_stripe_unit = 0
backup_ceph_stripe_count = 0
restore_discard_excess_bytes = true</codeblock></li>
        <li> Copy <codeph>client.cinder-backup.keyring</codeph> to the controller nodes:<ol
            id="ol_xx5_51x_gt">
            <li>Login to controller node as a root user and execute the following
                  command.<codeblock>ceph auth get-or-create client.cinder-backup | tee /etc/ceph/ceph.client.cinder-backup.keyring</codeblock><p><b>OR</b></p><p>You
                can copy the keyring from the deployer
                node<codeblock>scp /etc/ceph/<?oxy_custom_start type="oxy_content_highlight" color="255,255,0"?>ceph.client.cinder-backup.keyring<?oxy_custom_end?></codeblock>
                to <codeph>/etc/ceph</codeph> folder on all the controller nodes.</p></li>
          </ol></li>
        <li>Commit your configuration to the local repository to configure cinder on the deployer node.<p>
            <codeblock>cd /home/stack/helion/hos/ansible
git add -A
git commit -m "&lt;commit message>"</codeblock>
          </p><note> Enter your commit message &lt;commit message></note></li>
        <li>Change the directory to <codeph>~/scratch/ansible/next/hos/ansible</codeph> and run the
          following command to create a deployment
          directory.<codeblock><codeph>ansible-playbook -i hosts/localhost ready-deployment.yml</codeph></codeblock></li>
        <li>Run the following ansible
            playbook:<codeblock>ansible-playbook -i hosts/verb_hosts cinder-reconfigure.yml</codeblock><p>Once
            cinder is configured, launch the Horizon dashboard to create a cinder volume type to
            verify the ceph configuration as backend . Once you are able to create the volume, you
            can delete the created volume. For more information on verify the ceph configuration,
            refer to <xref href="../verify_backend_configuration.dita#topic_q5q_trz_jt">Verify
              Backend Configuration</xref>.</p></li>
      </ol></p>
    <section>
      <p><b>Attach Ceph Volume to Instance:</b></p>
      <p>Perform the following steps to attach Ceph volume to an instance: <ol id="ol_vx4_4gv_kt">
          <li>On the deployer node, edit <codeph>kvm-hypervisor.conf.j2</codeph> at
              <codeph>/home/stack/helion/my_cloud/config/nova/kvm-hypervisor.conf.j2</codeph> to add
            the following for RBD volume
              attach:<codeblock>[libvirt]
rbd_user = cinder
rbd_secret_uuid = 457eb676-33da-42ec-9a8c-9293d545c337</codeblock><note>The
                <codeph>rbd_secret_uuid</codeph> is same as available at
                <codeph>/home/stack/helion/my_cloud/config/ceph/settings.yml</codeph>.</note></li>
          <li>Commit your configuration to the local repository to configure cinder on the deployer node.<p>
              <codeblock>cd /home/stack/helion/hos/ansible
git add -A
git commit -m "&lt;commit message>"</codeblock>
            </p><note> Enter your commit message &lt;commit message></note></li>
          <li>Change the directory to <codeph>~/scratch/ansible/next/hos/ansible</codeph> and run
            the following command to create a deployment
            directory.<codeblock><codeph>ansible-playbook -i hosts/localhost ready-deployment.yml</codeph></codeblock></li>
          <li>Run the following ansible
            playbook:<codeblock>ansible-playbook -i hosts/verb_hosts run nova-reconfigure.yml</codeblock></li>
        </ol></p>
    </section>
    <section>
      <title id="ceph-operation">
        <b>Ceph Operations</b></title>
      <p>After the successful deployment of Ceph, you can perform the following Ceph operations:<ul>
          <li>Check the status of Ceph OSD and Monitor Services</li>
          <li>Start OSD Nodes and Monitor Services</li>
          <li>Stop OSD Nodes and Monitor Services<p><b>Check the status of Ceph OSD and Monitor
                Services</b>
            </p><p>Perform the following steps to check the stays of Ceph OSD Nodes and Monitor Services:<ol>
                <li>Login to Deployer Node.</li>
                <li>Change to the following
                  directory:<codeblock>~/scratch/ansible/next/hos/ansible</codeblock></li>
                <li>Run the ansible
                  playbook:<codeblock>ansible-playbook -i hosts/verb_hosts ceph-status.yml</codeblock></li>
              </ol></p><p><b>Start OSD and Monitor Services</b></p><p>Perform the following steps to
              start OSD Nodes and Monitor Services:<ol>
                <li>Login to Deployer Node.</li>
                <li>Change to the following
                  directory:<codeblock>cd ~/scratch/ansible/next/hos/ansible</codeblock></li>
                <li>Run the ansible
                  playbook:<codeblock>ansible-playbook -i hosts/verb_hosts ceph-start.yml</codeblock></li>
              </ol></p><p><b>Stop OSD Nodes and Monitor Services</b></p><p>Perform the following
              steps to stop OSD Nodes and Monitor Services:</p><p>
              <ol>
                <li>Login to Deployer Node.</li>
                <li>Change to the following
                  directory:<codeblock>cd ~/scratch/ansible/next/hos/ansible</codeblock></li>
                <li>Run the ansible
                  playbook:<codeblock>ansible-playbook -i hosts/verb_hosts ceph-stop.yml</codeblock></li>
              </ol>
            </p>
          </li>
        </ul></p>
    </section>
  </body>
</topic>
