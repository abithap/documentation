<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic
  PUBLIC "-//OASIS//DTD DITA Topic//EN" "http://docs.oasis-open.org/dita/v1.1/OS/dtd/topic.dtd" >
<topic id="alarmdefinitions">
  <title>Alarm Definitions</title>
  <shortdesc>List of service-specific alarms and the recommended troubleshooting steps.</shortdesc>
  <body>
    <p>When alarms are triggered it is helpful to review the service logs. Details on how to use the
      logging UI built into the Operations Console can be found here: <xref
        href="../logging/centralized_logging.dita#logging/interface">Logging Interface</xref></p>
    <section id="services"><title>Services</title>
      <p>Alarms for these services are included:</p>
      <ul>
        <li><xref href="#alarmdefinitions/haproxy">HAProxy</xref></li>
        <li><xref href="#alarmdefinitions/heat">Heat</xref></li>
        <li><xref href="#alarmdefinitions/hlinux">HP Linux for HP Helion OpenStack</xref></li>
        <li><xref href="#alarmdefinitions/logging">Logging</xref></li>
        <li><xref href="#alarmdefinitions/monasca">Monasca</xref></li>
        <li><xref href="#alarmdefinitions/mysql">MySQL</xref></li>
        <li><xref href="#alarmdefinitions/neutron">Neutron</xref></li>
        <li><xref href="#alarmdefinitions/nova">Nova</xref></li>
        <li><xref href="#alarmdefinitions/rabbitmq">RabbitMQ</xref></li>
        <li><xref href="#alarmdefinitions/swift">Swift</xref></li>
      </ul>
    </section>
    <section id="ceilometer"><title>Ceilometer</title>
      <p><table frame="all" rowsep="1" colsep="1" id="ceilometer_table">
          <tgroup cols="4">
            <colspec colname="c1" colnum="1" colwidth="1*"/>
            <colspec colname="c2" colnum="2" colwidth="2*"/>
            <colspec colname="c3" colnum="3" colwidth="1*"/>
            <colspec colname="c4" colnum="4" colwidth="2*"/>
            <thead>
              <row>
                <entry>Alarm Name</entry>
                <entry>Description</entry>
                <entry>Likely Cause</entry>
                <entry>Mitigation Tasks to Perform</entry>
              </row>
            </thead>
            <tbody>
              <row>
                <entry>Process Check</entry>
                <entry>process_name=ceilometer-agent-notification</entry>
                <entry>process crashed</entry>
                <entry/>
              </row>
              <row>
                <entry>Process Check</entry>
                <entry>process_name=ceilometer-collector</entry>
                <entry>process crashed</entry>
                <entry/>
              </row>
              <row>
                <entry>Process Check</entry>
                <entry>process_name=ceilometer-agent-central</entry>
                <entry>process crashed</entry>
                <entry/>
              </row>
            </tbody>
          </tgroup>
        </table></p>
    </section>
    <section id="cinder"><title>Cinder</title>
      <p><table frame="all" rowsep="1" colsep="1" id="cinder_table">
          <tgroup cols="4">
            <colspec colname="c1" colnum="1" colwidth="1.0*"/>
            <colspec colname="c2" colnum="2" colwidth="1.0*"/>
            <colspec colname="c3" colnum="3" colwidth="1.0*"/>
            <colspec colname="c4" colnum="4" colwidth="1.0*"/>
            <thead>
              <row>
                <entry>Alarm Name</entry>
                <entry>Description</entry>
                <entry>Likely Cause</entry>
                <entry>Mitigation Tasks to Perform</entry>
              </row>
            </thead>
            <tbody>
              <row>
                <entry>Process Check</entry>
                <entry>process_name=cinder-api</entry>
                <entry>process crashed</entry>
                <entry/>
              </row>
              <row>
                <entry>Process Check</entry>
                <entry>process_name=cinder-scheduler</entry>
                <entry>process crashed</entry>
                <entry/>
              </row>
            </tbody>
          </tgroup>
        </table></p>
    </section>
    <section id="glance"><title>Glance</title>
      <p><table frame="all" rowsep="1" colsep="1" id="glance_table">
          <tgroup cols="4">
            <colspec colname="c1" colnum="1" colwidth="1.0*"/>
            <colspec colname="c2" colnum="2" colwidth="1.0*"/>
            <colspec colname="c3" colnum="3" colwidth="1.0*"/>
            <colspec colname="c4" colnum="4" colwidth="1.0*"/>
            <thead>
              <row>
                <entry>Alarm Name</entry>
                <entry>Description</entry>
                <entry>Likely Cause</entry>
                <entry>Mitigation Tasks to Perform</entry>
              </row>
            </thead>
            <tbody>
              <row>
                <entry>Process Check</entry>
                <entry>component=glance-api</entry>
                <entry>The API is unresponsive</entry>
                <entry/>
              </row>
              <row>
                <entry>Process Check</entry>
                <entry>component=glance-registry</entry>
                <entry>The Glance registry is unresponsive</entry>
                <entry/>
              </row>
            </tbody>
          </tgroup>
        </table></p>
    </section>
    <section id="haproxy">
      <title>HAProxy</title>
      <table frame="all" rowsep="1" colsep="1" id="table_flj_2tl_zs">
        <tgroup cols="4" align="left">
          <colspec colname="c1" colnum="1" colwidth="1.0*"/>
          <colspec colname="c2" colnum="2" colwidth="1.0*"/>
          <colspec colname="c3" colnum="3" colwidth="1.0*"/>
          <colspec colname="c4" colnum="4" colwidth="1.0*"/>
          <thead>
            <row>
              <entry>Alarm Name</entry>
              <entry>Description</entry>
              <entry>Likely Cause</entry>
              <entry>Mitigation Tasks to Perform</entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry>Process Check</entry>
              <entry>process_name=haproxy</entry>
              <entry>Haproxy is not running on this machine.</entry>
              <entry>Restart haproxy on the alarming host.</entry>
            </row>
          </tbody>
        </tgroup>
      </table>
    </section>
    <section id="heat">
      <title>Heat</title>
      <table frame="all" rowsep="1" colsep="1" id="table_k33_xcq_bt">
        <tgroup cols="4">
          <colspec colname="c1" colnum="1" colwidth="1.0*"/>
          <colspec colname="c2" colnum="2" colwidth="1.0*"/>
          <colspec colname="c3" colnum="3" colwidth="1.0*"/>
          <colspec colname="c4" colnum="4" colwidth="1.0*"/>
          <thead>
            <row>
              <entry>Alarm Name</entry>
              <entry>Description</entry>
              <entry>Likely Cause</entry>
              <entry>Mitigation Tasks to Perform</entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry>Process Check</entry>
              <entry>heat-api process check on each node</entry>
              <entry>Process crashed.</entry>
              <entry>Restart process on affected node. Review logs.</entry>
            </row>
            <row>
              <entry>Process Check</entry>
              <entry>heat-api-cfn process check on each node</entry>
              <entry>Process crashed.</entry>
              <entry>Restart process on affected node. Review logs.</entry>
            </row>
            <row>
              <entry>Process Check</entry>
              <entry>heat-api-cloudwatch process check on each node</entry>
              <entry>Process crashed.</entry>
              <entry>Restart process on affected node. Review logs.</entry>
            </row>
            <row>
              <entry>Process Check</entry>
              <entry>heat-engine process check on each node</entry>
              <entry>Process crashed.</entry>
              <entry>Restart process on affected node. Review logs.</entry>
            </row>
            <row>
              <entry>HTTP Status</entry>
              <entry>heat-api local http status check on each node</entry>
              <entry>Process hung or crashed.</entry>
              <entry>Restart process on affected node. Review logs.</entry>
            </row>
            <row>
              <entry>HTTP Status</entry>
              <entry>heat-api-cfn local http status check on each node</entry>
              <entry>Process hung or crashed.</entry>
              <entry>Restart process on affected node. Review logs.</entry>
            </row>
            <row>
              <entry>HTTP Status</entry>
              <entry>heat-api remote http status check only on one node</entry>
              <entry>Process hung or crashed.</entry>
              <entry>Restart process on affected node. Review logs.</entry>
            </row>
            <row>
              <entry>HTTP Status</entry>
              <entry>heat-api-cfn remote http status check only on one node</entry>
              <entry>Process hung or crashed.</entry>
              <entry>Restart process on affected node. Review logs.</entry>
            </row>
          </tbody>
        </tgroup>
      </table>
    </section>
    <section id="hlinux">
      <title>HP Linux for HP Helion OpenStack</title>
      <table frame="all" rowsep="1" colsep="1" id="table_ik5_3km_zs">
        <tgroup cols="4">
          <colspec colname="c1" colnum="1" colwidth="1.0*"/>
          <colspec colname="c2" colnum="2" colwidth="1.0*"/>
          <colspec colname="c3" colnum="3" colwidth="1.0*"/>
          <colspec colname="c4" colnum="4" colwidth="1.0*"/>
          <thead>
            <row>
              <entry>Alarm Name</entry>
              <entry>Description</entry>
              <entry>Likely Cause</entry>
              <entry>Mitigation Tasks to Perform</entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry>CPU Usage</entry>
              <entry>Alarms on high CPU usage</entry>
              <entry>Heavy load or runaway processes</entry>
              <entry>Log onto the box and diagnose heavy CPU usage.</entry>
            </row>
            <row>
              <entry>Disk Inode Usage</entry>
              <entry>Nearly out of inodes for a partition.</entry>
              <entry>Many files on the disk.</entry>
              <entry>Investigate cleanup of data or migration to other partitions.</entry>
            </row>
            <row>
              <entry>Disk Usage</entry>
              <entry>High Disk usage.</entry>
              <entry>Large files on the disk</entry>
              <entry>Investigate cleanup of data or migration to other partitions.</entry>
            </row>
            <row>
              <entry>Host Status</entry>
              <entry>Alerts when a host is unreachable</entry>
              <entry>Host or network is down</entry>
              <entry>If a single host attempt to restart the box. If multiple investigate network
                issues.</entry>
            </row>
            <row>
              <entry>Memory Usage</entry>
              <entry>High memory usage.</entry>
              <entry>Overloaded system or services with memory leaks</entry>
              <entry>Log onto the system investigate high memory users.</entry>
            </row>
            <row>
              <entry>Network Errors</entry>
              <entry>Alarms on a high network error rate.</entry>
              <entry>Bad network or cabling</entry>
              <entry>Take this system out of service until the network can be fixed.</entry>
            </row>
          </tbody>
        </tgroup>
      </table>
    </section>
    <section id="horizon"><title>Horizon</title>
      <p>
        <table frame="all" rowsep="1" colsep="1" id="table_vtf_3md_3t">
          <tgroup cols="4">
            <colspec colname="c1" colnum="1" colwidth="1.0*"/>
            <colspec colname="c2" colnum="2" colwidth="1.0*"/>
            <colspec colname="c3" colnum="3" colwidth="1.0*"/>
            <colspec colname="c4" colnum="4" colwidth="1.0*"/>
            <thead>
              <row>
                <entry>Alarm Name</entry>
                <entry>Description</entry>
                <entry>Likely Cause</entry>
                <entry>Mitigation Tasks to Perform</entry>
              </row>
            </thead>
            <tbody>
              <row>
                <entry>HTTP Status</entry>
                <entry>Alerts when Horizon's login page isn't returned</entry>
                <entry>Apache is not running or misconfiguration</entry>
                <entry>Check that Apache is running; investigate Horizon logs</entry>
              </row>
            </tbody>
          </tgroup>
        </table>
      </p>
    </section>
    <section id="keystone"><title>Keystone</title>
      <p>
        <table frame="all" rowsep="1" colsep="1" id="table_bw1_jmd_3t">
          <tgroup cols="4">
            <colspec colname="c1" colnum="1" colwidth="1.0*"/>
            <colspec colname="c2" colnum="2" colwidth="1.0*"/>
            <colspec colname="c3" colnum="3" colwidth="1.0*"/>
            <colspec colname="c4" colnum="4" colwidth="1.0*"/>
            <thead>
              <row>
                <entry>Alarm Name</entry>
                <entry>Description</entry>
                <entry>Likely Cause</entry>
                <entry>Mitigation Tasks to Perform</entry>
              </row>
            </thead>
            <tbody>
              <row>
                <entry>HTTP Status</entry>
                <entry>component-keystone-api</entry>
                <entry>API is unresponsive</entry>
                <entry/>
              </row>
              <row>
                <entry>Process Check</entry>
                <entry>process_name=keystone-main</entry>
                <entry>Process crashed</entry>
                <entry/>
              </row>
              <row>
                <entry>Process Check</entry>
                <entry>process_name=keystone-admin</entry>
                <entry>Process crashed</entry>
                <entry/>
              </row>
            </tbody>
          </tgroup>
        </table>
      </p>
    </section>
    <section id="logging">
      <title>Logging</title>
      <table frame="all" rowsep="1" colsep="1" id="table_gc2_ndh_1t">
        <tgroup cols="5">
          <colspec colname="c1" colnum="1" colwidth="0.5*"/>
          <colspec colname="c2" colnum="2" colwidth="0.5*"/>
          <colspec colname="c3" colnum="3" colwidth="1*"/>
          <colspec colname="c4" colnum="4" colwidth="1*"/>
          <colspec colname="c5" colnum="5" colwidth="3*"/>
          <thead>
            <row>
              <entry>Alarm Name</entry>
              <entry>Component</entry>
              <entry>Description</entry>
              <entry>Likely Cause</entry>
              <entry>Mitigation Tasks to Perform</entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry>Elasticsearch Unassigned Shards</entry>
              <entry>Elasticsearch</entry>
              <entry>Elasticsearch unassigned shards count &gt; 0</entry>
              <entry>Environment could be misconfigured</entry>
              <entry>
                <ul>
                  <li>If you have unassigned shards, make sure that all servers are at the same
                    version of Elasticsearch.</li>
                  <li>Make sure local (true) is set.</li>
                  <li>You may need to change the number_of_replicas to a different value, then back
                    (e.g. from 2 to 1 and back to 2)</li>
                </ul>
              </entry>
            </row>
            <row>
              <entry>Elasticsearch Field Data Evictions</entry>
              <entry>Elasticsearch</entry>
              <entry>Elasticsearch Field Data Evictions count &gt; 0</entry>
              <entry>Field Data Evictions may be found even though it is nowhere near the limit
                set.</entry>
              <entry>Field Data Evictions may be found even though it is nowhere near the limit set.
                This could be because the maximum cache size specified is split between the
                partitions, so for 16 partitions, any given field data inserted would actually have
                a maximum size of indices.fielddata.cache.size / 16. This value may need to be
                increased.</entry>
            </row>
            <row>
              <entry>Elasticsearch Bulk Pool Rejected</entry>
              <entry>Elasticsearch</entry>
              <entry>Elasticsearch Bulk Pool Rejected count &gt; 0</entry>
              <entry>Thread pool size could be too small.</entry>
              <entry>You may need to increase the threadpool.bulk.queue_size (from 50 to 100, for
                example).</entry>
            </row>
            <row>
              <entry>Elasticsearch Number of Log Entries</entry>
              <entry>Elasticsearch</entry>
              <entry>Elasticsearch Number of Log Entries</entry>
              <entry>The number of log entries may get too large</entry>
              <entry>Kibana may hang if the number of log entries is too large (e.g. above 40,000).
                You may need to keep the page size small enough (about 20,000 results), because if
                it is larger (e.g. 200,000), it may hang the browser.</entry>
            </row>
            <row>
              <entry>Elasticsearch Open File Descriptors </entry>
              <entry>Elasticsearch</entry>
              <entry>Elasticsearch Open File Descriptors &gt; 90%</entry>
              <entry>Configured to allow too many file descriptors to be opened.</entry>
              <entry>Make sure that the limits.conf settings are being used, and adjust them if
                needed. A "ulimit" statement could cause this value to be ignored (overwriting the
                value passed by the ulimit statement).</entry>
            </row>
            <row>
              <entry>Elasticsearch Max Total Indices Size</entry>
              <entry>Elasticsearch</entry>
              <entry>Elasticsearch Total Indices size
                &gt;elasticsearch_max_total_indices_size_in_bytesdefined by user</entry>
              <entry>elasticsearch_max_total_indices_size_in_bytes may be set too low, or you may
                have insufficient resources.</entry>
              <entry>If the total size of all the indices exceeds the size of the
                elasticsearch_max_total_indices_size_in_bytes as you have defined it, you can adjust
                the value if you want to give more space and you have the resources. If not, you may
                need to adjust the Curator to remove indices sooner.</entry>
            </row>
            <row>
              <entry>Process Check</entry>
              <entry>Elasticsearch</entry>
              <entry>Process not running.</entry>
              <entry>Process crashed.</entry>
              <entry>If the Elasticsearch process goes down, try restarting it with:
                <codeblock>sudo systemctl restart elasticsearch</codeblock>
              </entry>
            </row>
            <row>
              <entry>Process Check</entry>
              <entry>Logstash</entry>
              <entry>Process not running.</entry>
              <entry>Process crashed.</entry>
              <entry>If the Logstash process goes down, try restarting it with:
                <codeblock>sudo systemctl restart logstash</codeblock>
              </entry>
            </row>
            <row>
              <entry>Process Check</entry>
              <entry>Beaver</entry>
              <entry>Process not running.</entry>
              <entry>Process crashed.</entry>
              <entry>If the Logstash process goes down, try restarting it with:
                <codeblock>sudo systemctl restart beaver</codeblock>
              </entry>
            </row>
            <row>
              <entry>Process Check</entry>
              <entry>Apache</entry>
              <entry>Process not running.</entry>
              <entry>Process crashed.</entry>
              <entry>If the Logstash process goes down, try restarting it with:
                <codeblock>sudo systemctl restart apache2</codeblock>
              </entry>
            </row>
            <row>
              <entry>Process Check</entry>
              <entry>Kibana</entry>
              <entry>Process not running.</entry>
              <entry>Process crashed.</entry>
              <entry>If the Logstash process goes down, try restarting it with:
                <codeblock>sudo systemctl restart kibana</codeblock>
              </entry>
            </row>
          </tbody>
        </tgroup>
      </table>
    </section>
    <section id="monasca">
      <title>Monasca</title>
      <table frame="all" rowsep="1" colsep="1" id="table_dry_245_zs">
        <tgroup cols="5">
          <colspec colname="c1" colnum="1" colwidth="1.0*"/>
          <colspec colname="c2" colnum="2" colwidth="1.0*"/>
          <colspec colname="c3" colnum="3" colwidth="1.0*"/>
          <colspec colname="c4" colnum="4" colwidth="1.0*"/>
          <colspec colname="c5" colnum="5" colwidth="1.0*"/>
          <thead>
            <row>
              <entry>Alarm Name</entry>
              <entry>Component</entry>
              <entry>Description</entry>
              <entry>Likely Cause</entry>
              <entry>Mitigation Tasks to Perform</entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry>HTTP Status</entry>
              <entry>persister</entry>
              <entry>Persister Health Check</entry>
              <entry>The process has crashed or a dependency is out.</entry>
              <entry>If the process has crashed, restart. If a dependent service is down, address
                that issue.</entry>
            </row>
            <row>
              <entry>HTTP Status</entry>
              <entry>api</entry>
              <entry>API Health Check</entry>
              <entry>The process has crashed or a dependency is out.</entry>
              <entry>If the process has crashed, restart. If a dependent service is down, address
                that issue.</entry>
            </row>
            <row>
              <entry>Kafka Consumer Lag</entry>
              <entry>Kafka</entry>
              <entry>Consumers are falling behind for a particular Kafka topic.</entry>
              <entry>There is a slow down in the system or heavy load.</entry>
              <entry>Look for high load in the various systems. This alert can fire for multiple
                topics or on multiple hosts. Which alarms are firing can help diagnose likely
                causes, ie if all on one machine it could be the machine. If one topic across
                multiple machines it is likely the consumers of that topic, etc.</entry>
            </row>
            <row>
              <entry>Monasca Agent Collection Time</entry>
              <entry>monasca-agent</entry>
              <entry>The time the agent took to collect metrics.</entry>
              <entry>Heavy load on the box or a stuck agent plugin.</entry>
              <entry>Address load issue on the machine. If needed restart the agent.</entry>
            </row>
            <row>
              <entry>Monasca Agent Emit Time</entry>
              <entry>monasca-agent</entry>
              <entry>The time the agent took to send metrics to the Monasca API.</entry>
              <entry>API or the network is slow.</entry>
              <entry>Check the network graphs and look for indication of networks issues. If coming
                from most boxes it is possibly api problems.</entry>
            </row>
            <row>
              <entry>Process Check</entry>
              <entry>Kafka</entry>
              <entry>Process not running.</entry>
              <entry>Process crashed.</entry>
              <entry>Restart process on affected node. Review logs.</entry>
            </row>
            <row>
              <entry>Process Check</entry>
              <entry>notification</entry>
              <entry>Process not running.</entry>
              <entry>Process crashed.</entry>
              <entry>Restart process on affected node. Review logs.</entry>
            </row>
            <row>
              <entry>Process Check</entry>
              <entry>storm</entry>
              <entry>Process not running.</entry>
              <entry>Process crashed.</entry>
              <entry>Restart process on affected node. Review logs.</entry>
            </row>
            <row>
              <entry>Process Check</entry>
              <entry>zookeeper</entry>
              <entry>Process not running.</entry>
              <entry>Process crashed.</entry>
              <entry>Restart process on affected node. Review logs.</entry>
            </row>
            <row>
              <entry>Zookeeper Latency</entry>
              <entry>zookeeper</entry>
              <entry>Zookeeper is experiencing high latency.</entry>
              <entry>Heavy system load.</entry>
              <entry>Check the individual system as well as activity across the entire
                service.</entry>
            </row>
          </tbody>
        </tgroup>
      </table>
    </section>
    <section id="mysql"><title>MySQL</title>
      <table frame="all" rowsep="1" colsep="1" id="table_ft3_bp5_zs">
        <tgroup cols="4">
          <colspec colname="c1" colnum="1" colwidth="1.0*"/>
          <colspec colname="c2" colnum="2" colwidth="1.0*"/>
          <colspec colname="c3" colnum="3" colwidth="1.0*"/>
          <colspec colname="c4" colnum="4" colwidth="1.0*"/>
          <thead>
            <row>
              <entry>Alarm Name</entry>
              <entry>Description</entry>
              <entry>Likely Cause</entry>
              <entry>Mitigation Tasks to Perform</entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry>MySQL Slow Query Rate</entry>
              <entry>MySQL is reporting many slow queries.</entry>
              <entry>System load.</entry>
              <entry>This could be an indication of near capacity limits or an exposed bad query.
                First, check overall system load and then investigate MySQL details.</entry>
            </row>
            <row>
              <entry>Process Check</entry>
              <entry>Alarms when MySQL process is not found.</entry>
              <entry>MySQL crashed.</entry>
              <entry>Restart MySQL on the affected node.</entry>
            </row>
          </tbody>
        </tgroup>
      </table>
    </section>
    <section id="neutron">
      <title>Neutron</title>
      <table frame="all" rowsep="1" colsep="1" id="table_bcj_5p4_1t">
        <tgroup cols="5">
          <colspec colname="c1" colnum="1" colwidth="1.0*"/>
          <colspec colname="c2" colnum="2" colwidth="1.0*"/>
          <colspec colname="c3" colnum="3" colwidth="1.0*"/>
          <colspec colname="c4" colnum="4" colwidth="1.0*"/>
          <colspec colname="c5" colnum="5" colwidth="1.0*"/>
          <thead>
            <row>
              <entry>Alarm Name</entry>
              <entry>Description</entry>
              <entry>Monitor Location</entry>
              <entry>Likely Cause</entry>
              <entry>Mitigation Tasks to Perform</entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry>Process Check</entry>
              <entry>neutron-openvswitch-agent pid check</entry>
              <entry>local node</entry>
              <entry>Process crashed.</entry>
              <entry>Restart service on affected node. Review logs.</entry>
            </row>
            <row>
              <entry>Process Check</entry>
              <entry>neutron-l3-agent pid check</entry>
              <entry>local node</entry>
              <entry>Process crashed.</entry>
              <entry>Restart service on affected node. Review logs.</entry>
            </row>
            <row>
              <entry>Process Check</entry>
              <entry>neutron-dhcp-agent pid check</entry>
              <entry>local node</entry>
              <entry>Process crashed.</entry>
              <entry>Restart service on affected node. Review logs.</entry>
            </row>
            <row>
              <entry>Process Check</entry>
              <entry>neutron-metadata-agent pid check</entry>
              <entry>local node</entry>
              <entry>Process crashed.</entry>
              <entry>Restart service on affected node. Review logs.</entry>
            </row>
            <row>
              <entry>Process Check</entry>
              <entry>neutron-rootwrap pid check</entry>
              <entry>local node</entry>
              <entry>Process crashed.</entry>
              <entry>Currently neutron-rootwrap is only used to run ovsdb-client. To restart this,
                restart neutron-openvswitch-agent on the affected node. Review logs.</entry>
            </row>
            <row>
              <entry>Process Check</entry>
              <entry>neutron-lbaas-agent</entry>
              <entry>local node</entry>
              <entry>Processed crashed.</entry>
              <entry>Restart service on affected node. Review logs. </entry>
            </row>
            <row>
              <entry>Process Check</entry>
              <entry>neutron-lbaasv2-agent</entry>
              <entry>local node</entry>
              <entry>Processed crashed.</entry>
              <entry>Restart service on affected node. Review logs. </entry>
            </row>
            <row>
              <entry>Process Check</entry>
              <entry>neutron-vpn-agent</entry>
              <entry>local node</entry>
              <entry>Processed crashed.</entry>
              <entry>Restart service on affected node. Review logs. </entry>
            </row>
            <row>
              <entry>Process Check</entry>
              <entry>neutron-server pid check</entry>
              <entry>local node</entry>
              <entry>Process crashed.</entry>
              <entry>Restart service on affected node. Review logs.</entry>
            </row>
            <row>
              <entry>HTTP Status</entry>
              <entry>neutron api health check</entry>
              <entry>local node</entry>
              <entry>Process stuck if neutron-server Process Check OK</entry>
              <entry>Restart service on affected node. Review logs.</entry>
            </row>
            <row>
              <entry>HTTP Status</entry>
              <entry>neutron api health check</entry>
              <entry>remote node</entry>
              <entry>Node crashed or connectivity lost if local node HTTP Status OK or
                UNKNOWN</entry>
              <entry>Reboot node or repair network. Review logs.</entry>
            </row>
          </tbody>
        </tgroup>
      </table>
    </section>
    <section id="nova">
      <title>Nova</title>
      <table frame="all" rowsep="1" colsep="1" id="table_bv5_q2h_1t">
        <tgroup cols="4">
          <colspec colname="c1" colnum="1" colwidth="1.0*"/>
          <colspec colname="c3" colnum="2" colwidth="1.0*"/>
          <colspec colname="c4" colnum="3" colwidth="1.0*"/>
          <colspec colname="c5" colnum="4" colwidth="1.0*"/>
          <thead>
            <row>
              <entry>Alarm Name</entry>
              <entry>Description</entry>
              <entry>Likely Cause</entry>
              <entry>Mitigation Tasks to Perform</entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry>Process Check</entry>
              <entry>nova-api check</entry>
              <entry>Process crashed.</entry>
              <entry>Restart process on affected node. Review nova-api log files.</entry>
            </row>
            <row>
              <entry>HTTP Status</entry>
              <entry>nova-api health check</entry>
              <entry>Process crashed.</entry>
              <entry>Restart process on affected node. Review nova-api log files. Try to connect
                locally to the http port to see if the connection is accepted.</entry>
            </row>
            <row>
              <entry>Process Check</entry>
              <entry>nova-cert check</entry>
              <entry>Process crashed.</entry>
              <entry>Restart process on affected node. Review nova-cert.log files.</entry>
            </row>
            <row>
              <entry>Process Check</entry>
              <entry>nova-compute check</entry>
              <entry>Process crashed.</entry>
              <entry>Restart process on affected node. Review nova-compute.log files.</entry>
            </row>
            <row>
              <entry>Process Check</entry>
              <entry>nova-conductor check</entry>
              <entry>Process crashed.</entry>
              <entry>Restart process on affected node. Review nova-conductor.log files.</entry>
            </row>
            <row>
              <entry>Process Check</entry>
              <entry>nova-consoleauth check</entry>
              <entry>Process crashed.</entry>
              <entry>Restart process on affected node. Review nova-consoleauth log files.</entry>
            </row>
            <row>
              <entry>Process Check</entry>
              <entry>nova-novncproxy check</entry>
              <entry>Process crashed.</entry>
              <entry>Restart process on affected node. Review nova-novncproxy log files.</entry>
            </row>
            <row>
              <entry>Process Check</entry>
              <entry>nova-scheduler check</entry>
              <entry>Process crashed.</entry>
              <entry>Restart process on affected node. Review nova-scheduler.log files.</entry>
            </row>
            <row>
              <entry>Process Bound Check</entry>
              <entry>process_name=nova-api. Check that the number of processes found is in a
                predefined range</entry>
              <entry>Process crashed or too many processes running</entry>
              <entry>Stop all the processes and restart nova-api process. Review system and nova-api
                logs.</entry>
            </row>
          </tbody>
        </tgroup>
      </table>
    </section>
    <section id="opsconsole"><title>Operations Console</title>
      <p>
        <table frame="all" rowsep="1" colsep="1" id="table_fdn_hpd_3t">
          <tgroup cols="4">
            <colspec colname="c1" colnum="1" colwidth="1.0*"/>
            <colspec colname="c2" colnum="2" colwidth="1.0*"/>
            <colspec colname="c3" colnum="3" colwidth="1.0*"/>
            <colspec colname="c4" colnum="4" colwidth="1.0*"/>
            <thead>
              <row>
                <entry>Alarm Name</entry>
                <entry>Description</entry>
                <entry>Likely Cause</entry>
                <entry>Mitigation Tasks to Perform</entry>
              </row>
            </thead>
            <tbody>
              <row>
                <entry>HTTP Status</entry>
                <entry>service=ops-console</entry>
                <entry>The Operations Console is unresponsive</entry>
                <entry/>
              </row>
              <row>
                <entry>Process Check</entry>
                <entry>process_name=leia-leia_monitor</entry>
                <entry>Process crashed</entry>
                <entry/>
              </row>
            </tbody>
          </tgroup>
        </table>
      </p></section>
    <section id="rabbitmq">
      <title>RabbitMQ</title>
      <table frame="all" rowsep="1" colsep="1" id="table_rsb_mp5_zs">
        <tgroup cols="4">
          <colspec colname="c1" colnum="1" colwidth="1.0*"/>
          <colspec colname="c2" colnum="2" colwidth="1.0*"/>
          <colspec colname="c3" colnum="3" colwidth="1.0*"/>
          <colspec colname="c4" colnum="4" colwidth="1.0*"/>
          <thead>
            <row>
              <entry>Alarm Name</entry>
              <entry>Description</entry>
              <entry>Likely Cause</entry>
              <entry>Mitigation Tasks to Perform</entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry>Process Check</entry>
              <entry>rabbitmq-server process not running</entry>
              <entry>Process crashed.</entry>
              <entry>Restart process on affected node. Review logs.</entry>
            </row>
            <row>
              <entry>RabbitMQ Queue Depth</entry>
              <entry>Alarms when the depth of the message queue is high.</entry>
              <entry>System load or a slow/failed consumer.</entry>
              <entry>Look for high load in the system. This alarm can fire for multiple queues and
                multiple machines. Which ones it fired on can indicate the cause. If only a single
                queue is slowing, the cause could be that the consumer is down.</entry>
            </row>
          </tbody>
        </tgroup>
      </table>
    </section>
    <section id="swift">
      <title>Swift</title>
      <table frame="all" rowsep="1" colsep="1" id="table_zn3_2pb_bt">
        <tgroup cols="4">
          <colspec colname="c1" colnum="1" colwidth="1.0*"/>
          <colspec colname="c2" colnum="2" colwidth="1.0*"/>
          <colspec colname="c3" colnum="3" colwidth="1.0*"/>
          <colspec colname="c4" colnum="4" colwidth="1.0*"/>
          <thead>
            <row>
              <entry>Alarm Name</entry>
              <entry>Description</entry>
              <entry>Likely Cause</entry>
              <entry>Mitigation Tasks to Perform</entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry>swiftlm-scan monitor</entry>
              <entry>Alarms if swiftlm-scan cannot execute a monitoring task</entry>
              <entry>The swiftlm-scan program is used to monitor/measure a number of metrics. If it
                is unable to monitor or measure something, it raises this alarm.</entry>
              <entry>Examine the "Details" field and look for a "msg" field. The text may explain
                the error problem. To view/confirm this, you can also log into the host specified by
                the "hostname" dimension and run this command "sudo swiftlm-scan | python
                -mjson.tool". The "msg" field is contained in the "value_meta" item.</entry>
            </row>
            <row>
              <entry>Swift file ownership</entry>
              <entry>Alarms if files/directories in /srv/node/ or /etc/swift are not owned by
                Swift.</entry>
              <entry>
                <p>For files in /etc/swift, somebody has probably manually edited or created a
                  file.</p>
                <p>For directories in /srv/node/*, it may happen that root partition was reimages or
                  reinstalled and the UID assigned to the swift user changes. The directories and
                  files are then not owned by the UID assigned to the swift user.</p>
              </entry>
              <entry>
                <p>For files in /etc/swift, use this command "sudo chown swift.swift /etc/swift/,
                  /etc/swift/*"</p>
                <p>For directories and files in /srv/node/*, compare the swift UID of this system
                  and other systems and the UID of the owner of /srv/node/*. If possible, make the
                  UID of the Swift user match the directories/files. Otherwise, change the ownership
                  of all files and directories under the /srv/node path.</p>
              </entry>
            </row>
            <row>
              <entry>Drive URE errors</entry>
              <entry>Alarms if swift-drive-audit reports a unrecoverable read error on a drive used
                by Swift</entry>
              <entry>An unrecoverable read error has occurred when Swift attempted to access a
                directory.</entry>
              <entry>
                <p>The UREs reported only apply to filesystem metadata (i.e., directory structures).
                  For UREs in object files, the swift system automatically deletes the file and
                  replicates a fresh copy from one of the other replicas.</p>
                <p>UREs are a normal feature of large disk drives. It does not mean that the drive
                  has failed. However, if you get regular UREs on a specific drives, then this may
                  indicate that the drive is indeed failed and should be replaced.</p>
                <p>You can use standard XFS repair actions to correct the UREs in the
                  filesystem.</p>
                <p>If the XFS repair fails, you should wipe the GPT table as follows (where LETTER
                  is replaced by the actual drive name)</p>
                <codeblock>sudo dd if=/dev/zero of=/dev/sd&lt;LETTER&gt; bs=$((1024*1024)) count=1</codeblock>
                <p>Then, on the deployer, run the _swift-configure.yml playbook. This will reformat
                  the drive, remount it and restart Swift services.</p>
                <p>It is safe to reformat drives containing Swift data because Swift mains other
                  copies of the data (usually, Swift is configured to have three replicas of all
                  data).</p>
              </entry>
            </row>
            <row>
              <entry>Swift services</entry>
              <entry>Alarms if a Swift process is not running.</entry>
              <entry>A daemon specified by the "component" dimension on the host specfied by the
                "hostname" dimension has stopped running.</entry>
              <entry>
                <p>Examine the /var/log/swift/swift.log file for possible error messages related the
                  Swift process. The process in question is listed in the alarm dimensions in the
                  "component" dimension.</p>
                <p>Restart Swift processes by running the swift-start.yml playbook.</p>
              </entry>
            </row>
            <row>
              <entry>Swift filesystem mount status</entry>
              <entry>Alarms if a filesystem/drive used by Swift in not correctly mounted</entry>
              <entry>
                <p>The device specified by the "device" dimension is not correctly mounted at the
                  mountpoint specified by the "mount" dimension.</p>
                <p>The most probable cause is that the drive has failed or that it had a temporary
                  failure during the boot process and remained unmounted.</p>
                <p>Other possible causes are a filesystem corruption that prevents the device from
                  being mounted.</p>
              </entry>
              <entry>
                <p>Reboot the node and see if the filesystem remains unmounted.</p>
                <p>If the filesystem is corrupt, see the process used for the "Drive URE errors"
                  alarm to wipe and reformat the drive..</p>
              </entry>
            </row>
            <row>
              <entry>Swift host ping</entry>
              <entry>Alarms if the specified Swift target host cannot be pinged</entry>
              <entry>The server is down.</entry>
              <entry>
                <p>Reboot the server.</p>
                <p>If many servers have this alarm, it may indicate that the network has a
                  failure.</p>
              </entry>
            </row>
            <row>
              <entry>Swift host socket connect</entry>
              <entry>Alarms if a socket cannot be opened to the specified target Swift host and
                port</entry>
              <entry>The server may be down. The Swift process running on the server may have
                stopped. You may be able to distinguish these situations by looking to see if there
                are "Swift services" or "Swift host ping" alarms. The "msg" field may also provide
                additional information.</entry>
              <entry>
                <p>If the server is down, boot it.</p>
                <p>If Swift processes have stopped, you can restart them by using the
                  swift-start.yml playbook. if this fails, rebooting the node will restart the Swift
                  processes.</p>
              </entry>
            </row>
            <row>
              <entry>Swift memcache connect</entry>
              <entry>Alarms if a socket cannot be opened to the specified target memcached
                server</entry>
              <entry>The server may be down. The memcached deamon running the server may have
                stopped.</entry>
              <entry>
                <p>If the server is down, boot it.</p>
                <p>If memcached has stopped, you can restart them by using the memcached-start.yml
                  playbook. if this fails, rebooting the node will restart the process.</p>
                <p>If the server is running and memcached is running, there may be a network problem
                  blocking port 11211.</p>
                <p>If you see sporadic alarms on different servers, the system may be running out of
                  resources. Contact HP Support for advise.</p>
              </entry>
            </row>
            <row>
              <entry>Process Check</entry>
              <entry>Alarms when the specified process is not running</entry>
              <entry>If the "service" dimension is "object-store", see the description of the "Swift
                Services" alarm for possible causes.</entry>
              <entry>If the "service" dimension is "object-store", see the description of the "Swift
                Services" alarm for possible mitigation tasks.</entry>
            </row>
            <row>
              <entry>HTTP Status</entry>
              <entry>Alarms when the specified HTTP endpoint is down or not reachable</entry>
              <entry>If the "service" dimension is "object-store", see the description of the "Swift
                host socket connect" alarm for possible causes.</entry>
              <entry>If the "service" dimension is "object-store", see the description of the "Swift
                host socket connect" alarm for possible mitigation tasks.</entry>
            </row>
          </tbody>
        </tgroup>
      </table>
    </section>
  </body>
</topic>
