<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic
  PUBLIC "-//OASIS//DTD DITA Topic//EN" "http://docs.oasis-open.org/dita/v1.1/OS/dtd/topic.dtd" >
<topic xml:lang="en-us" id="logging">
  <title>HP Helion <tm tmtype="reg">OpenStack</tm> 2.0: Centralized Logging</title>
  <body>
    <p>A typical HP Helion OpenStack cloud consists of multiple servers which makes locating a
      specific log from a single server difficult. The HP Helion OpenStack Centralized Logging
      feature helps the administrator triage and troubleshoot the distributed cloud deployment from
      a single location.</p>
    <p>The Centralized Logging feature collects logs on a central system, rather than leaving the
      logs scattered across the network. The administrator can use a single Kibana interface to view
      log information in charts, graphs, tables, histograms, and other forms.</p>
    <p>In addition to each of the HP Helion OpenStack services, Centralized Logging also processes
      logs for the following features:</p>
    <ul>
      <li>HAProxy</li>
      <li>syslog</li>
      <li>keepalived</li>
    </ul>
    <p>This document describes the Centralized Logging feature and contains the following
      sections:</p>
    <ul>
      <li>
        <xref type="section" href="#logging/install">Installation</xref>
      </li>
      <li>
        <xref type="section" href="#logging/components">Centralized Logging Components</xref>
      </li>
      <li>
        <xref type="section" href="#logging/types">Centralized Logging Data</xref>
      </li>
      <li>
        <xref href="#logging/config">Centralized Logging Configuration</xref>
      </li>
      <li>
        <xref type="section" href="#logging/kibana">Kibana Configuration</xref>
        <ul>
          <li>
            <xref type="section" href="#logging/interface">Logging into Kibana</xref>
          </li>
        </ul>
      </li>
      <li>
        <xref type="section" href="#logging/troubleshooting">Using Centralized Logging to
          Troubleshoot Issues</xref>
      </li>
      <li>
        <xref type="section" href="#logging/monitoring">Monitoring Centralized Logging</xref>
      </li>
      <li>
        <xref type="section" href="#logging/troubleshooting_issues">Troubleshooting Centralized
          Logging Issues</xref>
      </li>
      <li>
        <xref type="section" href="#logging/info">For More Information</xref>
      </li>
    </ul>
    <section id="install">
      <title>Installation</title>
      <p>The Centralized Logging feature is automatically installed as part of the HP Helion
        OpenStack installation. The base logging levels will be tuned during installation according
        to the amount of RAM allocated to your control plane nodes to ensure optimum
        performance.</p>
      <p>No specific configuration is required to use Centralized Logging. However, you can tune or
        configure the individual components as needed for your environment as detailed in the <xref
          href="#logging/config">Logging Configuration</xref> section.</p>
    </section>
    <section id="components">
      <title>Centralized Logging Components</title>
      <p>Centralized logging consists of several components, detailed below:</p>
      <ul>
        <li>
          <p>
            <b>Beaver</b> is a python daemon that takes information in log files and sends the
            content to RabbitMQ.</p>
        </li>
        <li>
          <p>
            <b>RabbitMQ</b> is a message broker for collection of logging data across nodes.</p>
        </li>
        <li>
          <p>
            <b>logstash</b> is a log processing system for receiving, processing and outputting
            logs. logstash retrieves logs from RabbitMQ, processes and enriches the data, then
            stores the data in Elasticsearch.</p>
        </li>
        <li>
          <p>
            <b>Elasticsearch</b> is a data store offering fast indexing and querying.</p>
        </li>
        <li>
          <p>
            <b>Kibana</b> is a client-side JavaScript application to visualize the data in
            Elasticsearch through a web browser. Kibana enables you to create charts and graphs
            using the log data.</p>
        </li>
        <li><b>Curator</b> is a tool provided by Elasticsearch to manage indices.</li>
      </ul>
      <p>These components are configured to work out-of-the-box and the admin should be able to view
        log data using the default configurations.</p>
      <p>At a high level, the Helion services forward logs to Beaver. Then, Beaver forwards JSON
        messages to RabbitMQ on the controller0 (management controller) node. Logstash connects to
        RabbitMQ to read queued messages and process the messages according to the Logstash
        configuration file. Logstash then forwards the processed log files in Elasticsearch. Users
        can use the Kibana interface to view and analyze the information, as shown in the following
        figure:</p>
      <p>
        <image href="../../media/centralized_logging_diagram.png" placement="break"/>
      </p>
      <note>The arrows come <b>from</b> the active (requesting) side <b>to</b> the passive
        (listening) side. The active side is always the one providing credentials, so the arrows may
        also be seen as coming from the credential holder to the application requiring
        authentication.</note>
    </section>
    <section id="types">
      <title>Centralized Logging Data</title>
      <p>The following table lists the types of logs collected by Centralized Logging and provides
        information on how the logs are maintained.</p>
      <table>
        <tgroup cols="6">
          <colspec colname="col1"/>
          <colspec colname="col2"/>
          <colspec colname="col3"/>
          <colspec colname="col4"/>
          <colspec colname="col5"/>
          <colspec colname="col6"/>
          <thead>
            <row>
              <entry>Data name</entry>
              <entry>Confidentiality</entry>
              <entry>Integrity</entry>
              <entry>Availability</entry>
              <entry>Backup?</entry>
              <entry>Description</entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry>Log records</entry>
              <entry>Restricted</entry>
              <entry>High</entry>
              <entry>Medium</entry>
              <entry>No</entry>
              <entry>Log records have a limited life, and are not archived. The log file on the
                local filesystem provides a fallback source of logging data (up to 20GB or 45 days)
                if the logging system fails.</entry>
            </row>
            <row>
              <entry>Log metadata</entry>
              <entry>Restricted</entry>
              <entry>High</entry>
              <entry>Medium</entry>
              <entry>No</entry>
              <entry>Elasticsearch indexes logged data to allow flexible searching.</entry>
            </row>
            <row>
              <entry>Credentials</entry>
              <entry>Confidential</entry>
              <entry>High</entry>
              <entry>Medium</entry>
              <entry>No</entry>
              <entry>Credentials for access to Elasticsearch and RabbitMQ are stored in
                configuration files owned by root with mode 0600.</entry>
            </row>
            <row>
              <entry>Kibana metadata</entry>
              <entry>Confidential</entry>
              <entry>High</entry>
              <entry>High</entry>
              <entry>No</entry>
              <entry>Kibana stores its search queries, visualizations and dashboards in the
                ".kibana" index. This index will be replicated across Elasticsearch cluster nodes
                and is highly available.</entry>
            </row>
            <row>
              <entry>Monitoring metrics</entry>
              <entry>Restricted</entry>
              <entry>High</entry>
              <entry>High</entry>
              <entry>Yes</entry>
              <entry>Monasca Server stores the various logging metrics and alarm-definitions. These
                are managed by Monasca.</entry>
            </row>
            <row>
              <entry>Beaver configuration</entry>
              <entry>Restricted</entry>
              <entry>High</entry>
              <entry>High</entry>
              <entry>Yes</entry>
              <entry>These are backed up as part of deployer repo changes maintained by the
                administrator.</entry>
            </row>
            <row>
              <entry>Logrotate configuration</entry>
              <entry>Restricted</entry>
              <entry>High</entry>
              <entry>High</entry>
              <entry>Yes</entry>
              <entry>These are backed up as part of deployer repo changes maintained by the
                administrator.</entry>
            </row>
            <row>
              <entry>Curator configuration</entry>
              <entry>Restricted</entry>
              <entry>High</entry>
              <entry>Medium</entry>
              <entry>Yes</entry>
              <entry>Cron job to periodically run and keep the old Elasticsearch indices
                pruned/closed. These are backed up as part of deployer repos changes maintained by
                the administrator.</entry>
            </row>
          </tbody>
        </tgroup>
      </table>
    </section>
    <section id="config">
      <title>Centralized Logging Configuration</title>
      <!--
        <p>To configure logging, modify the specific services setting file in the
          <codeph>~/helion/hos/ansible/roles/logging-common/defaults/main.yml</codeph> directory as
        needed, then reconfigure. Here is a snippet of the <codeph>keystone.yml</codeph> file
        showing the logging settings for the Keystone service. An explanation of this file is
        below.</p>
      <codeblock>
  keystone:
    files:
    - centralized_logging:
        enabled: false
        format: json
        file:
        - /var/log/keystone/keystone.log
        options:
        - daily
        - maxsize 400M
        - rotate 45
        - compress
        - delaycompress
        - missingok
        - create 640 keystone keystone
        - copytruncate
  - centralized_logging:
        enabled: true
        format: rawjson
        file:
        - /var/log/keystone/keystone-json.log
        options:
        - daily
        - maxsize 400M
        - rotate 5
        - compress
        - delaycompress
        - missingok
        - create 640 keystone keystone
        - copytruncate
...</codeblock>
      <p>In the above example, here is a description of the options:</p>
      <ul>
        <li><b>enabled</b> indicates whether centralized logging is enabled for the file specified.
          If this value is set to <codeph>false</codeph> then this log will not be sent to
          elasticsearch.</li>
        <li><b>format</b> indicates what format the log file will be in. <codeph>rawjson</codeph> is
          often the choice for centralized logging files because they are easier to parse
          through.</li>
        <li><b>file</b> indicates the name and location of the log file.</li>
        <li><b>options</b> indicate the options selected for this log file. Examples are: <ul>
            <li><b>daily</b> indicates a 24-hour rotation selection</li>
            <li><b>maxsize</b> indicates the maximum size for the file before rotation occurs</li>
            <li><b>rotate</b> indicates the number of files to rotate through</li>
            <li><b>compress</b> indicates that the file will be compressed</li>
          </ul></li>
      </ul>
      <p><b>How to Configure the Log Levels for Specific Services</b></p> -->
      <p>To configure the logging level for a specific service follow these steps:</p>
      <p>
        <table frame="all" rowsep="1" colsep="1" id="loglevel_config">
          <tgroup cols="4">
            <colspec colname="c1" colnum="1"/>
            <colspec colname="c2" colnum="2"/>
            <colspec colname="c3" colnum="3"/>
            <colspec colname="c4" colnum="4"/>
            <thead>
              <row>
                <entry>Service</entry>
                <entry>Sub-component</entry>
                <entry>Expected Default Logging Level</entry>
                <entry>Steps to Change Log Level</entry>
              </row>
            </thead>
            <tbody>
              <row>
                <entry morerows="5">Ceilometer</entry>
                <entry>ceilometer-api</entry>
                <entry>INFO</entry>
                <entry>Ceilometer log level is configurable in
                    <b>roles/_CEI-CMN/defaults/main.yml:ceilometer_loglevel: INFO</b>. You can
                  change the ceilometer_loglevel in <b>roles/_CEI-CMN/defaults/main.yml</b> for all
                  components during deployment or after deployment. You can also change log level
                  for <b>ceilometer-api</b> only by changing
                    <b>roles/CEI-API/templates/api-logging.conf.j2:level: DEBUG</b>
                  <p>For post deployment log level changes, please re-run the following from
                    deployer node:
                    <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts site.yml ceilometer-reconfigure.yml</codeblock></p></entry>
              </row>
              <row>
                <entry>ceilometer-collector</entry>
                <entry>INFO</entry>
                <entry>Ceilometer log level is configurable in
                    <b>roles/_CEI-CMN/defaults/main.yml:ceilometer_loglevel: INFO</b>. You can
                  change the ceilometer_loglevel in <b>roles/_CEI-CMN/defaults/main.yml</b> for all
                  components during deployment or after deployment. You can also change log level
                  for <b>ceilometer-collector</b> only by changing
                    <b>roles/CEI-COL/templates/collector-logging.conf.j2:level: DEBUG</b>
                  <p>For post deployment log level changes, please re-run the following from
                    deployer node:
                    <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts site.yml ceilometer-reconfigure.yml</codeblock></p></entry>
              </row>
              <row>
                <entry>ceilometer-alarm-evaluator-json</entry>
                <entry>INFO</entry>
                <entry/>
              </row>
              <row>
                <entry>ceilometer-agent-notification</entry>
                <entry>INFO</entry>
                <entry>Ceilometer log level is configurable in
                    <b>roles/_CEI-CMN/defaults/main.yml:ceilometer_loglevel: INFO</b>. You can
                  change the ceilometer_loglevel in <b>roles/_CEI-CMN/defaults/main.yml</b> for all
                  components during deployment or after deployment. You can also change log level
                  for <b>ceilometer-agent-notification</b> only by changing
                    <b>roles/CEI-NAG/templates/agent-notification-logging.conf.j2:level: DEBUG</b>
                  <p>For post deployment log level changes, please re-run the following from
                    deployer node:
                    <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts site.yml ceilometer-reconfigure.yml</codeblock></p></entry>
              </row>
              <row>
                <entry>ceilometer-agent-central</entry>
                <entry>INFO</entry>
                <entry>Ceilometer log level is configurable in
                    <b>roles/_CEI-CMN/defaults/main.yml:ceilometer_loglevel: INFO</b>. You can
                  change the ceilometer_loglevel in <b>roles/_CEI-CMN/defaults/main.yml</b> for all
                  components during deployment or after deployment. You can also change log level
                  for <b>ceilometer-agent-central</b> only by changing
                    <b>roles/CEI-CAG/templates/agent-central-logging.conf.j2:level: DEBUG</b>
                  <p>For post deployment log level changes, please re-run the following from
                    deployer node:
                    <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts site.yml ceilometer-reconfigure.yml</codeblock></p></entry>
              </row>
              <row>
                <entry>ceilometer-expirer</entry>
                <entry>INFO</entry>
                <entry>Ceilometer log level is configurable in
                    <b>roles/_CEI-CMN/defaults/main.yml:ceilometer_loglevel: INFO</b>. You can
                  change the ceilometer_loglevel in <b>roles/_CEI-CMN/defaults/main.yml</b> for all
                  components during deployment or after deployment. You can also change log level
                  for <b>ceilometer-expirer</b> only by changing
                    <b>roles/CEI-EXP/templates/expirer-logging.conf.j2:level: DEBUG</b>
                  <p>For post deployment log level changes, please re-run the following from
                    deployer node:
                    <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts site.yml ceilometer-reconfigure.yml</codeblock></p></entry>
              </row>
              <row>
                <entry>Ceph</entry>
                <entry>ceph-mon <p>ceph-osd</p></entry>
                <entry>INFO</entry>
                <entry>Ceph log levels are configurable both during initial deployment and after the
                  deployment. Use the following variables (default is <b>INFO</b>, case-sensitive): <codeblock>ansible-playbook -i hosts/verb_hosts site.yml -e "ceph_mon_log_level=DEBUG, ceph_osd_log_level=DEBUG"</codeblock>
                  <!--<p>You can also use <b>-e "helion_loglevel=DEBUG"</b> to specify the log level,
                    which sets the log level across multiple HLM services (this includes both OSD
                    and MON). However the former two take higher precedence than
                      <b>helion_loglevel</b> variable. The same variables can be used while running
                      <b>ceph-deploy.yml</b> play also.</p>-->
                </entry>
              </row>
              <row>
                <entry morerows="2">Freezer</entry>
                <entry>freezer-agent</entry>
                <entry>INFO</entry>
                <entry>Currently the freezer-agent does not support debug logs. INFO level is the
                  default one used for this component.</entry>
              </row>
              <row>
                <entry>freezer-api</entry>
                <entry>INFO</entry>
                <entry>Currently the freezer-api does not support debug logs. INFO level is the
                  default one used for this component.</entry>
              </row>
              <row>
                <entry>freezer-scheduler</entry>
                <entry>INFO</entry>
                <entry>Currently the freezer-scheduler does not support debug logs. INFO level is
                  the default one used for this component.</entry>
              </row>
              <row>
                <entry>Glance</entry>
                <entry/>
                <entry>INFO</entry>
                <entry>Glance exposes logging configuration templates at deployer under
                    <codeph>~/helion/my_cloud/config/glance/
                    glance-[api,registry]-logging.conf</codeph>. After the needed changes are done
                  (for example logging level changes to local or centralized logging)
                  glance-reconfigure playbook must be ran targeting relevant nodes to deploy the
                  configuration change.
                  <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts glance-reconfigure.yml</codeblock></entry>
              </row>
              <row>
                <entry>Heat</entry>
                <entry/>
                <entry>INFO</entry>
                <entry>There are logging config files in my_cloud/config/heat directory[
                  api-cfn-logging.conf.j2 api-cloudwatch-logging.conf.j2 api-logging.conf.j2
                  engine-logging.conf.j2 ] . User can edit these files and set the desired log level
                  for both local as well as centralized logging. After making the changes run
                  heat-reconfigure playbook from the deployer to apply the changes.
                  <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts heat-reconfigure.yml</codeblock></entry>
              </row>
              <row>
                <entry>Keystone</entry>
                <entry>key-api</entry>
                <entry>INFO</entry>
                <entry>To configure logging levels, see <xref
                    href="../identity/identity_reconfigure.dita">Identity Service
                  Reconfigure</xref></entry>
              </row>
              <row>
                <entry>Monasca</entry>
                <entry/>
                <entry>WARN</entry>
                <entry>
                  <p>All of the Monasca logging is defaulted to WARN. To change the level for all of
                    our services you can run:</p>
                  <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts monasca-reconfigure.yml -e "monasca_log_level=DEBUG"
ansible-playbook -i hosts/verb_hosts monasca-agent-reconfigure.yml -e "monasca_log_level=DEBUG"</codeblock>
                  <p>If you want to change per a service each of the following var files have that
                    variable that you can set to the desired level:</p>
                  <ul>
                    <li>roles/monasca-persister/defaults/main.yml</li>
                    <li>roles/zookeeper/defaults/main.yml</li>
                    <li>roles/storm/defaults/main.yml</li>
                    <li>roles/monasca-notification/defaults/main.yml</li>
                    <li>roles/monasca-api/defaults/main.yml</li>
                    <li>roles/kafka/defaults/main.yml</li>
                    <li>roles/monasca-agent/defaults/main.yml (This one you will need to add the
                      variable)</li>
                  </ul>
                  <p>You will then need to run:</p>
                  <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts monasca-reconfigure.yml -e "monasca_log_level=DEBUG"
ansible-playbook -i hosts/verb_hosts monasca-agent-reconfigure.yml -e "monasca_log_level=DEBUG"</codeblock>
                </entry>
              </row>
              <row>
                <entry morerows="1">Nova</entry>
                <entry>nova-api</entry>
                <entry>INFO</entry>
                <entry>Nova service log levels are configurable in
                    <b>~/helion/my_cloud/config/nova/*-logging.conf.j2</b>. After you edit the
                  specify the level you would like, you will need to run the following series of
                  commands to commit them to the environment:<p>
                    <codeblock>cd ~/helion/hos/ansible
git add -A
git commit -m "Change Nova logging"
cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml
ansible-playbook -i hosts/localhost ready-deployment.yml
cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts site.yml nova-reconfigure.yml</codeblock>
                  </p>
                </entry>
              </row>
              <row>
                <entry>nova-compute</entry>
                <entry>INFO</entry>
                <entry>Nova service log levels are configurable in
                    <b>~/helion/my_cloud/config/nova/*-logging.conf.j2</b>. After you edit the
                  specify the level you would like, you will need to run the following series of
                  commands to commit them to the environment:<p>
                    <codeblock>cd ~/helion/hos/ansible
git add -A
git commit -m "Change Nova logging"
cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml
ansible-playbook -i hosts/localhost ready-deployment.yml
cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts site.yml nova-reconfigure.yml</codeblock>
                  </p></entry>
              </row>
              <row>
                <entry>Operations Console</entry>
                <entry>ops-web <p>ops-mon</p></entry>
                <entry>INFO</entry>
                <entry>
                  <p>Operations Console server log levels are configurable both during initial
                    deployment and after the deployment. Use the following variable (default is
                      <b>INFO</b>, case-sensitive):</p>
                  <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts site.yml -e "ops_console_loglevel=DEBUG"</codeblock>
                  <!--<p>You can also use <b>-e "helion_loglevel=DEBUG"</b> to specify the log level,
                    which sets the log level across multiple services. However the
                      <b>ops_console_loglevel</b> takes higher precedence than the
                      <b>helion_loglevel</b> variable. The same variables can be used while running
                    the <b>ops-console-deploy.yml</b> playbook as well.</p>-->
                </entry>
              </row>
              <row>
                <entry>Sherpa</entry>
                <entry/>
                <entry>INFO</entry>
                <entry>
                  <p>Logging level is set in <codeph>/etc/sherpa/sherpa.conf</codeph>. To change the
                    logging level, just change from INFO to DEBUG in the following line in
                    sherpa.conf:</p>
                  <codeblock>'sherpa': {'level': 'INFO', 'handlers': ['file', 'json']}</codeblock>
                </entry>
              </row>
              <row>
                <entry>Swift</entry>
                <entry/>
                <entry>INFO</entry>
                <entry>Swift service log levels are configurable in
                    <b>~/stack/helion/my_cloud/config/swift/account-server.conf.j2</b>. After you
                  edit the specify the level you would like, you will need to run the following
                  series of commands to commit them to the environment:<p>
                    <codeblock>cd ~/helion/hos/ansible
git add -A
git commit -m "Change Swift logging"
cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml
ansible-playbook -i hosts/localhost ready-deployment.yml
cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts site.yml swift-reconfigure.yml</codeblock>
                  </p>
                </entry>
              </row>
              <row>
                <entry>VSA</entry>
                <entry>vsa-installer</entry>
                <entry>INFO</entry>
                <entry>
                  <p>VSA log levels are configurable for<b> vsa-installer</b> module. Use the
                    following variables (default is <b>INFO</b>, case-sensitive):</p>
                  <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts site.yml -e "vsa_installer_log_level=DEBUG"</codeblock>
                  <!--<p>You can also use <b>-e "helion_loglevel=DEBUG"</b> to specify the log level,
                    which sets the log level across multiple HLM services. However the
                      <b>vsa_installer_log_level</b> take higher precedence than
                      <b>helion_loglevel</b> variable. The same variables can be used while running
                      <b>vsa-deploy.yml</b> play also.</p>-->
                </entry>
              </row>
            </tbody>
          </tgroup>
        </table>
      </p>
    </section>
    <section id="kibana">
      <title>Kibana Configuration</title>
      <p>You can use the Kibana dashboards to view log data. Kibana is a tool developed to create
        charts, graphs, tables, and histograms based on logs send to Elasticsearch by logstash.</p>
      <p>While creating Kibana dashboards is beyond the scope of this document, it is important to
        know that you can use the default Kibana dashboards or create custom dashboards. The
        dashboards are JSON files that you can modify or create new dashboards based on existing
        dashboards.</p>
      <note>Kibana is client-side software. To operate properly, the browser must be able to access
        port 5601 on the control plane.</note>
    </section>
    <section id="interface">
      <title>Logging into Kibana</title>
      <p>There are two ways to access Kibana:</p>
      <ul>
        <li>Through the <b>Logging Dashboard</b> option in the Operations Console</li>
        <li>Direct link via port 5601 on the Horizon VIP address</li>
      </ul>
      <p>Details:</p>
      <p><b>Operations Console method</b> - Access the <xref
          href="monitoring_service.dita#monitoring/working" type="section">Operations
        Console</xref>.</p>
      <p>Navigate to the <b>Logging Dashboard</b> via the menu.</p>
      <p><b>Direct Access method</b> - From either your deployer or controller node, run the
        following command to find your Horizon VIP:</p>
      <codeblock>grep vip-HZN-WEB /etc/hosts</codeblock>
      <p>Access to Kibana will be over port 5601 of your Horizon VIP address. Example:</p>
      <codeblock>http://&lt;VIP&gt;:5601</codeblock>
      <b>Login Credentials</b>
      <p>Log in with these credentials:</p>
      <ul>
        <li>Username: kibana</li>
        <li>Password: 1f3pNxQmXXELJn/MJ4jOHixePSXlu4eZzhi1gJjr7mI=</li>
      </ul>
    </section>
    <section id="troubleshooting"><title>Using Centralized Logging to Troubleshoot Issues</title>
      <p>You can troubleshoot service-specific issues by reviewing the logs. After logging into
        Kibana, follow these steps to load the logs for viewing:</p>
      <ol>
        <li>Navigate to the <b>Settings</b> menu to configure an index pattern to search for.</li>
        <li>In the <b>Index name or pattern</b> field, you can enter <codeph>logstash-*</codeph> to
          query all elasticsearch indices.</li>
        <li>Click the green <b>Create</b> button to create and load the index.</li>
        <li>Navigate to the <b>Discover</b> menu to load the index and make it available to
          search.</li>
      </ol>
      <note>If you want to search specific elasticsearch indices, you can run <codeph>curl
          localhost:9200/_cat/indices?v</codeph> from the control plane to get a full list of
        available indices.</note>
      <p>Once the logs load you can change the timeframe from the dropdown in the upper-righthand
        corner of the Kibana window. You have the following options to choose from:</p>
      <ul>
        <li>Quick - a variety of timeframe choices will be available here</li>
        <li>Relative - allows you to select a start time relative to the current time to show this
          range</li>
        <li>Absolute - allows you to select a date range to query</li>
      </ul>
      <p>When searching there are common fields you will want to use, such as:</p>
      <ul>
        <li>type - this will include the service name, such as <codeph>keystone</codeph> or
            <codeph>ceilometer</codeph></li>
        <li>host - you can specify a specific host to search for in the logs</li>
        <li>file - you can specify a specific log file to search</li>
      </ul>
      <p>For more details on using Kibana and Elasticsearch to query logs, see <xref
          href="https://www.elastic.co/guide/en/kibana/3.0/working-with-queries-and-filters.html"
          scope="external" format="html"
          >https://www.elastic.co/guide/en/kibana/3.0/working-with-queries-and-filters.html</xref></p>
    </section>
    <section id="monitoring"><title>Monitoring Centralized Logging</title>
      <p>To help keep abreast of potential logging issues and resolve issues before they affect
        logging, you may wish to monitor the Centralized Logging Alarms. To do so:</p>
      <ol>
        <li>Log in to the Operations Console GUI</li>
        <li>Navigate to the Monitoring | Alarm Definitions page</li>
        <li>Find the alarm definitions that are applied to the various hosts. See the <xref
            href="alarms.dita#alarmdefinitions/logging">Alarm Definitions List</xref> for the
          Centralized Logging Alarm Definitions.</li>
        <li>Navigate to the Monitoring | Alarm page</li>
        <li>Find the alarm definitions applied to the various hosts. These should match the alarm
          definitions in the <xref href="alarms.dita#alarmdefinitions/logging">Logging Alarm
            Definitions list</xref> .</li>
        <li>See if the alarm is green (good) or is in a bad state. If any are in a bad state, see
          the possible actions to perform in the <xref href="alarms.dita#alarmdefinitions/logging"
            >Alarms Definitions list</xref>.</li>
      </ol>
      <p>You can use this filtering technique in the "Alarms" page to look for the following:</p>
      <ol>
        <li>To look for Processes that may be down, filter for "Process" then make sure the process
          are up: <ol>
            <li>Elasticsearch</li>
            <li>Logstash</li>
            <li>RabbitMQ</li>
            <li>Beaver</li>
            <li>Apache</li>
          </ol></li>
      </ol>
      <p>To look for sufficient Disk space, filter for "Disk"</p>
      <p>To look for sufficient RAM Memory, filter for "Memory"</p>
    </section>
    <section id="troubleshooting_issues"><title>Troubleshooting Centralized Logging Issues</title>
      <p>If centralized logging is not working, go through the following steps to
        troubleshooting:</p>
      <ul>
        <li><b>Processes</b> - Ensure that all the centralized logging processes are up.<ul>
            <li>Elasticsearch</li>
            <li>Logstash</li>
            <li>RabbitMQ</li>
            <li>Beaver</li>
            <li>Apache</li>
          </ul></li>
        <li><b>Check Disk Space</b> - Ensure that you have not run out of disk space.</li>
        <li><b>RAM</b> - Ensure that you have sufficient RAM and that it is allocated properly.</li>
      </ul>
      <b>Logging Best Practices</b>
      <p>By default a full backup includes all logs. Logs can be large, which can slow down the
        backup process. During this time, the control plane is offline.</p>
      <p>To balance having useful logs with optimizing system performance, you should routinely
        clean up logs from <codeph>logstash</codeph> to only keep logs of interest. You should only
        do a full backup if you need to preserve logs. In most cases, the exclude option is
        appropriate.</p>
      <b>Logs can fill the control plane node</b>
      <p>Logs are stored in one index per day in the Elasticsearch database but are not rotated.</p>
      <p>Because logs are not rotated, the logs can accumulate and consume significant space on the
        control plane.</p>
      <p>Use the following command to view information on log storage:</p>
      <codeblock>curl http://localhost:9200/_cat/indices?v</codeblock>
      <p>The following is an example of output from this command:</p>
      <codeblock>
health index pri rep docs.count docs.deleted store.size pri.store.size
yellow logstash-2015.02.10 5 1 33758082 0 16.5gb 16.5gb
yellow logstash-2015.02.11 5 1 27471823 0 13.6gb 13.6gb</codeblock>
      <p>You can delete older data using following command:</p>
      <codeblock>curl -X DELETE http://localhost:9200/[index name]</codeblock>
      <b>The number of logstash processes might not be sufficient</b>
      <p>On larger installations, logstash may not be able to process logs as they are produced. In
        this case unprocessed logs are stored in RabbitMQ queue and this queue is increasing.</p>
      <p>To check size of queue, use the following command:</p>
      <codeblock>rabbitmqctl list_queues | grep logs</codeblock>
      <p>The output of the command is similar to the following:</p>
      <codeblock>logstash-queue 0</codeblock>
      <p>If the queue is more than zero, wait a few minutes and run command again. If the size has
        increased, logs are not being processed quickly enough and the number of logstash processes
        should be increased.</p>
      <p>To increase logstash processes:</p>
      <ol>
        <li>Stop logstash using command <codeph>service logstash stop</codeph></li>
        <li>In the <codeph>/etc/default/logstash</codeph> file, find the
            <codeph>LS_NUM_INSTANCES</codeph> line and change the value to a larger number.</li>
        <li>Restart logstash using the command <codeph>service logstash start</codeph></li>
      </ol>
      <p>In approximately 15 minutes logstash starts processing. Re-check the queue size to make
        sure the queue size is reducing.</p>
    </section>
    <section id="info">
      <title>For More Information</title>
      <p>For information the centralized logging components, use the following links:</p>
      <ul>
        <li><xref href="http://logstash.net/docs" scope="external" format="html"
          >Logstash</xref></li>
        <li><xref href="http://www.elasticsearch.org/guide" scope="external" format="html"
            >Elasticsearch</xref></li>
        <li><xref href="http://www.elasticsearch.org/blog/scripting-security" scope="external"
            format="html">Elasticsearch Scripting and Security</xref></li>
        <li><xref href="http://beaver.readthedocs.org" scope="external" format="html"
          >Beaver</xref></li>
        <li><xref href="http://www.rabbitmq.com/" scope="external" format="html"
          >RabbitMQ</xref></li>
        <li><xref href="http://www.elasticsearch.org/guide/en/kibana/current/index.html"
            scope="external" format="html">Kibana Dashboard</xref></li>
      </ul>
    </section>
  </body>
</topic>
