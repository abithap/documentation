<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="topic_flx_j2b_ws">
  <title>Beta 1: Installing HP Helion <tm tmtype="reg">OpenStack</tm> on Baremetal</title>
  <body>
    <section><title>Known Restrictions</title>
      <ul>
        <li>Ensure the <xref href="hardware.dita">minimum hardware requirements</xref> are met.</li>
        <li>All disks on the system(s) will be wiped: /dev/sda will be used for the OS but all other
          disks will be wiped.</li>
        <li>The deployer node must run on HP Linux for HP Helion OpenStack. It is part of the ISO
          you will download from the <xref
            href="https://helion.hpwsportal.com/catalog.html#/Category/%7B%22categoryId%22%3A10311%7D/Show"
            format="html" scope="external">Helion Downloads</xref> page. There is only one download;
          it includes both the OS and the Beta0 installer in one ISO.</li>
        <li>Three NIC are tested, one 1G used for PXE and two bonded 10G for everything else. All
          machines can net boot from PXE and use the deployer as a DHCP server.</li>
        <li>All machines of a single type should be the same, that is, all computes, all VSAs, and
          so on.</li>
        <li>The deployer node must be a dedicated node in Beta0.</li>
        <li>The machine hosting the deployer and all baremetal systems must be connected to a
          management network. Nodes on this management network must be able to reach the iLO
          subsystem of each baremetal system to enable host reboots as part of the install process.
          The HP Helion OpenStack architecture requires that the IPMI network is a separate network
          and that a route exists from the management network to the IPMI network for iLO
          access.</li>
      </ul>
    </section>
    <section id="Prereqs">
      <title>Before you Start</title>
      <p>Prepare your baremetal hardware, as follows, on all nodes:</p>
      <ul>
        <li>Setup the iLO Advanced license in the iLO configuration.</li>
        <li>Switch from UEFI to Legacy BIOS.</li>
        <li>Ensure that the network to be used for PXE installation has PXE enabled.</li>
        <li>Ensure that the other networks have PXE disabled.</li>
        <li>Insert the CD ROM in the Virtual Media drive on the iLO.</li>
      </ul>
    </section>
    <section id="DeployerInstall">
      <title>Set up the Deployer</title>
      <ol>
        <li>Create LUN(s), if required.</li>
        <li>Download the HP Helion OpenStack Deployer ISO from the <xref
            href="https://helion.hpwsportal.com/catalog.html#/Category/%7B%22categoryId%22%3A10311%7D/Show"
            format="html" scope="external">Helion Downloads</xref> page after signing up and being
          approved for the program.</li>
        <li>Boot your deployer from the ISO.</li>
        <li>Enter "install" to start installation.</li>
        <li>Select the language.</li>
        <li>Select the location.</li>
        <li>Select the keyboard layout.</li>
        <li>Select the primary network interface, if prompted:<ul>
            <li>Assign IP address, netmask</li>
          </ul></li>
        <li>Create new account:<ul>
            <li>Enter a username.</li>
            <li>Enter a password.</li>
            <li>Enter time zone if prompted to do so.</li>
            <li>Synchronize the time on all nodes manually. NTP will be installed later.</li>
          </ul></li>
      </ol>
      <p>At the end of this section you should have a deployer node set up with hLinux on it.</p>
    </section>
    <section id="HLM_Node_Personalization">
      <title>Configure and Run the Deployer</title>
      <ol>
        <li>On the deployer node, enter the following command to create the SSH keypair if one is
          not already present:<codeblock>ssh-keygen -t rsa</codeblock>
        </li>
        <li>Add ~/.ssh/id_rsa.pub to ~/.ssh/authorized_keys file:
          <codeblock>cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys</codeblock></li>
        <li>Confirm that ssh localhost works without a password and that you can get from external
          sources, both with and without sudo.</li>
        <li>Log in to the deployer node as the user you created during the setup phase, and mount
          the install media at /media/cdrom, for example,
          <codeblock>sudo mount /media/cdrom</codeblock></li>
        <li> Unpack the following tarball:
          <codeblock>tar zxvf /media/cdrom/hos-2.0.0/hlm-deployer-2.0.0-20150805T115313Z.tgz</codeblock>
        </li>
        <li>Run the following included script:
          <codeblock>~/hlm-deployer/hlm-init-2.0.0-1.bash</codeblock></li>
      </ol>
      <p>At the end of this section you should have a local directory structure, as described
        below:</p>
      <codeblock>
helion/                        Top level directory
helion/examples/               Directory contains the config input files of the example clouds
helion/my_cloud/definition/    Directory contains the config input files
helion/my_cloud/config/        Directory contains .j2 files which are symlinks to the /hlm/ansible directory
helion/hlm/                    Directory contains files used by the installer
</codeblock>
    </section>
    <section id="Configuration">
      <title>Configure your Environment</title>
      <ol>
        <li>Setup your configuration files, as follows: <ol>
            <li>See the sample set of configuration files in the
              ~/helion/examples/one-region-poc-with-vsa directory. The accompanying README.md file
              explains the contents of each of the configuration files.</li>
            <li>Copy the example configuration files into the required setup directory and edit them
              as required: <codeblock>cp -r ~/helion/examples/one-region-poc-with-vsa/* </codeblock>
              <p>Note: There have been changes to some of the files since Beta 0.</p> The
              configuration files for editing can be found at the following location:
                <codeblock>~/helion/my_cloud/definition/data</codeblock><ul id="ul_wwn_rtx_bt">
                <li>The baremetalConfig.yml file should specify the server information for your
                  environment.</li>
                <li>The servers.yml file contains the IP address information for the hardware.</li>
                <li>The networks.yml file contains networking information.</li>
                <li>The control_plane.yml file contains information about the services that will be
                  installed.</li>
                <li>The <codeph>/data/disks_controller.yml</codeph> contains the Swift information
                  (swiftobj disk-group). This describes what disks on each of the servers are
                  assigned to Swift and the rings that uses those drives. For more information,
                  refer to <xref href="swift/swift_beta1.xml#topic_qkg_l4v_bt">cloud
                  model</xref>.</li>
                <li>The <codeph>config/swift/rings.yml</codeph> specifies the ring which maps the
                  logical names of data to locations on a particular disks. For more information,
                  refer to <xref href="swift/swift_beta1.xml#topic_qkg_l4v_bt">cloud
                  model</xref>.</li>
                <li>In the <codeph>net_interfaces.yml</codeph> file, replace all instances of
                  “bond-mode: 1” to “bond-mode: active-backup”.</li>
              </ul></li>
            <li>Commit your configuration into git, as follows:
              <codeblock>git add –A
git commit –m “My config”
</codeblock></li>
          </ol></li>
        <li>Run the configuration processor, as follows: <codeblock>cd ~/helion/hlm/ansible</codeblock>
          <codeblock>ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock>The CP
          output is placed on to the following two private branches of the git repo: <ul>
            <li>staging-ansible</li>
            <li>staging-cp-persistent</li>
          </ul>
        </li>
        <li><ol>
            <li>Check the generated host files in
                <codeph>~/helion/my_cloud/stage/ansible/host_vars/</codeph> to ensure the correct
              IPs are included. You can review the ansible change by running the following command:
              <codeblock>git show staging-ansible</codeblock></li>
            <li>Use the 
              <codeblock>ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock> file to create a
              deployment directory. Run your "verb_host" commands from the following directory:
              <codeblock> ~/scratch/ansible/next/hos/ansible</codeblock></li>
          </ol></li>
      </ol>
    </section>
    <section id="CobblerDeploy">
      <title>Deploy Cobbler </title>
      <ol>
        <li>Run the following command:
          <codeblock>export ANSIBLE_HOST_KEY_CHECKING=False</codeblock></li>
        <li>Run the following playbook:
          <codeblock>ansible-playbook -i hosts/localhost cobbler-deploy.yml</codeblock></li>
      </ol>
    </section>
    <section id="NodeProvision"><title>Provision the Nodes</title>
      <ol>
        <li>Run the following command, which will reimage all the nodes using PXE:
          <codeblock>ansible-playbook -i hosts/localhost bm-reimage.yml</codeblock></li>
        <li>Wait for the nodes to install and come back up.</li>
      </ol>
    </section>
    <section id="CloudDeploy"><title>Deploy the Cloud</title>
      <ol>
        <li>Run the following command:
            <codeblock>ansible-playbook -i hosts/verb_hosts osconfig-run.yml</codeblock><ol
            id="ol_rqv_xmm_dt">
            <li>Verify the network is working correctly. Ping each IP (excluding VSA-BLK and VIPs)
              from the /etc/hosts file from one of the controller nodes.</li>
          </ol>
        </li>
        <li> (<b>Optional</b>) To validate object storage cloud model, execute the following command
          on the deployer
            :<codeblock>ansible-playbook -i hosts/verb_hosts _swift-validate-input-model.yml</codeblock><p>If
            any error is triggered when you validate the object storage, refer to <xref
              href="swift/troubleshooting_beta1.dita#topic_sl2_nht_dt"
            >Swift-Troubleshooting</xref></p></li>
        <li>Modify the ./helion/hlm/ansible/hlm-deploy.yml to comment out the line containing
          horizon.</li>
        <li>Run the following command:
          <codeblock>ansible-playbook -i hosts/verb_hosts hlm-deploy.yml -e tuning_selector=medium</codeblock>
        </li>
      </ol>
    </section>
    <section id="VSA">
      <title>Install and Configure VSA (optional).</title>
      <p>You can set up Virtual Storage Appliance (VSA) using the instructions on the following
        page: <xref href="vsa.dita#topic_hjr_4x1_dt">Install and Configure VSA
        (optional)</xref>.</p>
    </section>
    <section id="Post-Installation"><title>Verification Steps</title>
      <ol>
        <li>This step may be used to verify your cloud installation. Note that running the command
          below will download a cirros image from the internet, upload it to Glance, and create a
          Neutron external
            network.<codeblock>ansible-playbook -i hosts/verb_hosts hlm-cloud-configure.yml</codeblock><note>You
            can optionally specify the external network CIDR here too. If you choose not to
            excercise this option or use a wrong value, the VMs will not be accessible over the
            network.</note><codeblock>ansible-playbook -i hosts/verb_hosts hlm-cloud-configure.yml -e EXT_NET_CIDR=10.240.96.0/20</codeblock></li>
        <li>Run the following command, which will replace <codeph>/etc/hosts</codeph> on the
          deployer:
          <codeblock>ansible-playbook -i hosts/localhost cloud-client-setup.yml</codeblock> As the
          /etc/hosts no longer have entries for HLM, sudo commands may become a bit slower. To fix
          this issue, once this step is complete, add "hlm" after "127.0.0.1 localhost". The result
          will look like
          this:<codeblock>...
# Localhost Information
127.0.0.1 localhost hlm
</codeblock></li>
      </ol>
    </section>
  </body>
</topic>
