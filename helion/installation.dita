<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="topic_flx_j2b_ws">
  <title>Beta 0: Installing HP Helion OpenStack on Baremetal</title>
  <prolog>
    <metadata>
      <othermeta name="layout" content="default"/>
      <othermeta name="product-version" content="HP Helion Openstack"/>
      <othermeta name="product-version" content="HP Helion Openstack 2.0"/>
      <othermeta name="role" content="Systems Administrator"/>
      <othermeta name="role" content="Cloud Architect"/>
      <othermeta name="role" content="Storage Administrator"/>
      <othermeta name="role" content="Network Administrator"/>
      <othermeta name="role" content="Service Developer"/>
      <othermeta name="role" content="Cloud Administrator"/>
      <othermeta name="role" content="Application Developer"/>
      <othermeta name="role" content="Network Engineer"/>
      <othermeta name="role" content="Geraldine K."/>
      <othermeta name="product-version1" content="HP Helion Openstack"/>
      <othermeta name="product-version2" content="HP Helion Openstack 2.0"/>
    </metadata>
  </prolog>
  <body>
<note>This assumes a BM deployer.</note>
<note>This assumes hlinux:
      http://tarballs.gozer.ftclab.gozer.hpcloud.net/hos/hos-2.0.0/canary_build/01-18/hLinux-cattleprod-amd64-blaster-netinst-20150729-hlm.2015-08-05T13:03:59_731caae.iso</note>
<section><title>Known Restrictions</title>
    <ol id="ol_kr3_2fb_ws">
      <li>All disks on the system(s) will be wiped /dev/sda will be used for the OS but all other disks will also be wiped.</li>
      <li>Install on nodes via legacy bios setting (No UEFI)</li>
      <li>Three NIC are tested, one 1G used for PXE and two bonded 10G for everything else. Important factor is that all machines can net boot from PXE and use deployer as DHCP server.</li>
      <li>The network will need to be set up with various number of VLANS depending on needs. Just make sure model reflects that.</li>
      <li>Number of disks must be reflected in model as well. We used machines that have 5, but not always used all disks on every node</li>
      <li>All machines used were same, but with right model there is no such restriction - although all machines of single type should be same, like all computes, all VSAs etc</li>
      <li>This will only work anywhere, as no external network is used. However due to bugs we had to download some components from HP sources.</li>
      <li>The deployer node needs to be a dedicated node in Beta0.</li>
      <li>The machine hosting the deployer, and all baremetal systems have to be connected to a
          management network. Nodes on this management network must be able to reach the iLO
          subsystem of each baremetal system to enable host reboots as part of the install process.
          The HP Helion OpenStack architecture requires that the IPMI network is a separate network
          and that a route exists from management network to the IPMI network for iLO access.</li>
      <li>Need 4 local disks on nodes in the control plane (for root os and swift).</li>
      <li>Machines will need to be fully reinstalled on next drop â€“ upgrade from beta0 to beta1 is
        not supported.</li>
      <li>The machine types should be homogeneous.</li>
        <li>Is possible to add additional VSA and compute nodes. The default is 1 of each.</li>
        <li>You may assign server roles to each or your nodes manually, or allow the installer to
          assign them.</li>
    </ol>
</section>
    <section><title>Installing hLinux Deployer from ISO</title>
   <ol><li>Create LUN(s), if required.</li>
     <li>Boot from ISO</li>
     <li>Enter 'install' to start installation</li>
     <li>Select Language</li>
     <li>Select Location</li>
     <li>Select Keyboard layout</li>
     <li>Select appropriate network interface <ul>
            <li>Assign IP address, netmask</li>
          </ul></li>
       <li>Create new account
         <ul>
           <li>Enter a username.</li>
           <li>Enter a password.</li>
           <li>Enter timezone if prompted to do so.</li>
                <li>Synchronize the time on all nodes manually. NTP will be installed later.</li>
         </ul></li>
</ol></section>
    <section><title>Installing HP Helion OpenStack</title>
      <ol>
        <li>Check that ssh localhost works without a password and that you can get from external sources, both with and without sudo.</li>
        <li>Use virtual media or download the ISO on to the system at /media/cdrom</li>
        <li>Untar the hlm-deployer.tgz file in ~stack</li>
        <li>Run included script, e.g. ~/hlm-deployer/hlm-init-2.0.0.bash </li>
        <li>Observe output for: 
          <ul>
            <li>TASK: [deployer-setup | init-deployer-apt-repo | Find cmc32 apt tarball] ****** </li>
            <li>ok: [localhost] => (item=/media/cdrom/extras/hLinux-cmc-i386.tar.gz)</li>
            <li>If this fails to appear then you need to reinit deployer and point to where this files is located. Note that even when it fails it still is green and says ok! To fix run command: ansible-playbook -i hosts/localhost -e cmc32_apt_tarball_glob="/media/cdrom/extras/hLinux-cmc-i386.tar.gz" deployer-init.yml</li>
            <li>Modify location as required.</li>
          </ul></li>
        <li>Create the model. For now it can be based on Padawan. cd ~/hlm-input-model/2.0/hp-ci ; cp -r padwan new_model_name ; cd new_model_name</li>
        <li>Edit the data/baremetalConfig.yml file to contain right info for hardware.  In my case I used yml file from last installtion on this rack.</li>
        <li>Edit data/servers.yml to contain ips from baremetalConfig.yml</li>
        <li>Edit data/net_global.yml to have right network data</li>
        <li>Modify the data/disks_controller.yml file specific to Swift harware requirements. </li>
        <li>If you are not configuring with Swift for whatever reason, including lack of hardware,
          comment out the Swift config entries in ccp.yml</li>
        <li>Run the configuration processor and check that your IP addresses are correct in the following:
          <ol>
            <li>cd ~/helion/hlm/ansible</li>
            <li>ansible- ansible-playbook -i hosts/localhost config-processor-run_v2.yml</li>
            <li>Confirm that the values used by the configuration processor are the values you want.
              Edit generated_files/etc/hosts and confirm that the IPs there are correct for your
              system. Do not proceed until they are correct. If you need to re-run the configuration
              processor, you must delete hlm-configuration-processor/output first. Otherwise, the
              existing values will be re-used even if you changed the input. Any inconsistent or
              unworkable configuration choices will be flagged by the installer, which will provide
              feedback explaining the issues. Changes can then be made and the config processor can
              be rerun with the new values. Note, the config processor must be rerun after making
              config changes. </li>
          </ol> 
        </li>
        <li>Prepare bare metal hardware (this can be done in parallel with the above steps, or in advance)
          <ol>
            <li>iLO Advanced license</li>
            <li>Ensure every system has 5 enabled disk devices</li>
            <li>Switch from UEFI to Legacy BIOS</li>
          </ol>
        </li> 
        <li>Export ANSIBLE_HOST_KEY_CHECKING=False</li>
        <li>time ansible-playbook -i hosts/localhost cobbler-deploy.yml</li>
        <li>Run the following command: <codeph>ansible-playbook -i hosts/localhost
            cobbler-provision.yml</codeph>
          <p/>This will power down all nodes, set them to pxe boot and power them up again. </li>
        <li>Wait for the nodes to install: they will power down at the end.</li>
        <li>Power up recently-installed systems, e.g. ansible-playbook -i hosts/localhost cobbler-power-up.yml</li>
        <li>ansible-playbook -i hosts/verb_hosts osconfig-runV2.yml
          <ul>
            <li>1.	verify the network is working correctly. Ping each IP (excluding VSA-BLK and VIPs) from the /etc/hosts file from one of the controller nodes.</li>
          </ul>
        </li>
        <li>Set the following environment variables:
          <ol>
            <li>export http_proxy=http://web-proxy.rose.hp.com:8080/</li>   
            <li>export https_proxy=http://web-proxy.rose.hp.com:8080/</li>
            <li>export no_proxy=localhost,127.0.0.1,&lt;deployer_IP&gt;</li>
          </ol>
        </li>
        <li>Install the ethtool package on all servers</li>
        <li>Modify the ./helion/hlm/ansible/hlm-deploy.yml:
       <ol>
         <li>Comment out the line containing ironic-deploy</li>
         <li>Comment out the line containing logging-deploy</li>
       </ol>
</li>
        <li>ansible-playbook -i hosts/verb_hosts hlm-deploy.yml -e tuning_selector=medium
          (unverified) </li>
        <li>ansible-playbook -i hosts/verb_hosts hlm-cloud-configure.yml
          <ul><li>Note: You can optionally specify the external network CIDR here too: e.g. ansible-playbook -i hosts/verb_hosts hlm-cloud-configure.yml -e EXT_NET_CIDR=10.240.96.0/20</li></ul></li>
        <li>ansible-playbook -i hosts/localhost cloud-client-setup.yml</li>
        <li>Install and configure VSA </li>
        <li>As per HLM-533/HLM-973, the default root partition layout is too small and a consequence
          is that logfiles will quickly fill up the / filesystem. As root, apply the following
          workaround to all systems: 
          <ol>
            <li>lvextend -l +95%FREE /dev/hlm-vg/root</li>
            <li>resize2fs /dev/hlm-vg/root</li>
          </ol></li>
        <li> Note: Optional step to configure DNS resolution on deployer in FtC CDL labs. This is necessary in order to get 'pip install' type commands to work.
        <ul>
          <li>Replace /etc/resolv.conf on the deployer node with these lines:
<codeblock>
search ftc.usa.hp.com
nameserver 10.1.64.20
nameserver 10.1.65.20
nameserver 10.240.20.1
</codeblock>
</li>
</ul>
</li>
</ol> 
</section>
<section>
   <title>Installing and configuring VSA (optional)</title>
  <ol>
    <li>Deploy VSA by ansible -i hosts/verb_hosts vsa-deploy.yml </li>
    <li>If above has error stderr: tar: /opt/stack/service/vsa/venv/lib/helion-vsa-11.5.01.0079.tgz: Cannot open: No such file or directory then you need to manually upload this file into that location. File is located in http://10.1.56.95/helion-vsa-11.5.01.0079.tgz then rerun vsa-deploy.This is temporal issue and should be fixed by very next release.</li>
    <li>Note IP numbers of VSA VMs from ansible output, look like this: "NIC 1:                  Virtual Network: vsa-network, IP Address: 10.241.103.11, Subnet Mark: 255.255.255.0, Gateway: 10.241.103.1",</li>
    <li>Install Centralized Management Console on machine that can access IPs of VSA VMs linux installer is on. A copy is at http://10.1.56.95/HP_StoreVirtual_Linux_Centralized_Management_Console_for_Linux_BM480-10573.bin</li>
    <li>Manually add IPs of those VMs</li>
    <li>Choose Tasks -> Management Group -> New Management Group from menu above</li>
    <li>Enter a name for the management group: hlm003-vsa-mg</li>
    <li>Likewise a user and password: stack / stack</li>
    <li>Add NTP servers, to get a list log in into any vsa node and type ntpq -pn</li>
    <li>Skip DNS and SMTP section</li>
    <li>Add cluster name hlm003-vsa-cluster</li>
    <li>Enter VIP, you can find it on deployer in file helion/my_cloud/stage/info/net_info.yml section cluster_ip</li>
    <li>Skip volume creation</li>
    <li>Wait for completion</li>
    <li>Edit file helion/hlm/ansible/roles/_CND-CMN/templates/cinder.conf.j2 and make enabled_backends=vsa-1</li>
    <li>Change
      {% if cinder_enabled_backends == 'lvm-1' %} to
      {% if cinder_enabled_backends == 'no-backends' %}</li>
    <li>Uncomment and fill VSA section, here example:
      <ul>
        <li>[vsa-1]</li>
        <li>hplefthand_password: stack</li>
        <li>hplefthand_clustername: hlm003-vsa-cluster</li>
        <li>hplefthand_api_url: https://10.241.103.7:8081/lhos</li>
        <li>hplefthand_username: stack</li>
        <li>hplefthand_iscsi_chap_enabled: true</li>
        <li>volume_backend_name: vsa-backend-hlm003</li>
        <li>volume_driver: cinder.volume.drivers.san.hp.hp_lefthand_iscsi.HPLeftHandISCSIDriver</li>
        <li> hplefthand_debug: false</li>
      </ul>
</li>
    <li>Run ansible-playbook -i hosts/verb_hosts cinder-reconfigure.yml</li>
  </ol>
</section>
  </body>
</topic>
