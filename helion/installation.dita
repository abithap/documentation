<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="topic_flx_j2b_ws">
  <title>Beta 1: Installing HP Helion <tm tmtype="reg">OpenStack</tm> on Baremetal</title>
  <body>
    <section id="important-notes"><title>Important Notes</title>
      <ul>
        <li>Ensure the <xref href="hardware.dita">minimum hardware requirements</xref> are met.</li>
        <li>Make sure <b>all disks on the system(s) are wiped</b> before you begin the install.</li>
        <li>/dev/sda will be used for the OS.</li>
        <li>Both HP Linux for HP Helion OpenStack and HP Helion OpenStack are part of the ISO you
          will download from the <xref
            href="https://helion.hpwsportal.com/catalog.html#/Category/%7B%22categoryId%22%3A10311%7D/Show"
            format="html" scope="external">Helion Downloads</xref> page. There is only one download;
          it includes both the OS and the Beta1 installer in one ISO.</li>
        <li>Three NICs are tested, one 1Gb used for PXE and two bonded 10Gb for everything else. All
          machines can net boot from PXE and use the deployer as a DHCP server.</li>
        <li>All machines of a given role should be the same configuration.</li>
        <li>The deployer node must be a dedicated node in Beta1.</li>
        <li>The machine hosting the deployer and all baremetal systems must be connected to a
          management network. Nodes on this management network must be able to reach the iLO
          subsystem of each baremetal system to enable host reboots as part of the install process.
          The HP Helion OpenStack architecture requires that the IPMI network is a separate network
          and that a route exists from the management network to the IPMI network for iLO
          access.</li>
        <li>Note that the requirement to have a dedicated network for OS-install and system
          deployment has been removed in Beta 1. More information can be found on the<xref
            href="supportedconfigs.dita#topic_ckw_kbc_ws"> supported configuration page</xref>.</li>
      </ul>
    </section>
    <section id="Prereqs">
      <title>Before You Start</title>
      <p>Prepare your baremetal hardware, as follows, on all nodes:</p>
      <ul>
        <li>Set up the iLO Advanced license in the iLO configuration. Make sure the iLO user has
          admin privileges.</li>
        <li> HP Helion OpenStack 2.0 Beta 1 will detect and use the "BIOS" mode you have selected
          for each node; UEFI or legacy BIOS. UEFI support is new in Beta 1.</li>
        <li>Ensure that the network interface to be used for PXE installation has PXE enabled.</li>
        <li>Ensure that the other network interfaces have PXE disabled.</li>
        <li>Ensure that any logical drives (LUN) for the servers you will be using are created to
          meet the disk requirements outlined in the <xref href="hardware.dita">Minimum Hardware
            Requirements</xref>.</li>
      </ul>
    </section>
    <section id="DeployerInstall">
      <title>Set up the Deployer</title>
      <ol>
        <li>Download the HP Helion OpenStack Deployer ISO from the <xref
            href="https://helion.hpwsportal.com/catalog.html#/Category/%7B%22categoryId%22%3A10311%7D/Show"
            format="html" scope="external">Helion Downloads</xref> page after signing up and being
          approved for the program.</li>
        <li>Boot your deployer from the ISO. Insert the CD ROM in the Virtual Media drive on the
          iLO.</li>
        <li>Enter "install" to start installation. (note "install" all lower case)</li>
        <li>Select the language.</li>
        <li>Select the location.</li>
        <li>Select the keyboard layout.</li>
        <li>Select the primary network interface, if prompted:<ul>
            <li>Assign IP address, netmask</li>
          </ul></li>
        <li>Create new account:<ul>
            <li>Enter a username.</li>
            <li>Enter a password.</li>
            <li>Enter time zone if prompted to do so.</li>
            <li>Synchronize the time on all nodes manually. NTP will be installed later.</li>
          </ul></li>
      </ol>
      <p>At the end of this section you should have a deployer node set up with hLinux on it.</p>
    </section>
    <section id="HLM_Node_Personalization">
      <title>Configure and Run the Deployer</title>
      <note>It's critical that you don't run as root. Run as the user you just created (or stack if
        you left the default of "stack"), but do not run as root.</note>
      <ol>
        <li>Log in to the deployer node as the user you created during the setup phase, and mount
          the install media at /media/cdrom, for example,
          <codeblock>sudo mount hLinux-cattleprod-amd64-blaster-netinst-20150729-hlm.2015-08-31T16:48:07_340c639.iso /media/cdrom</codeblock></li>
        <li> Unpack the following tarball: Note this file name will
          change<codeblock>tar zxvf /media/cdrom/hos-2.0.0/hos-2.0.0-b.1-20150831T154315Z.tgz</codeblock></li>
        <li>Run the following included script:
          <codeblock>~/hos-2.0.0-b.1/hos-init.bash</codeblock></li>
      </ol>
      <p>At the end of this section you should have a local directory structure, as described
        below:</p>
      <codeblock>
helion/                        Top level directory
helion/examples/               Directory contains the config input files of the example clouds
helion/my_cloud/definition/    Directory contains the config input files
helion/my_cloud/config/        Directory contains .j2 files which are symlinks to the /hos/ansible directory
helion/hos/                    Directory contains files used by the installer
</codeblock>
    </section>
    <section id="Configuration">
      <title>Configure Your Environment</title>
      <ol>
        <li>Setup your configuration files, as follows: <ol>
            <li>See the sample set of configuration files in the
              ~/helion/examples/entry-scale-with-vsa directory. The accompanying README.md file
              explains the contents of each of the configuration files. </li>
            <li>Copy the example configuration files into the required setup directory and edit them
              as required: <codeblock>cp -r ~/helion/examples/entry-scale-with-vsa/* ~/helion/my_cloud/definition/</codeblock>
              <note id="note">There have been changes to some of the files since Beta 0. The
                configuration files for editing can be found at the following location:
                <codeblock>~/helion/my_cloud/definition/</codeblock></note>
              <ul>
                <li>The <codeph>cloudConfig.yml</codeph> file should contain the NTP and DNS
                  nameserver information for your environment.</li>
                <li>The <codeph>data/baremetalConfig.yml</codeph> file should specify the server
                  information for your environment.</li>
                <li>The <codeph>data/servers.yml</codeph> file contains the IP address information
                  for the hardware.</li>
                <li>The <codeph>data/networks.yml</codeph> file contains networking information. It
                  is recommended that you set your <codeph>start-address</codeph> at .10 in your
                  subnet range as to avoid conflicts with the virtual IPs being used by the rest of
                  your environment.</li>
                <li>The <codeph>data/net_interfaces.yml</codeph> file contains information related
                  to your network interfaces. If you are going to use NIC bonding this is where you
                  would do that.</li>
                <li>The <codeph>data/control_plane.yml</codeph> file contains information about the
                  services that will be installed. No edits should be needed for this file.</li>
                <li>The <codeph>/data/disks_controller.yml</codeph> contains the Swift information
                  (swiftobj disk-group). This describes what disks on each of the servers are
                  assigned to Swift and the rings that uses those drives. To know more about the
                  disk drives, refer to <xref
                    href="swift/swift_input_cloud_model.dita#modify-input-model">cloud model
                    modification</xref>.</li>
                <li>The <codeph>config/swift/rings.yml</codeph> specifies the ring which maps the
                  logical names of data to locations on a particular disks. To know more about the
                  rings, refer to <xref href="swift/ring_specifications.dita#ring-specification"
                    >ring specification</xref>.</li>
              </ul></li>
            <li>Edit the <codeph>~/helion/my_cloud/config/logging/main.yml</codeph> file and under
              the <b>Elasticsearch defaults</b> section, change the value of the string below to
              something other than <codeph>elasticsearch</codeph>:
              <codeblock>vars.elasticsearch_cluster_name</codeblock> Example:
                <codeph>helioncloud</codeph></li>
            <li>Commit your configuration to the <xref href="using_git.dita#topic_u3v_1yz_ct">local
                git repo</xref>, as follows:
              <codeblock>cd ~/helion/hos/ansible
git add -A
git commit -m "My config or other commit message"
</codeblock></li>
          </ol></li>
      </ol>
    </section>
    <section id="CobblerDeploy">
      <title>Deploy Cobbler </title>
      <ol>
        <li>Run the following command:
          <codeblock>export ANSIBLE_HOST_KEY_CHECKING=False</codeblock></li>
        <li>Run the following playbook:
          <codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost cobbler-power-status.yml
ansible-playbook -i hosts/localhost cobbler-deploy.yml</codeblock></li>
      </ol>
    </section>
    <section id="NodeProvision"><title>Provision the Nodes</title>
      <ol>
        <li>Run the following command, which will reimage all the nodes using PXE. Note: change
          directories to /helion/hos/ansible if not already
          there:<codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost bm-reimage.yml</codeblock></li>
        <li>The script will wait for the nodes to install and come back up.</li>
      </ol>
    </section>
    <section id="run-config-processor"><title>Run the Configuration Processor and Deploy the
        Cloud</title>
      <ol>
        <li>Run the configuration processor, as follows:
          <codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock>
          The configuration processor output is placed onto the following two private branches of
          the git repo: <ul>
            <li>staging-ansible</li>
            <li>staging-cp-persistent</li>
          </ul>
          <note>See <xref href="using_git.dita">Using Git for Configuration Management</xref> for
            more information.</note>
        </li>
        <li>Check your configuration: <ol>
            <li>You can review the ansible change by running the following command:
              <codeblock>git show staging-ansible</codeblock></li>
            <li>Use the playbook below to create a deployment directory:
              <codeblock>ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
            <li>Run your "verb_host" commands using the following steps: <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts site.yml</codeblock>
              <note>The step above runs osconfig to configure the cloud and hlm-deploy, to deploy
                the cloud. Therefore, this step may run for quite some time while all the nodes are
                configured (perhaps 45 minutes or more, depending on the number of nodes, to a
                degree).</note>
            </li>
            <li>Verify that the network is working correctly. Ping each IP (excluding VSA-BLK and
              VIPs) from the /etc/hosts file from one of the controller nodes.</li>
          </ol></li>
      </ol>
    </section>
    <section id="validate_swift"><title>Validate Object Storage (Optional)</title>
      <ol>
        <!--<li>Run the following command:
            <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts osconfig-run.yml</codeblock><ol
            id="ol_rqv_xmm_dt">
            <li>Verify the network is working correctly. Ping each IP (excluding VSA-BLK and VIPs)
              from the /etc/hosts file from one of the controller nodes.</li>
          </ol>
        </li>-->
        <li><p>To validate object storage cloud model, execute the following command on the
            deployer:</p>
          <codeblock>ansible-playbook -i hosts/verb_hosts _swift-validate-input-model.yml</codeblock>
          <p>If any error is raised when you validate the object storage, refer to <xref
              href="swift/troubleshooting.dita#topic_sl2_nht_dt">Swift-Troubleshooting</xref></p></li>
        <!-- <li>Run the following command:
          <codeblock>ansible-playbook -i hosts/verb_hosts hlm-deploy.yml -e tuning_selector=medium</codeblock>
        </li>-->
      </ol>
    </section>
    <section id="cinder"><title>Install and Configure a Block Storage Backend (Optional)</title>
      <p>There are two options for the Block Storage backend: VSA and Ceph. Without a valid backend
        your Block Storage (Cinder) implementation will not be usable. These documents will assist
        you in installing and configuring a backend solution.</p>
      <b>Ceph</b>
      <ul>
        <li><xref href="cinder/ceph/ceph_overview.dita">Ceph Overview</xref></li>
        <li><xref href="cinder/ceph/ceph_deploy.dita#topic_jrc_5lz_ft">Ceph Installation and
            Configuration</xref></li>
      </ul>
      <b>VSA</b>
      <ul>
        <li><xref href="cinder/vsa/vsa.dita">VSA Installation and Configuration</xref></li>
      </ul>
    </section>
    <section id="Post-Installation"><title>Verification Steps (Optional)</title>
      <ol>
        <li>This step may be used to verify that your cloud is up and running. However, note that
          running the command below will download a cirros image from the internet, upload it to
          Glance, and create a Neutron external
            network.<codeblock>ansible-playbook -i hosts/verb_hosts hlm-cloud-configure.yml</codeblock><note>You
            can optionally specify the external network CIDR here too. If you choose not to
            excercise this option or use a wrong value, the VMs will not be accessible over the
            network.</note><codeblock>ansible-playbook -i hosts/verb_hosts hlm-cloud-configure.yml -e EXT_NET_CIDR=10.240.96.0/20</codeblock></li>
        <li>Run the following command, which will replace <codeph>/etc/hosts</codeph> on the
          deployer:
          <codeblock>ansible-playbook -i hosts/localhost cloud-client-setup.yml</codeblock> As the
          /etc/hosts file no longer has entries for HLM, sudo commands may become a bit slower. To
          fix this issue, once this step is complete, add "hlm" after "127.0.0.1 localhost". The
          result will look like
          this:<codeblock>...
# Localhost Information
127.0.0.1 localhost hlm
</codeblock></li>
      </ol>
    </section>
  </body>
</topic>
