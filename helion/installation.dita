<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="topic_flx_j2b_ws">
  <title>Beta 1: Installing HP Helion <tm tmtype="reg">OpenStack</tm> on Baremetal</title>
  <body>
    <section><title>Known Restrictions</title>
      <ul>
        <li>Ensure the <xref href="hardware.dita">minimum hardware requirements</xref> are met.</li>
        <li>Make sure all disks on the system(s) are wiped before you begin the install. /dev/sda
          will be used for the OS.  Disks (other than sda) that are not blank will be skipped
          (because the system is afraid you might have valuable data on them). Other disks will be
          used for whatever you configure them to be used for in your model.</li>
        <li>Both HP Linux for HP Helion OpenStack and HP Helion OpenStack are part of the ISO you
          will download from the <xref
            href="https://helion.hpwsportal.com/catalog.html#/Category/%7B%22categoryId%22%3A10311%7D/Show"
            format="html" scope="external">Helion Downloads</xref> page. There is only one download;
          it includes both the OS and the Beta1 installer in one ISO.</li>
        <li>Three NICs are tested, one 1Gb used for PXE and two bonded 10Gb for everything else. All
          machines can net boot from PXE and use the deployer as a DHCP server.</li>
        <li>All machines of a single type should be the same, that is, all compute nodes, all VSA
          nodes, and so on.</li>
        <li>The deployer node must be a dedicated node in Beta1.</li>
        <li>The machine hosting the deployer and all baremetal systems must be connected to a
          management network. Nodes on this management network must be able to reach the iLO
          subsystem of each baremetal system to enable host reboots as part of the install process.
          The HP Helion OpenStack architecture requires that the IPMI network is a separate network
          and that a route exists from the management network to the IPMI network for iLO
          access.</li>
      </ul>
    </section>
    <section id="Prereqs">
      <title>Before you Start</title>
      <p>Prepare your baremetal hardware, as follows, on all nodes:</p>
      <ul>
        <li>Setup the iLO Advanced license in the iLO configuration.</li>
        <li>Select the required mode (legacy BIOS or UEFI ) on each node through its BIOS settings
          before beginning the install process. HOS 2.0 will detect and use the mode you have
          selected for each node; UEFI support is new in Beta 1.</li>
        <li>Ensure that the network to be used for PXE installation has PXE enabled.</li>
        <li>Ensure that the other networks have PXE disabled.</li>
        <li>Insert the CD ROM in the Virtual Media drive on the iLO.</li>
      </ul>
    </section>
    <section id="DeployerInstall">
      <title>Set up the Deployer</title>
      <ol>
        <li>Create LUN(s), if required.</li>
        <li>Download the HP Helion OpenStack Deployer ISO from the <xref
            href="https://helion.hpwsportal.com/catalog.html#/Category/%7B%22categoryId%22%3A10311%7D/Show"
            format="html" scope="external">Helion Downloads</xref> page after signing up and being
          approved for the program.</li>
        <li>Boot your deployer from the ISO.</li>
        <li>Enter "install" to start installation.</li>
        <li>Select the language.</li>
        <li>Select the location.</li>
        <li>Select the keyboard layout.</li>
        <li>Select the primary network interface, if prompted:<ul>
            <li>Assign IP address, netmask</li>
          </ul></li>
        <li>Create new account:<ul>
            <li>Enter a username.</li>
            <li>Enter a password.</li>
            <li>Enter time zone if prompted to do so.</li>
            <li>Synchronize the time on all nodes manually. NTP will be installed later.</li>
          </ul></li>
      </ol>
      <p>At the end of this section you should have a deployer node set up with hLinux on it.</p>
    </section>
    <section id="HLM_Node_Personalization">
      <title>Configure and Run the Deployer</title>
      <ol>
        <li>Log in to the deployer node as the user you created during the setup phase, and mount
          the install media at /media/cdrom, for example,
          <codeblock>sudo mount /media/cdrom</codeblock></li>
        <li> Unpack the following tarball:
          <codeblock>tar zxvf /media/cdrom/hos-2.0.0/hos-2.0.0-1-20150827T092008Z.tgz</codeblock>
        </li>
        <li>Run the following included script:
          <codeblock>~/hlm-deployer/hos-init.bash </codeblock></li>
      </ol>
      <p>At the end of this section you should have a local directory structure, as described
        below:</p>
      <codeblock>
helion/                        Top level directory
helion/examples/               Directory contains the config input files of the example clouds
helion/my_cloud/definition/    Directory contains the config input files
helion/my_cloud/config/        Directory contains .j2 files which are symlinks to the /hos/ansible directory
helion/hos/                    Directory contains files used by the installer
</codeblock>
    </section>
    <section id="Configuration">
      <title>Configure Your Environment</title>
      <ol>
        <li>Setup your configuration files, as follows: <ol>
            <li>See the sample set of configuration files in the
              ~/helion/examples/entry-scale-with-vsa directory. The accompanying README.md file
              explains the contents of each of the configuration files. </li>
            <li>Copy the example configuration files into the required setup directory and edit them
              as required: <codeblock>cp -r ~/helion/examples/entry-scale-with-vsa/* </codeblock>
              Note: There have been changes to some of the files since Beta 0. The configuration
              files for editing can be found at the following location:
                <codeblock>~/helion/my_cloud/definition/data</codeblock><ul id="ul_wwn_rtx_bt">
                <li>The baremetalConfig.yml file should specify the server information for your
                  environment.</li>
                <li>The servers.yml file contains the IP address information for the hardware.</li>
                <li>The networks.yml file contains networking information.</li>
                <li>The control_plane.yml file contains information about the services that will be
                  installed.</li>
                <li>The <codeph>/data/disks_controller.yml</codeph> contains the Swift information
                  (swiftobj disk-group). This describes what disks on each of the servers are
                  assigned to Swift and the rings that uses those drives. To know more about the
                  disk drives, refer to <xref
                    href="swift/swift_input_cloud_model.dita#topic_qkg_l4v_bt">cloud model
                    modification</xref>.</li>
                <li>The <codeph>config/swift/rings.yml</codeph> specifies the ring which maps the
                  logical names of data to locations on a particular disks. To know more about the
                  rings, refer to <xref href="swift/ring_parameters.dita#topic_efw_k55_dt">ring
                    specification</xref>.</li>
                <li>In the <codeph>net_interfaces.yml</codeph> file, replace all instances of
                  “bond-mode: 1” to “bond-mode: active-backup”.</li>
              </ul></li>
            <li>Commit your configuration to the <xref href="using_git.dita#topic_u3v_1yz_ct">local
                git repo</xref>, as follows:
              <codeblock>git add –A
git commit –m "My config"
</codeblock></li>
          </ol></li></ol>

    </section>
    <section id="CobblerDeploy">
      <title>Deploy Cobbler </title>
      <ol>
        <li>Run the following command:
          <codeblock>export ANSIBLE_HOST_KEY_CHECKING=False</codeblock></li>
        <li>Run the following playbook:
          <codeblock>cd ~/helion/hos/ansible</codeblock><codeblock>ansible-playbook -i hosts/localhost cobbler-deploy.yml</codeblock></li>
      </ol>
    </section>
    <section id="NodeProvision"><title>Provision the Nodes</title>
      <ol>
        <li>Run the following command, which will reimage all the nodes using PXE:
          <codeblock>ansible-playbook -i hosts/localhost bm-reimage.yml</codeblock></li>
        <li>The script will wait for the nodes to install and come back up.</li>
      </ol>
    </section>
    <section><title>Run the Configuration Processor</title>
      <ol>
        <li>Run the configuration processor, as follows:
          <codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock>
          The configuration processor output is placed onto the following two private branches of
          the git repo: <ul>
            <li>staging-ansible</li>
            <li>staging-cp-persistent</li>
          </ul>
        </li>
        <li><ol>
          <li>Check the generated host files in
            <codeph>~/helion/my_cloud/stage/ansible/host_vars/</codeph> to ensure the correct
            IPs are included. You can review the ansible change by running the following command:
            <codeblock>git show staging-ansible</codeblock></li>
          <li>Use the
            <codeblock>ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock> file
            to create a deployment directory. Run your "verb_host" commands from the following
            directory: <codeblock> ~/scratch/ansible/next/hos/ansible</codeblock></li>
        </ol></li>
      </ol>
    </section>
    <section id="CloudDeploy"><title>Deploy the Cloud</title>
      <ol>
        <li>Run the following command:
            <codeblock>ansible-playbook -i hosts/verb_hosts osconfig-run.yml</codeblock><ol
            id="ol_rqv_xmm_dt">
            <li>Verify the network is working correctly. Ping each IP (excluding VSA-BLK and VIPs)
              from the /etc/hosts file from one of the controller nodes.</li>
          </ol>
        </li>
        <li> (<b>Optional</b>) To validate object storage cloud model, execute the following command
          on the deployer
            :<codeblock>ansible-playbook -i hosts/verb_hosts _swift-validate-input-model.yml</codeblock><p>If
            any error is triggered when you validate the object storage, refer to <xref
              href="swift/troubleshooting_beta1.dita#topic_sl2_nht_dt"
            >Swift-Troubleshooting</xref></p></li>
        <li>Modify the ./helion/hos/ansible/hlm-deploy.yml to comment out the line containing
          horizon.</li>
        <li>Run the following command:
          <codeblock>ansible-playbook -i hosts/verb_hosts hlm-deploy.yml -e tuning_selector=medium</codeblock>
        </li>
      </ol>
    </section>
    <section id="VSA">
      <title>Install and Configure VSA (optional).</title>
      <p>You can set up Virtual Storage Appliance (VSA) using the instructions on the following
        page: <xref href="vsa.dita#topic_hjr_4x1_dt">Install and Configure VSA
        (optional)</xref>.</p>
    </section>
    <section id="Post-Installation"><title>Verification Steps</title>
      <ol>
        <li>This step may be used to verify your cloud installation. Note that running the command
          below will download a cirros image from the internet, upload it to Glance, and create a
          Neutron external
            network.<codeblock>ansible-playbook -i hosts/verb_hosts hlm-cloud-configure.yml</codeblock><note>You
            can optionally specify the external network CIDR here too. If you choose not to
            excercise this option or use a wrong value, the VMs will not be accessible over the
            network.</note><codeblock>ansible-playbook -i hosts/verb_hosts hlm-cloud-configure.yml -e EXT_NET_CIDR=10.240.96.0/20</codeblock></li>
        <li>Run the following command, which will replace <codeph>/etc/hosts</codeph> on the
          deployer:
          <codeblock>ansible-playbook -i hosts/localhost cloud-client-setup.yml</codeblock> As the
          /etc/hosts file no longer has entries for HLM, sudo commands may become a bit slower. To
          fix this issue, once this step is complete, add "hlm" after "127.0.0.1 localhost". The
          result will look like
          this:<codeblock>...
# Localhost Information
127.0.0.1 localhost hlm
</codeblock></li>
      </ol>
    </section>
  </body>
</topic>
