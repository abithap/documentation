<?xml version="1.0" encoding="UTF-8"?>
<!--This work by HP Helion Openstack is licensed under a Creative Commons Attribution-ShareAlike 3.0 Unported License. See the accompanying LICENSE file for more information.-->
<!DOCTYPE concept PUBLIC "-//OASIS//DTD DITA Concept//EN" "concept.dtd">
<concept id="input_model">
  <title>Helion <tm tmtype="reg">OpenStack</tm> 2.0 Input Model</title>
  <conbody>
    <section id="intro"><title>Introduction</title>
      <p>This page describes how the Helion OpenStack 2.0 Input Model can be used to define and
        configure your cloud environment.</p>
      <p>The Helion OpenStack 2.0 product ships with a set of sample input-models that can be used
        as starting points for defining your custom cloud.</p>
      <p>The input model allows the user to describe the cloud configuration in terms of:</p>
      <p><ul>
          <li>Cloud Topology <ul>
              <li>Which OpenStack services run on which server nodes</li>
              <li>How individual servers are configured in terms of disk and network adapters</li>
              <li>Describes the overall network configuration of the cloud</li>
              <li>Network traffic separation</li>
              <li>CIDR and VLAN assignments</li>
            </ul>
          </li>
        </ul></p>
      <p>The document is structured as follows:</p>
      <p><ul>
          <li><uicontrol>Concepts</uicontrol> - This explains the ideas behind the declarative model
            approach used in Helion OpenStack 2.0 and the core concepts used in describing that
            model</li>
          <li><uicontrol>Input Model</uicontrol> - This section provides a description of each of
            the configuration entities in the input model</li>
          <li><uicontrol>Core Examples</uicontrol> - In this section we provide samples and meanings
            of some of the more important configuration entities</li>
        </ul></p>
    </section>
    <section id="3.0_concepts"><title>Helion OpenStack 2.0 Concepts</title>
      <p>A Helion OpenStack 2.0 system is defined by a declarative model that is described in a
        series of configuration objects. These objects are stored in YAML files and the product
        ships with a number of examples. These examples can potentially be used almost as shipped –
        just modifying to provide site specific IP addresses, or customized to meet site
        requirements.</p>
      <p><image href="../media/config/concept_diag.png"/></p>
    </section>
    <section id="3.1_Cloud"><title>Cloud</title>
      <p><i>A cloud consists of one or more control planes.</i></p>
      <p>The <codeph>cloudConfig.yml</codeph> file will define your Cloud name, NTP server
        configuration, DNS settings, and your SMTP settings.</p>
    </section>
    <section id="3.2_ControlPlanes"><title>Control Planes</title>
      <p><i>A control plane runs one or more <uicontrol>services</uicontrol> distributed across
            <uicontrol>clusters</uicontrol> and <uicontrol>resource groups</uicontrol></i></p>
      <p><i>A control plane uses servers with a particular
        <uicontrol>server-role</uicontrol></i></p>
      <p>A Control Plane provides the operating environment for a set of services; normally
        consisting of set of shared services (MySQL, RabbitMQ, HAproxy, Apache, etc), OpenStack
        control services (API, schedulers, etc) and the resources they are managing (compute,
        storage, etc)</p>
      <p>A simple cloud may have a single control plane which runs all of the services. A more
        complex cloud may have multiple control planes with a relationship between them. Services
        that need to consume (use) another service (such as Neutron consuming MySql, Nova consuming
        Neutron) always use the service within the same control plane before looking in any related
        control planes. It is the job of the HLM configuration processor to resolve these
        relationships and make sure that each service is provided with the configuration details to
        connect to the appropriate service.</p>
      <p>A Control Plane is structured as <uicontrol>clusters</uicontrol> and
          <uicontrol>resource-nodes</uicontrol>. The clusters are typically used to host the
        OpenStack services used to manage the cloud: such as API servers, Database servers, Neutron
        agents, Swift Proxies; while the Resource servers are used to host those scale out OpenStack
        services like Nova-Compute or Swift-Object service. This is a representation convenience
        rather than a strict rule, for example it is possible to locate the Swift-Object service in
        the management cluster in a smaller scale cloud that is not designed to scale out object
        serving.</p>
      <p>A cluster can be one or more servers, and you can have one or more clusters depending on
        the scale of the cloud that you are building. Spreading services across multiple clusters is
        more scalable, but of course it requires more servers. A common pattern for a large cloud is
        to run high data volume services such as monitoring and logging on a separate cluster. A
        cloud with a high object storage requirement will typically also run the Swift service on
        its own cluster.</p>
      <p>Clusters in this context are a mechanism for grouping service components onto physical
        servers, but all instances of a component in a control plane work collectively. So for
        example, if ha-proxy is configured into multiple clusters within the same control plane then
        all of those instances will work as a single instance of the ha-proxy services.</p>
      <p>In addition to its cluster, a control plane also has a collection of resource servers which
        provide compute and storage resources.</p>
    </section>
    <section id="3.2.1_ControlPlanesAndRegions"><title>Control Planes and Regions</title>
      <p>A region in OpenStack terms is a collection of URLs that together provide a set of services
        (Nova, Neutron, Swift, etc). As a user you have to explicitly decide which region you want
        to use.</p>
      <p>As the owner of a cloud, regions provide a way of segmenting resources for scale,
        resilience, and isolation. However, each region needs some resources for its control
        plane.</p>
      <p>Regions don't have to be disjointed. For example you can have a Swift service shared across
        more than one region, in which case the Swift URL for both regions will be the same. Some
        services do however have to be co-located in the same region (Nova, Neutron and Cinder for
        example)</p>
      <p>The way we express this in the cloud model is to talk about a Control Plane providing some
        or all of the endpoints for one or more Regions.</p>
    </section>
    <section id="3.3_Services"><title>Services</title>
      <p><i>A control plane runs one or more services</i></p>
      <p>A service is the collection of <uicontrol>service-components</uicontrol> that provide a
        particular feature; for example Nova provides the compute service and consists of the
        following service-components: nova-api, nova-scheduler, nova-conductor, nova-novncproxy, and
        nova-compute. Some services, like the authentication service Keystone only consist of a
        single service-element.</p>
      <p>All you need to know about services to define your cloud is the names of the service
        components. The details of the services themselves, and how they interact with each other is
        captured in service definition files provided by Helion OpenStack.</p>
      <p>When specifying your Helion OpenStack cloud you have to decide where components will run,
        and how they connect to the networks. For example, should they all run in one control plane
        sharing common services, or be distributed across multiple control planes to provide
        separate instances of some services. The supplied examples provide solutions for some
        typical configurations.</p>
      <p>We'll talk more about how to describe how services connect to the network later.</p>
    </section>
    <section id="3.4_ServerRoles"><title>Server Roles</title>
      <p><i>A region uses servers with a particular server role</i></p>
      <p>Clearly you're going to be running the services on some servers, and you're going to need
        to way to specify which servers you want to use where - and this is where the server-role
        comes in. A server role is just a name you use to identify a particular set of servers.
        You'll generally use a different role whenever the servers are physically different (have
        different disks or network interfaces), or if you want to use some specific servers in a
        particular role (for example to choose which of a set of identical servers are to be used in
        the control plane).</p>
      <p>Each server-role has a relationship to two other entities: <ul>
          <li>A server-role identifies which disk model to use to control how to configure and use
            its local storage</li>
          <li>A server-role identifies an interface model which describes how its network interfaces
            are to be configured and used. We'll cover this in more details the networking
            section.</li>
        </ul>
      </p>
    </section>
    <section id="3.5_DiskModel"><title>Disk Model</title>
      <p><i>Disk devices are associated with a device group or a volume group</i></p>
      <p><i>Device Groups are consumed by services</i></p>
      <p><i>Volume Groups are divided into Logical Volumes</i></p>
      <p><i>Logical Volumes are mounted as file systems or consumed by services</i></p>
      <p>A disk model defines how local storage is to be configured and presented to services. Disk
        models are identified by a name, which can be specified by the user. The configuration
        examples provide some typical configurations, although as this is an area that varies both
        on the services that are hosted on a server and the number of disks available it is hard to
        cover all permutations, and you may need to modify the examples to meet your
        requirements.</p>
      <p>Within a disk model, disks devices are specified as being part of either a
          <uicontrol>device-group</uicontrol> or a <uicontrol>volume-group</uicontrol>.</p>
      <p><image href="../media/config/disk_model.png"/></p>
      <p>A <uicontrol>device-group</uicontrol> is a set of one or more disks that are to be consumed
        directly by a service - for example, a set of disks to be used by Swift. The device-group
        identifies the list of disk devices, the service, and a few service specific attributes that
        tell the service about the intended use (for example, in the case of Swift this is the ring
        names). The service is responsible for the management of the disks, including creating and
        mounting file systems. (Swift can provide additional data integrity when it has full control
        over the filesystems and mount points)</p>
      <p>A <uicontrol>volume-group</uicontrol> is used to present disk devices as physical volumes
        in a LVM volume group. It also contains details of the Logical Volumes to be created,
        including the file system type and mount point. Logical Volume sizes are expressed as a
        percentage of the total capacity of the volume group. A Logical Volume can also be specified
        to be consumed by a service in the same way as a device-group. This supports the case of
        services which expect to manage their own devices on configurations with limited numbers of
        disk drives (for example single node demo systems).</p>
    </section>
    <section id="3.6_Servers"><title>Servers</title>
      <p><i>Servers have a role which determines how they will be used in the cloud</i></p>
      <p>Servers enumerate the resources available for your cloud. A server could be a physical
        server or a virtual machine, all Helion OpenStack requires is that it is capable of booting
        a supported operating system and that it has an IP address that can be accessed via SSH. In
        addition to this IP address, each server says what its <uicontrol>server-role</uicontrol>
        is.</p>
    </section>
    <section id="3.7_ServerGroups"><title>Server Groups</title>
      <p><i>A server is associated with a Server Group</i></p>
      <p><i>A control-plane can use Server Groups as failure zones for server allocation</i></p>
      <p><i>A server group may be associated with a list of networks</i></p>
      <p><i>A server group can contain other server groups</i></p>
      <p>The concept of physical servers located in a number of racks or enclosures in a data center
        is one that everyone can understand. Such racks generally provide a degree of physical
        isolation such as separate power and/or network connectivity. In a multi-region cloud,
        individual racks or groups of racks will normally be allocated to a specific region.</p>
      <p>In the Helion OpenStack model we support this concept by allowing you to define a hierarchy
        of server groups. Each server is associated with one server group, normally at the bottom of
        the hierarchy.</p>
      <p>Server groups are an optional part of the input model – if you don’t define any then all
        servers and networks will be allocated as if they are part of the same group.</p>
    </section>
    <section id="3.7.1_ServerGroupsAndFailureZones"><title>Server Groups and Failure Zones</title>
      <p>A control plane defines a list of server groups as the failure zones it wants to use
        servers from. All servers in a server group listed as a failure zone in the control plane,
        and any server groups they contain, are considered part of that failure zone for allocation
        purposes. For example, three levels of server groups could be used to model a failure zone
        consisting of multiple racks each of which in turn contain a number of enclosures.</p>
      <p><image href="../media/config/server_groups.png"/></p>
      <p>When allocating servers, the configuration processor will traverse down the hierarchy of
        server groups listed as failure zones until it can find an available server with the
        required role. It will allocate servers equally across each of the failure zones. A cluster
        or resource group can also independently specify the failure zone(s) it wants to use if
        needed.</p>
    </section>
    <section id="3.7.2_ServerGroupsAndNetworks"><title>Server Groups and Networks</title>
      <p>Each L3 network in a cloud needs to be associated with all or some of the servers, normally
        following a physical pattern (such as having separate networks for each rack or set of
        racks). This is also represented in the Helion OpenStack model via server groups, each
        server group listing zero or more networks that servers associated to server groups at or
        below this point in the hierarchy are connected to.</p>
      <p>When the Configuration Processor needs to resolve which specific network a server should be
        configured to use it traverses up the hierarchy of server groups, starting with the group
        the server is associated with, until it finds a server group which lists a network in the
        required network group.</p>
      <p>The level in the server group hierarchy at which a network is associated will depend on the
        span of connectivity it has to provide. In the above example there might be networks in some
        network groups which are per rack (i.e. Rack 1 and Rack 2 list different networks from the
        same network group) and networks in a different network group that have to span failure zone
        (the network used to provide floating IP addresses to VMs for example).</p>
    </section>
    <section id="3.8_Networking"><title>Networking</title>
      <p>In addition to the mapping of services to specific servers we also have to be able to
        define how the services connect to one or more networks.</p>
      <p>In a simple cloud there may be a single L3 network, but normally at least the External APIs
        that users will use to access the cloud, and the External Addresses that users will use to
        access their VMS are on separate networks. In more complex clouds it's not unusual to also
        separate out virtual networking between VMs, Block Storage traffic, and Volume traffic onto
        their own sets of networks. In a large cloud there may be separate L3 network segments for
        each rack (where the traffic is routeable - some traffic has to be on the same segment).</p>
      <p>To allow this kind of network segmentation we define the network configuration in two
        levels, <uicontrol>network-groups</uicontrol> and <uicontrol>networks</uicontrol>.</p>
    </section>
    <section id="3.8.1_NetworkGroups"><title>Network Groups</title>
      <p><i>Service endpoints attach to networks in a specific Network Group</i></p>
      <p><i>Network Groups can define routes to other networks</i></p>
      <p><i>Network Groups encapsulate the configuration for services via
            <uicontrol>network-tags</uicontrol></i></p>
      <p>A network-group defines the traffic separation model, and all of the properties that are
        common to the set of L3 *networks* that carry each type of traffic. They define where
        services are attached to the network model, and the routing within that model.</p>
      <p>Each network-group provides isolation for a collection of network traffic between service
        elements and between service-elements and external clients. The starting point for
        configuring a network-group then is to define the service endpoints that connect to it. The
        details of how each service connects, such as what port it uses, if it should be behind a
        load balancer, if and how it should be registered in Keystone, etc are defined in the
        service definition files provided by Helion OpenStack. All that has to be captured in the
        network-groups definition is the same service-element names that are used when defining
        regions. Helion OpenStackl also allows a "default" attachment to be used to specify "all
        service-elements" that aren't specifically connected to another network-group; so for
        example, to isolate Swift traffic the swift-account, swift-container, and swift-object
        service elements are attached to an "Object" network group and all other services are
        connected to a "Management" network group via the "default" relationship.</p>
      <p>In any multiple network configuration, controlling the network routing is a major concern
        and this is also defined at the network group level. Firstly, all networks are configured to
        provide the route to any other networks in the same network group. In addition, a
        network-group can be defined to have a route to other network-groups; for example if the
        internal APIs are in a dedicated network group (a common configuration in a complex network
        since a network group with load balancers cannot be segmented) then other network groups may
        need to include a route to this network so that services can access the internal API
        endpoints. Routes may also be required to define how to access external storage network,
        an/or to define a general default route.</p>
      <p>As part of the Helion OpenStack deployment, networks are configured to act as the default
        route for all traffic that was received via that network (so that response packets always
        return via the network the request came from).</p>
      <p>Note that Helion OpenStack will configure the routing rules on the servers it deploys, but
        ensuring that gateways can provide the required routes is the responsibility of your network
        configuration.</p>
    </section>
    <section id="3.8.1.1_LoadBalancers"><title>Load Balancers</title>
      <p>Load Balancers provide a specific type of routing and are also defined in network-groups as
        a relationship between a VIP on a network in one network group and a set of service
        endpoints (which may be on networks in the same or a different network group).</p>
      <p>Each load balancer is defined as part of the network group where the virtual IP will be
        presented – it follows that a network group containing a load balancer can only have one
        network associated to it.</p>
      <p>The load balancer definition includes a list of service-elements and roles it will provide
        A VIP for.</p>
      <p>The list of service-elements allows service specific load balancers to be defined on
        networks in different network-groups. As with other service-element relationships, all that
        is needed in the service-element name, the details of how the load balancer should be
        configured, are provided in the supplied service definitions. A "default" value is accepted
        to express "all service-elements which expect to be behind a load balancer and are not
        explicitly configured in another load balancer configuration". Note that is not necessary to
        say which network groups the service-elements that are being load balanced are attached to,
        the Configuration Processor can deduce that information.</p>
      <p>The endpoint roles make it possible to configure separate load balancers for public and
        internal access to services, and the Configuration Processor uses this information to both
        ensure the correct registrations in Keystone and to make sure internal traffic is routed to
        the correct endpoint. Helion services are configured to only connect to other services via
        internal VIPs and endpoints, allowing the name and security certificate of public endpoints
        to be controlled by the customer and set to values that may not be resolvable or accessible
        from the servers making up the Helion cloud.</p>
      <p>Note that each load balancer defined in the input model will be allocated a separate VIP,
        even when the load balancers are part of the same network group. Because of the need to be
        able to separate public and internal access Helion will not allow a single load balancer to
        provide both public and internal access. Load balancers in this context are logical entities
        (sets of rules to transfer traffic from a VIP to one or more endpoints) – multiple load
        balancers may be implemented by the same service within the cloud (e.g. a ha-proxy
        cluster).</p>
      <p>The following diagram shows a possible configuration in which the URL associated with the
        public endpoint has been set to a name that resolves for customers to a firewall controlling
        access to the cloud. Helion services use the internal URL to access a separate VIP.</p>
      <p><image href="../media/config/load_balancers.png"/></p>
    </section>
    <section id="3.8.1.2_NetworkTags"><title>Network Tags</title>
      <!-- need words here on the tags concept -->
      <p>Network tags are defined by Helion OpenStack service components, and are used in the input
        model to convey information between the model and the service.</p>
      <p>Network tags convey information about the physical network configuration to services,
        allowing the dependent aspects of the service to be automatically configured, for example an
        option in a service configuration file that contains the IP address of the corresponding
        network device on a server where that service component is installed.</p>
      <p>Network tags convey requirements a service may have on aspects of the server network
        configuration, for example that a bridge is required on the corresponding network device on
        a server where that service component is installed.</p>
      <p>See <xref href="#input_model/4.9.1_NetworkTags">Network Tags</xref> for more information on
        specific tags and their usage.</p>
    </section>
    <section id="3.8.2_Networks"><title>Networks</title>
      <p><i>A Network is part of a Network Group</i>></p>
      <p><i>A Network can be associated with a Server Group</i></p>
      <p>Networks, by comparison, are fairly simple definitions. Each network defines the CIDR,
        start and end address, gateway, and VLAN ID.</p>
    </section>
    <section id="3.8.3_InterfaceModel"><title>Interface Model</title>
      <p><i>A server-role identifies an interface model which describes how its network interfaces
          are to be configured and used.</i>></p>
      <p>Networks are mapped onto specific network interfaces via an interface-model, which
        describes the network devices that need to be created (bonds, ovs-bridges, etc) their
        properties, and the network-groups that are associated with them.</p>
      <p>An interface model acts like template; it can define how some or all of the network-groups
        are to be mapped for a particular combination of physical NICs. However, it is the
        service-elements on each server that determine which network-groups are required and hence
        which interfaces and networks will be configured.</p>
      <p>Interface-models can be shared between different server-roles (say an API role and a
        database role which may still need different disk models) even though these will require a
        different subset of the network groups providing the servers have the same number of network
        interfaces.</p>
      <p>Within an interface-model, physical ports are identified either by their device name
        (eth0), or where there are multiple interfaces by a port name that is resolved via a NIC
        mapping.</p>
      <p>To allow different physical servers to share an interface-model, the NIC mapping is defined
        as an optional property of each server.</p>
    </section>
    <section id="3.8.4_NICMapping"><title>NIC Mapping</title>
      <p>Where a server has more than a single NIC, or two NICs that are to be a single bond, then
        we need a NIC mapping to unambiguously identify each individual NIC. Interface enumeration
        (eth0, eth1, eth2, ...) is not a reliable mechanism, and so we use PCI bus enumeration
        instead.</p>
      <p>This level of abstraction will also allow a network function on SRIOV capable network cards
        to be used for network connections and consumed by services in a future release.</p>
    </section>
    <section id="4.0_ConfigurationObjects"><title>Helion OpenStack 2.0 Configuration Objects</title>
    </section>
    <section id="4.1_Cloud"><title>Cloud</title>
      <p>The top level cloud configuration file defines some global values for the Helion Cloud.</p>
      <table frame="all" rowsep="1" colsep="1" id="cloud_config">
        <tgroup cols="2">
          <colspec colname="c1" colnum="1"/>
          <colspec colname="c2" colnum="2"/>
          <thead>
            <row>
              <entry>Key</entry>
              <entry>Value Description</entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry>name</entry>
              <entry>A user-defined name for the cloud</entry>
            </row>
            <row>
              <entry>hostname-data (optional)</entry>
              <entry>Provides control over some parts of the generated names (see <xref
                  href="#input_model/5.1_NameGeneration">Name Generation</xref>) <p>Consists of two
                  values: <ul>
                    <li>host-prefix - default is to use the cloud name (above)</li>
                    <li>member-prefix - default is "-m"</li>
                  </ul></p></entry>
            </row>
            <row>
              <entry>ntp-servers (optional)</entry>
              <entry>A list of external NTP servers your cloud has access to. If specified by name,
                rather than IP, then the names need to be resolvable via the external DNS
                nameservers you specify in the next section. All servers running the "ntp-server"
                component will be configured to use these external NTP servers you specify
                here.</entry>
            </row>
            <row>
              <entry>dns-settings (optional)</entry>
              <entry>DNS configuration data that will be applied to all servers. See example
                configuration for a full list of values.</entry>
            </row>
            <row>
              <entry>smtp-settings (optional)</entry>
              <entry>SMTP client configuration data that will be applied to all servers. See example
                configurations for a full list of values.</entry>
            </row>
          </tbody>
        </tgroup>
      </table>
      <p>Here is the sample file included with the install:</p>
      <codeblock>
---
  product:
    version: 2

   cloud:
    <b>name: helion-entry-scale</b>

    # The following values are used when
    # building hostnames
    <b>hostname-data:</b>
        host-prefix: helion
        member-prefix: -m
        
    # Path to the rest of the input model
    data-dir: data

    # List of ntp servers for your site
    <b>ntp-servers:</b>
    #    - "ntp-server1"
    #    - "ntp-server2"

    # dns resolving configuration for your site
    # refer to resolv.conf for details on each option
    <b>dns-settings:</b>
    #  nameservers:
    #    - name-server1
    #    - name-server2
    #    - name-server3
    #
    #  domain: sub1.example.net
    #
    #  search:
    #    - sub1.example.net
    #    - sub2.example.net
    #
    #  sortlist:
    #    - 192.168.160.0/255.255.240.0
    #    - 192.168.0.0
    #
    #  # option flags are '&lt;name>:' to enable, remove to unset
    #  # options with values are '&lt;name>:&lt;value>' to set
    #
    #  options:
    #    debug:
    #    ndots: 2
    #    timeout: 30
    #    attempts: 5
    #    rotate:
    #    no-check-names:
    #    inet6:
    <b>smtp-settings:</b>
    #  server: smtp1.hp.com
    #  port: 25</codeblock>
      <note>Ensure that you remove the <codeph>#</codeph> tags in front of your nameservers so they
        are recognized.</note>
    </section>
    <!--
    <section id="baremetalConfig.yml"><title>baremetalConfig.yml</title>
      <p>The path to this file is:</p>
      <p><codeblock>~/helion/my_cloud/definition/data/baremetalConfig.yml</codeblock></p>
      <p>The key values to enter into this document are the details about each of the servers in
        your environment. Most of these values will be gathered from the iLO console or from your
        vlan setup information.</p>
      <p>Here is the sample file included with the install:</p>
      <codeblock>

  product:
    version: 2
  <b>baremetal_network:
    subnet: 192.168.10.0
    netmask: 255.255.255.0
    server_interface: eth2</b>
   baremetal_servers:
     -
        node_name: controller1
        node_type: ROLE-CONTROLLER
        <b>pxe_mac_addr: b2:72:8d:ac:7c:6f
        pxe_interface: eth2
        pxe_ip_addr: 192.168.10.3
        ilo_ip: 192.168.9.3
        ilo_user: admin
        ilo_password: password</b>
     -
        node_name: controller2
        node_type: ROLE-CONTROLLER
        <b>pxe_mac_addr: 8a:8e:64:55:43:76
        pxe_interface: eth2
        pxe_ip_addr: 192.168.10.4
        ilo_ip: 192.168.9.4
        ilo_user: admin
        ilo_password: password</b>
     -
        node_name: controller3
        node_type: ROLE-CONTROLLER
        <b>pxe_mac_addr: 26:67:3e:49:5a:a7
        pxe_interface: eth2
        pxe_ip_addr: 192.168.10.5
        ilo_ip: 192.168.9.5
        ilo_user: admin
        ilo_password: password</b>
     -
        node_name: compute1
        node_type: ROLE-COMPUTE
        <b>pxe_mac_addr: d6:70:c1:36:43:f7
        pxe_interface: eth2
        pxe_ip_addr: 192.168.10.6
        ilo_ip: 192.168.9.6
        ilo_user: admin
        ilo_password: password</b>
     -
        node_name: compute2
        node_type: ROLE-COMPUTE
        <b>pxe_mac_addr: 8e:8e:62:a6:ce:76
        pxe_interface: eth2
        pxe_ip_addr: 192.168.10.7
        ilo_ip: 192.168.9.7
        ilo_user: admin
        ilo_password: password</b>
     -
        node_name: compute3
        node_type: ROLE-COMPUTE
        <b>pxe_mac_addr: 22:f1:9d:ba:2c:2d
        pxe_interface: eth2
        pxe_ip_addr: 192.168.10.8
        ilo_ip: 192.168.9.8
        ilo_user: admin
        ilo_password: password</b>
     -
        node_name: vsa1
        node_type: ROLE-VSA
        <b>pxe_mac_addr: 8b:f6:9e:ca:3b:78
        pxe_interface: eth2
        pxe_ip_addr: 192.168.10.9
        ilo_ip: 192.168.9.9
        ilo_user: admin
        ilo_password: password</b>
     -
        node_name: vsa2
        node_type: ROLE-VSA
        <b>pxe_mac_addr: 8b:f6:9e:ca:3b:79
        pxe_interface: eth2
        pxe_ip_addr: 192.168.10.10
        ilo_ip: 192.168.9.10
        ilo_user: admin
        ilo_password: password</b>
     -
        node_name: vsa3
        node_type: ROLE-VSA
        <b>pxe_mac_addr: 8b:f6:9e:ca:3b:7a
        pxe_interface: eth2
        pxe_ip_addr: 192.168.10.11
        ilo_ip: 192.168.9.11
        ilo_user: admin
        ilo_password: password</b></codeblock>
    </section>
    -->
    <section id="4.2_ControlPlane"><title>Control Plane</title>
      <p>The snippet below shows the control plane definition file:</p>
      <codeblock>
---
  product:
    version: 2
        
  control-planes:
    - name: ccp
      region-name: region1
      failure-zones:
        - AZ1
        - AZ2
        - AZ3
      common-service-components:
        - logging-producer
        - monasca-agent
        - freezer-agent
        - stunnel
      clusters:
        - id: "1"
      name: c1
      server-role: ROLE-CONTROLLER
      member-count: 3
      allocation-policy: strict
      service-components:
        - ntp-server
        - swift-ring-builder
        - mysql
        - ip-cluster
        - apache2
        - keystone-api
        - keystone-client
        - rabbitmq
        - glance-api
        ...
        
  resource-nodes:
    - name: compute
      resource-prefix: comp
      server-role: ROLE-COMPUTE
      allocation-policy: any
      min-count: 0
      service-components:
        - ntp-client
        - nova-kvm
        - nova-compute
        - neutron-l3-agent
        - neutron-metadata-agent
        - neutron-openvswitch-agent
        - neutron-lbaasv2-agent</codeblock>
      <p>The key values to enter into this document are described in the table below:</p>
      <p>
        <table frame="all" rowsep="1" colsep="1" id="table_zrw_1z4_jt">
          <tgroup cols="2">
            <colspec colname="c1" colnum="1"/>
            <colspec colname="c2" colnum="2"/>
            <thead>
              <row>
                <entry>Key</entry>
                <entry>Value Description</entry>
              </row>
            </thead>
            <tbody>
              <row>
                <entry>name</entry>
                <entry>This name identifies the control plane and will be used as part of the
                  hostname prefix. (see <xref href="#input_model/5.1_NameGeneration">Name
                    Generation</xref>)</entry>
              </row>
              <row>
                <entry>region-name (optional)</entry>
                <entry>This name identifies the Keystone region within which services in the control
                  plane will be registered.</entry>
              </row>
              <row>
                <entry>failure-zones (optional)</entry>
                <entry>A list of Server Group names that servers for this control plane will be
                  allocated from. If no failure-zones are specified, only servers not associated
                  with a Server Group will be used.</entry>
              </row>
              <row>
                <entry>common-service-components (optional)</entry>
                <entry>This lists a set of service components that run on all servers in the control
                  plane (clusters and resource pools)</entry>
              </row>
              <row>
                <entry>clusters</entry>
                <entry>A list of clusters for this control plane (see <xref
                    href="#input_model/clusters" type="table">Clusters</xref>)</entry>
              </row>
              <row>
                <entry>resource-nodes</entry>
                <entry>A list of resource nodes for this control plane (see <xref
                    href="#input_model/resource_nodes" type="table">Resource Nodes</xref>)</entry>
              </row>
            </tbody>
          </tgroup>
        </table>
      </p>
      <p>To further breakdown what the options are in the <codeph>clusters</codeph> portion of this
        file, here are some field descriptions:</p>
      <p>
        <table frame="all" rowsep="1" colsep="1" id="clusters">
          <tgroup cols="2">
            <colspec colname="c1" colnum="1"/>
            <colspec colname="c2" colnum="2"/>
            <thead>
              <row>
                <entry>Key</entry>
                <entry>Value Description</entry>
              </row>
            </thead>
            <tbody>
              <row>
                <entry>id</entry>
                <entry>Each cluster must have a unique ID.</entry>
              </row>
              <row>
                <entry>name</entry>
                <entry>The cluster name provided is used in the hostname (see <xref
                    href="#input_model/5.1_NameGeneration">Name Generation</xref>). Cluster and
                  resource-node names must be unique within a control plane. This value is used to
                  persist server allocations (see <xref href="#input_model/5.2_PersistedData"
                    >Persisted Data</xref>) and cannot be changed once servers have been
                  allocated.</entry>
              </row>
              <row>
                <entry>server-role</entry>
                <entry>Only servers matching the specified role will be allocated to this cluster.
                  (see <xref href="#input_model/3.4_ServerRoles">Server Roles</xref> for a
                  description of server roles)</entry>
              </row>
              <row>
                <entry>service-components</entry>
                <entry>The list of service components (together with common-service-components for
                  the control plane) to be deployed on the servers allocated for the
                  cluster.</entry>
              </row>
              <row>
                <entry>
                  <p>member-count</p>
                  <p>min-count</p>
                  <p>max-count</p>
                  <p>(all optional)</p>
                </entry>
                <entry>
                  <p>Defines the number of servers to add to the cluster.</p><p>The number of
                    servers that can be supported in a cluster depends on the services it is
                    running. For example MySQL and RabbitMQ can only be deployed on clusters on 1
                    (non-HA) or 3 (HA) servers. Other services may support different sizes of
                    cluster.</p>
                  <p>If min-count is specified that at least that number of servers will be
                    allocated to the cluster. If min-count is not specified it defaults to a value
                    of 1.</p>
                  <p>If max-count is specified then the cluster will be limited to that number of
                    servers. If max-count is not specified then all servers matching the required
                    role and failure-zones will be allocated to the cluster.</p>
                  <p>If member-count is specified then it sets min-count and max-count to the
                    specified value.</p></entry>
              </row>
              <row>
                <entry>failure-zones (optional)</entry>
                <entry>A list of Server Groups that servers will be allocated from. If specified,
                  this overrides the value specified for the control-plane. If not specified, the
                  control-plane value is used. (see <xref
                    href="#input_model/3.7.1_ServerGroupsAndFailureZones">Server Groups and Failure
                    Zones</xref> for a description of server groups as failure zones)</entry>
              </row>
              <row>
                <entry>allocation-policy (optional)</entry>
                <entry>
                  <p>Defines how failure zones will be used when allocating servers.</p>
                  <p><b>strict</b>: Server allocations will be distributed across all specified
                    failure zones. (if max-count is not an exact factor of the number of zones than
                    some zones may provide one more server than other zones)</p>
                  <p><b>any</b>: Server allocations will be made from any combination of failure
                    zones</p>
                  <p>The default allocation-policy for a cluster is <i>strict</i></p>
                </entry>
              </row>
            </tbody>
          </tgroup>
        </table>
      </p>
      <p>To further breakdown what the options are in the <codeph>resource-nodes</codeph> portion of
        this file, here are some field descriptions:</p>
      <p>
        <table frame="all" rowsep="1" colsep="1" id="resource_nodes">
          <tgroup cols="2">
            <colspec colname="c1" colnum="1"/>
            <colspec colname="c2" colnum="2"/>
            <thead>
              <row>
                <entry>Key</entry>
                <entry>Value Description</entry>
              </row>
            </thead>
            <tbody>
              <row>
                <entry>name</entry>
                <entry><p>The name of this set of resource-nodes. Cluster names and resource-node
                    names must mutually be unique within a control plane.</p> This value is used to
                  persist server allocations (see <xref href="#input_model/5.2_PersistedData"
                    >Persisted Data</xref>), and cannot be changed once servers have been allocated.
                </entry>
              </row>
              <row>
                <entry>resource-prefix</entry>
                <entry>The resource-prefix is used in the name generation. (see <xref
                    href="#input_model/5.1_NameGeneration">Name Generation</xref>)</entry>
              </row>
              <row>
                <entry>server-role</entry>
                <entry>Only servers matching the specified role will be allocated to this cluster.
                  (see <xref href="#input_model/3.4_ServerRoles">Server Roles</xref> for a
                  description of server roles)</entry>
              </row>
              <row>
                <entry>service-components</entry>
                <entry>The list of service components (together with common-service-components for
                  the control plane) to be deployed on the servers in this resource group.</entry>
              </row>
              <row>
                <entry>
                  <p>member-count</p>
                  <p>min-count</p>
                  <p>max-count</p>
                  <p>(all optional)</p>
                </entry>
                <entry>
                  <p>Defines the number of servers to add to the resource group.</p>
                  <p>If min-count is specified that at least that number of servers will be
                    allocated. If min-count is not specified it defaults to a value of 0.</p>
                  <p>If max-count is specified then the resource group will be limited to that
                    number of servers. If max-count is not specified then all servers matching the
                    required role and failure-zones will be allocated to the resource group.</p>
                  <p>If member-count is specified then it sets min-count and max-count to the
                    specified value.</p>
                </entry>
              </row>
              <row>
                <entry>failure-zones (optional)</entry>
                <entry>A list of Server Groups that servers will be allocated from. If specified,
                  this overrides the value specified for the control-plane. If not specified, the
                  control-plane value is used. (see <xref
                    href="#input_model/3.7.1_ServerGroupsAndFailureZones">Server Groups and Failure
                    Zones</xref> for a description of server groups as failure zones)</entry>
              </row>
              <row>
                <entry>allocation-policy (optional)</entry>
                <entry>
                  <p>Defines how failure zones will be used when allocating servers.</p>
                  <p><b>strict</b>: Server allocations will be distributed across all specified
                    failure zones. (if max-count is not an exact factor of the number of zones than
                    some zones may provide one more server than other zones)</p>
                  <p><b>any</b>: Server allocations will be made from any combination of failure
                    zones</p>
                  <p>The default allocation-policy for a cluster is <i>any</i></p>
                </entry>
              </row>
            </tbody>
          </tgroup>
        </table>
      </p>
    </section>
    <section id="4.3_Servers"><title>Servers</title>
      <p>The servers configuration object is used to list the available servers for deploying the
        cloud.</p>
      <p>It can also be used as an input file to the operating system installation process. In this
        case some additional fields (identified below) will be necessary.</p>
      <p>
        <table frame="all" rowsep="1" colsep="1" id="servers">
          <tgroup cols="2">
            <colspec colname="c1" colnum="1"/>
            <colspec colname="c2" colnum="2"/>
            <thead>
              <row>
                <entry>Key</entry>
                <entry>Value Description</entry>
              </row>
            </thead>
            <tbody>
              <row>
                <entry>Id</entry>
                <entry>A user defined identifier for the server. Id's must be unique and are used to
                  track server allocations.</entry>
              </row>
              <row>
                <entry>ip-addr</entry>
                <entry>
                  <p>The IP address of the server for installation purposes.</p>
                  <p>This IP address is used by the config-processor to install and configure the
                    service components on this server.</p>
                  <p>When the servers.yml file is being used for operating system installation, this
                    IP address will be assigned to the node by the installation process.</p>
                  <p>This IP address (and associated NIC) can be re-used within the cloud
                    infrastructure. To enable this, the following conditions need to be met: <ol>
                      <li>The IP address must be a member of one of the network CIDRs defined in the
                        networks.yml file. (see <xref href="#input_model/3.8.2_Networks"
                          >Networks</xref>)</li>
                      <li>That network must be on an untagged VLAN.</li>
                    </ol></p>
                </entry>
              </row>
              <row>
                <entry>Role</entry>
                <entry>Identifies the role of the server. (see <xref
                    href="#input_model/3.4_ServerRoles">Server Roles</xref> for a description of
                  server roles)</entry>
              </row>
              <row>
                <entry>server-group (optional)</entry>
                <entry>Identifies the server group that this server belongs to. (see <xref
                    href="#input_model/3.7_ServerGroups">Server Groups</xref>)</entry>
              </row>
              <row>
                <entry>mac-addr (optional)</entry>
                <entry>Needed when <codeph>baremetalConfig.yml</codeph> file is being used for the
                  operating system installation. This identifies the MAC address on the server that
                  will be used to network install the operating system.</entry>
              </row>
              <row>
                <entry>ilo-ip (optional)</entry>
                <entry>Needed when the <codeph>baremetalConfig.yml</codeph> file is being used for
                  the operating system installation. This provides the IP address of the power
                  management (e.g. ipmi-ip, iLO) subsystem.</entry>
              </row>
              <row>
                <entry>ilo-user (optional)</entry>
                <entry>Needed when the <codeph>baremetalConfig.yml</codeph> file is being used for
                  the operating system installation. This provides the username of the power
                  management (e.g. ipmi-ip, iLO) subsystem.</entry>
              </row>
              <row>
                <entry>ilo-password (optional)</entry>
                <entry>Needed when the <codeph>baremetalConfig.yml</codeph> file is being used for
                  the operating system installation. This provides the user password of the power
                  management (e.g. ipmi-ip, iLO) subsystem.</entry>
              </row>
              <row>
                <entry>nic-mapping (optional)</entry>
                <entry>Name of the entry from the <codeph>nic-mappings.yml</codeph> file to apply to
                  this server in the <codeph>servers.yml</codeph> file. (see <xref
                    href="#input_model/4.8_NICMappings">NIC Mappings</xref>)</entry>
              </row>
            </tbody>
          </tgroup>
        </table>
      </p>
      <p>Here is a snippet of the <codeph>servers.yml</codeph> file included with the install:</p>
      <codeblock>
---
  product:
    version: 2
        
  servers:
    # NOTE: Addresses of servers need to be
    #       changed to match your environment.
    #
    #       Add additional servers as required
    #
    #       nic-mapping is optional.  The value specified is the name
    #       of an entry in the nic-mappings list from nic_mappings.yml.
    #       To use, uncomment and modify to match your environment.
    
    # Controllers
    - id: controller1
    ip-addr: 192.168.10.3
    role: ROLE-CONTROLLER
    # nic-mapping: HP_DL360_4PORT
    
    - id: controller2
    ip-addr: 192.168.10.4
    role: ROLE-CONTROLLER
    # nic-mapping: HP_DL360_4PORT
    
    - id: controller3
    ip-addr: 192.168.10.5
    role: ROLE-CONTROLLER
    # nic-mapping: HP_DL360_4PORT
       
    # Compute Nodes
    - id: compute1
    ip-addr: 192.168.10.6
    role: ROLE-COMPUTE
    # nic-mapping: MY_2PORT_SERVER
     
    # VSA Storage Nodes
    - id: vsa1
    ip-addr: 192.168.10.9
    role: ROLE-VSA
    # nic-mapping: MY_2PORT_SERVER</codeblock>
      <p>Here is a snippet of the <codeph>baremetalConfig.yml</codeph>file included with the
        install:</p>
      <codeblock>
---
  product:
    version: 2
    <b>baremetal_network:
       subnet: 192.168.10.0
       netmask: 255.255.255.0
       server_interface: eth2</b>
       baremetal_servers:
         -
         node_name: controller1
         node_type: ROLE-CONTROLLER
         <b>pxe_mac_addr: b2:72:8d:ac:7c:6f
           pxe_interface: eth2
           pxe_ip_addr: 192.168.10.3
           ilo_ip: 192.168.9.3
           ilo_user: admin
           ilo_password: password</b>
         -
         node_name: controller2
         node_type: ROLE-CONTROLLER
         <b>pxe_mac_addr: 8a:8e:64:55:43:76
           pxe_interface: eth2
           pxe_ip_addr: 192.168.10.4
           ilo_ip: 192.168.9.4
           ilo_user: admin
           ilo_password: password</b>
         -
         node_name: controller3
         node_type: ROLE-CONTROLLER
         <b>pxe_mac_addr: 26:67:3e:49:5a:a7
           pxe_interface: eth2
           pxe_ip_addr: 192.168.10.5
           ilo_ip: 192.168.9.5
           ilo_user: admin
           ilo_password: password</b>
         -
         node_name: compute1
         node_type: ROLE-COMPUTE
         <b>pxe_mac_addr: d6:70:c1:36:43:f7
           pxe_interface: eth2
           pxe_ip_addr: 192.168.10.6
           ilo_ip: 192.168.9.6
           ilo_user: admin
           ilo_password: password</b>
         -
         node_name: vsa1
         node_type: ROLE-VSA
         <b>pxe_mac_addr: 8b:f6:9e:ca:3b:78
           pxe_interface: eth2
           pxe_ip_addr: 192.168.10.9
           ilo_ip: 192.168.9.9
           ilo_user: admin
           ilo_password: password</b></codeblock>
    </section>
    <section id="4.4_ServerGroups"><title>Server Groups></title>
      <p>Server groups provide a mechanism for organizing servers and networks into a hierarchy that
        can be used for allocation and network resolution (see <xref
          href="#input_model/3.7_ServerGroups">Server Groups</xref>)</p>
      <p>
        <table frame="all" rowsep="1" colsep="1" id="server_groups">
          <tgroup cols="2">
            <colspec colname="c1" colnum="1"/>
            <colspec colname="c2" colnum="2"/>
            <thead>
              <row>
                <entry>Key</entry>
                <entry>Value Description</entry>
              </row>
            </thead>
            <tbody>
              <row>
                <entry>name</entry>
                <entry>A user defined name for the server group. The name is used to link
                  server-groups together, and to identify server-groups to be used as failure zones
                  in a Control Plane. (see <xref href="#input_model/4.2_ControlPlane">Control
                    Plane</xref>)</entry>
              </row>
              <row>
                <entry>server-groups (optional)</entry>
                <entry>A list of server group names that are nested below this group in the
                  hierarchy. Each server group can only be listed in one other server group (i.e. in
                  a strict tree topology).</entry>
              </row>
              <row>
                <entry>networks (optional)</entry>
                <entry>A list of network names (see <xref href="#input_model/4.10_Networks"
                    >Networks</xref>) that will be matched up to the servers via the server groups.
                  (see <xref href="#input_model/3.7.2_ServerGroupsAndNetworks">Server Groups and
                    Networks</xref>)</entry>
              </row>
            </tbody>
          </tgroup>
        </table>
      </p>
      <p>Here is the sample file included with the install:</p>
      <codeblock>
---
  product:
    version: 2
        
  server-groups:
   
  #
  # Server Groups provide a mechanism for organizing servers
  # into a hierarchy that reflected the physical topology.
  #
  # When allocating a server the configuration processor
  # will search down the hierarchy from the list of server
  # groups identified as the failure-zones for the control
  # plane until it finds an available server of the requested
  # role.   If the allocation policy is "strict" servers are
  # allocated from different failure-zones.
  #
  # When determining which network from a network group to
  # associate with a server the configuration processor will
  # search up the hierarchy from the server group containing the
  # server until it finds a network in the required network
  # group.
  #
  
  #
  # In this example there is only one network in each network
  # group and so we put all networks in the top level server
  # group.   Below this we create server groups for three
  # failure zones, within which servers are grouped by racks.
  #
  # Note: the association of servers to server groups is part
  # of the server definition (servers.yml)
  #
  
  #
  # At the top of the tree we have a
  # group for the global networks
  #
  - name: GROUP_GLOBAL_NET
    server-groups:
      - AZ1
      - AZ2
      - AZ3
        networks:
        - NET_EXTERNAL_API
        - NET_EXTERNAL_VM
        - NET_GUEST
        - NET_MGMT
        
  #
  # Create a group for each failure zone
  #
  - name: AZ1
    server-groups:
      - RACK1
        
  - name: AZ2
    server-groups:
      - RACK2
        
  - name: AZ3
    server-groups:
      - RACK3
   
  #
  # Create a group for each rack
  #
  - name: RACK1
  
  - name: RACK2
  
  - name: RACK3</codeblock>
    </section>
    <section id="4.5_ServerRoles"><title>Server Roles</title>
      <p>Server roles lists different server roles that you use in your cloud. Each server role is
        linked to two other configuration objects:</p>
      <p>
        <ul>
          <li>Disk model (see <xref href="#input_model/4.6_DiskModels">Disk Models</xref>)</li>
          <li>Interface model (see <xref href="#input_model/4.7_InterfaceModels">Interface
              Models</xref>)</li>
        </ul>
      </p>
      <p>Server roles are referenced in the servers (see <xref href="#input_model/4.3_Servers"
          >Servers</xref>) configuration object above.</p>
      <p>
        <table frame="all" rowsep="1" colsep="1" id="server_roles">
          <tgroup cols="2">
            <colspec colname="c1" colnum="1"/>
            <colspec colname="c2" colnum="2"/>
            <thead>
              <row>
                <entry>Key</entry>
                <entry>Value Description</entry>
              </row>
            </thead>
            <tbody>
              <row>
                <entry>name</entry>
                <entry>A user defined name for the role.</entry>
              </row>
              <row>
                <entry>interface-model</entry>
                <entry>A cross reference to the interface model to be used for this server
                    role.<p>Different server roles can use the same interface model.</p></entry>
              </row>
              <row>
                <entry>disk-model</entry>
                <entry>A cross reference to the disk model to use for this server role.<p>Different
                    server roles can use the same disk model.</p></entry>
              </row>
            </tbody>
          </tgroup>
        </table>
      </p>
      <p>Here is the sample file included with the install:</p>
      <codeblock>
---
  product:
     version: 2
        
  server-roles:
    
  - name: ROLE-CONTROLLER
    interface-model: INTERFACE_SET_CONTROLLER
    disk-model: DISK_SET_CONTROLLER
    
  - name: ROLE-COMPUTE
    interface-model: INTERFACE_SET_COMPUTE
    disk-model: DISK_SET_COMPUTE
  
  - name: ROLE-VSA
    interface-model: INTERFACE_SET_VSA
    disk-model: DISK_SET_VSA</codeblock>
    </section>
    <section id="4.6_DiskModels"><title>Disk Models</title>
      <p>The disk model is used to specify how the directly attached disks on the server should be
        configured. It can also identify which service or service component consumes the disk, e.g.
        Swift object server, and provide service specific information associated with the disk.</p>
      <p>Disks can be used as raw devices or as logical volumes and the disk model provides a
        configuration item for each.</p>
      <p>If the operating system has been installed by the installation process then the root disk
        will already have been set up as a Volume Group with a single logical volume. This logical
        volume will have been created on a partition identified, symbolically, in the configuration
        files as /dev/sda_root. This is due to the fact that different BIOS systems (UEFI, Legacy)
        will result in different partition numbers on the root disk.</p>
      <p><b>Device Groups</b></p>
      <p>
        <table frame="all" rowsep="1" colsep="1" id="device_groups">
          <tgroup cols="2">
            <colspec colname="c1" colnum="1"/>
            <colspec colname="c2" colnum="2"/>
            <thead>
              <row>
                <entry>Key</entry>
                <entry>Value Descriptions</entry>
              </row>
            </thead>
            <tbody>
              <row>
                <entry>name</entry>
                <entry>A user defined name for the device group</entry>
              </row>
              <row>
                <entry>devices</entry>
                <entry>A list of named devices to be assigned to this group. There must be at least
                  one device in the group.</entry>
              </row>
              <row>
                <entry>consumer</entry>
                <entry>Identifies the name of one of the storage services (e.g. one of the
                  following: Swift, Cinder, Ceph, VSA, etc) that will consume the disks in this
                  device group.</entry>
              </row>
              <row>
                <entry>consumer attributes</entry>
                <entry>These will vary according to the service consuming the device group. The
                  examples section provides sample content for the different services.</entry>
              </row>
            </tbody>
          </tgroup>
        </table>
      </p>
      <p><b>Volume Groups</b></p>
      <p>The <codeph>volume-groups</codeph> configuration object is used to define volume groups and
        their constituent logical volumes.</p>
      <p>Note that the volume groups are not exact analogs of device groups. A volume group
        specifies a set of physical volumes used to make up a volume group that is then subdivided
        into multiple logical volumes.</p>
      <p>The installation process automatically creates a volume group name “hlm-vg” on the first
        drive in the system. It creates a “root” logical volume on this. The volume group can be
        expanded by adding more physical volumes (see examples). In addition, it is possible to
        create more logical volumes on this volume group to provide dedicated capacity for different
        services or file system mounts.</p>
      <p>Additional volume groups can also be created using the <codeph>volume-groups</codeph> model
        specification.</p>
      <p>Here is a description of the fields in the <codeph>volume-groups</codeph> portion of the
        disk model files:</p>
      <p>
        <table frame="all" rowsep="1" colsep="1" id="volume_groups">
          <tgroup cols="2">
            <colspec colname="c1" colnum="1"/>
            <colspec colname="c2" colnum="2"/>
            <thead>
              <row>
                <entry>Key</entry>
                <entry>Value Descriptions</entry>
              </row>
            </thead>
            <tbody>
              <row>
                <entry>name</entry>
                <entry>The name that will be assigned to the volume group</entry>
              </row>
              <row>
                <entry>physical-volumes</entry>
                <entry><p>A list of physical disks that make up the volume group.</p> As installed
                  by the HOS OS install process the volume group “hlm-vg” will use a large partition
                  (sda_root) on the first disk. This can be expanded by adding additional
                  disk(s)</entry>
              </row>
              <row>
                <entry>logical-volumes</entry>
                <entry>A list of logical volume devices to be create from the above named volume
                  group</entry>
              </row>
              <row>
                <entry>name</entry>
                <entry>The name to assign to the logical volume</entry>
              </row>
              <row>
                <entry>size</entry>
                <entry>The size, expressed as a percentage of the entire volume group capacity, to
                  assign to the logical volume</entry>
              </row>
              <row>
                <entry>fstype (optional)</entry>
                <entry>The file system type to create on the logical volume. If non specified the
                  volume is not formatted</entry>
              </row>
              <row>
                <entry>mkfs-opts (optional)</entry>
                <entry>Options, e.g. “-O large_file” to pass to the mkfs command</entry>
              </row>
              <row>
                <entry>mount (optional)</entry>
                <entry>Mount point for the file system</entry>
              </row>
              <row>
                <entry>consumer attributes (optional, consumer dependent)</entry>
                <entry>
                  <p>These will vary according to the service consuming the device group. The
                    examples section provides sample content for the different services.</p>
                  <p>Note, not all services support the use of logical volumes. VSA requires raw
                    devices.</p></entry>
              </row>
            </tbody>
          </tgroup>
        </table>
      </p>
      <p>Here is the sample file for <codeph>disks_compute.yml</codeph> included with the
        install:</p>
      <codeblock>
---
  product:
    version: 2
        
  disk-models:
  - name: DISK_SET_COMPUTE
    # Disk model to be used for compute nodes
    # /dev/sda_root is used as a volume group for /, /var/log and /var/crash
    # sda_root is a templated value to align with whatever partition is really used
    # This value is checked in os config and replaced by the partition actually used
    # on sda e.g. sda1 or sda5
    # /dev/sdb is used as a volume group for /var/lib (for VM storage)
    # Additional discs can be added to either volume group
    volume-groups:
    # The policy is not to consume 100% of the space of each volume group.
    # 5% should be left free for snapshots and to allow for some flexibility.
        - name: hlm-vg
          physical-volumes:
            - /dev/sda_root
          logical-volumes:
            - name: root
              size: 35%
              fstype: ext4
              mount: /
            - name: log
              size: 50%
              mount: /var/log
              fstype: ext4
              mkfs-opts: -O large_file
            - name: crash
              size: 10%
              mount: /var/crash
              fstype: ext4
              mkfs-opts: -O large_file
        
        - name: vg-comp
          # this VG is dedicated to Nova Compute to keep VM IOPS off the OS disk
          physical-volumes:
            - /dev/sdb
          logical-volumes:
            - name: compute
              size: 95%
              mount: /var/lib/nova
              fstype: ext4
              mkfs-opts: -O large_file</codeblock>
      <p>Here is the sample file for <codeph>disks_controller.yml</codeph> included with the
        install:</p>
      <codeblock>
---
  product:
    version: 2
        
   disk-models:
   - name: DISK_SET_CONTROLLER
     # /dev/sda_root is used as a Volume Group for /, /var/log, and /var/crash
     # sda_root is a templated value to align with whatever partition is really used
     # This value is checked in os config and replaced by the partition actually used
     # on sda e.g. sda1 or sda5
     volume-groups:
     # The policy is not to consume 100% of the space of each volume group.
     # 5% should be left free for snapshots and to allow for some flexibility.
        - name: hlm-vg
          physical-volumes:
            - /dev/sda_root
          logical-volumes:
            - name: root
              size: 30%
              fstype: ext4
              mount: /
            - name: log
              size: 40%
              mount: /var/log
              fstype: ext4
              mkfs-opts: -O large_file
            - name: crash
              size: 10%
              mount: /var/crash
              fstype: ext4
              mkfs-opts: -O large_file
            - name: elasticsearch
              size: 10%
              mount: /var/lib/elasticsearch
              fstype: ext4
            - name: zookeeper
              size: 5%
              mount: /var/lib/zookeeper
              fstype: ext4
              consumer:
              name: os
        
      # Additional disk group defined for Swift
      # Additional disks can be added if availiable
      device-groups:
        - name: swiftobj
          devices:
            - name: /dev/sdb
            - name: /dev/sdc
          consumer:
            name: swift
            attrs:
              rings:
                - account
                - container
                - object-0</codeblock>
      <p>Here is the sample file for <codeph>disks_vsa.yml</codeph> included with the install:</p>
      <codeblock>
---
  product:
    version: 2

  disk-models:
  - name: DISK_SET_VSA
    # Disk model to be used for VSA nodes
    # /dev/sda_root is used as a Volume Group for /, /var/log, and /var/crash
    # sda_root is a templated value to align with whatever partition is really used
    # This value is checked in os config and replaced by the partition actually used
    # on sda e.g. sda1 or sda5
    volume-groups:
    # The policy is not to consume 100% of the space of each volume group.
    # 5% should be left free for snapshots and to allow for some flexibility.
      - name: hlm-vg
        physical-volumes:
          - /dev/sda_root
        logical-volumes:
          - name: root
            size: 30%
            fstype: ext4
            mount: /
          - name: log
            size: 45%
            mount: /var/log
            fstype: ext4
            mkfs-opts: -O large_file
          - name: crash
            size: 20%
            mount: /var/crash
            fstype: ext4
            mkfs-opts: -O large_file
         consumer:
           name: os
        
     # Disks to be used by VSA
     # Additional disks can be added if availiable
     device_groups:
       - name: vsa-data
         consumer:
           name: vsa
           usage: data
         devices:
           - name: /dev/sdc
           - name: vsa-cache
         consumer:
           name: vsa
           usage: adaptive-optimization
         devices:
           - name: /dev/sdb</codeblock>
    </section>
    <section id="4.7_InterfaceModels"><title>Interface Models</title>
      <p>An interface model has the following attributes:</p>
      <p>
        <table frame="all" rowsep="1" colsep="1" id="interface_model">
          <tgroup cols="2">
            <colspec colname="c1" colnum="1"/>
            <colspec colname="c2" colnum="2"/>
            <thead>
              <row>
                <entry>Key</entry>
                <entry>Value Description</entry>
              </row>
            </thead>
            <tbody>
              <row>
                <entry>name</entry>
                <entry>A user defined name for the interface model</entry>
              </row>
              <row>
                <entry>network-interfaces</entry>
                <entry>A list of network interface definitions</entry>
              </row>
            </tbody>
          </tgroup>
        </table>
      </p>
      <p>A network interface is an interface model that has the following attributes:</p>
      <p>
        <table frame="all" rowsep="1" colsep="1" id="table_pwy_j4p_jt">
          <tgroup cols="2">
            <colspec colname="c1" colnum="1"/>
            <colspec colname="c2" colnum="2"/>
            <thead>
              <row>
                <entry>Key</entry>
                <entry>Value Description</entry>
              </row>
            </thead>
            <tbody>
              <row>
                <entry>name</entry>
                <entry>A user defined name for the interface</entry>
              </row>
              <row>
                <entry>device</entry>
                <entry>
                  <p>A dictionary containing the network device name as seen on the associated
                    server. If a NIC mapping is defined for the server, the name may be the
                    logical-name specified by the mapping. When configuring a bond, this is used as
                    the bond device name.</p></entry>
              </row>
              <row>
                <entry>bond-data (optional)</entry>
                <entry>Used to define a bond. See the <codeph>bond-data</codeph> section below for
                  details.</entry>
              </row>
              <row>
                <entry>network-groups</entry>
                <entry>A list of one or more network groups containing networks that will operate on
                  this interface.</entry>
              </row>
            </tbody>
          </tgroup>
        </table>
      </p>
      <p>A <codeph>bond-data</codeph> definition is used to configure a bond device, and consists of
        the following attributes:</p>
      <p>
        <table frame="all" rowsep="1" colsep="1" id="table_oft_z4p_jt">
          <tgroup cols="2">
            <colspec colname="c1" colnum="1"/>
            <colspec colname="c2" colnum="2"/>
            <thead>
              <row>
                <entry>Key</entry>
                <entry>Value Descriptions</entry>
              </row>
            </thead>
            <tbody>
              <row>
                <entry>provider</entry>
                <entry>Identifies the software used to instantiate the bond device. The only
                  supported value is “linux”, which uses the linux bonding driver. Other providers
                  may be supported in a future release.</entry>
              </row>
              <row>
                <entry>devices</entry>
                <entry>A dictionary containing network device names used to form the bond. If a NIC
                  mapping is defined for the server, a name may be the logical-name specified by the
                  mapping.</entry>
              </row>
              <row>
                <entry>options</entry>
                <entry>A dictionary containing bond configuration options. The note below describes
                  options for the “<i>linux”</i> provider.</entry>
              </row>
            </tbody>
          </tgroup>
        </table>
      </p>
      <p>The linux bonding driver supports a large number of parameters that control the operation
        of the bond, as described in the <xref
          href="https://www.kernel.org/doc/Documentation/networking/bonding.txt" scope="external"
          format="html">Linux Ethernet Bonding Driver HOWTO</xref> document. The parameter names and
        values may be specified as key-value pairs in the <codeph>options</codeph> section of
          <codeph>bond-data</codeph>.</p>
      <p>Here is the sample file included with the install:</p>
      <codeblock>
---
  product:
    version: 2

  interface-models:
     # These examples uses eth3 and eth4 as a bonded
     # pair for all networks on all three server roles
     #
     # Edit the device names and bond options
     # to match your environment
     #
   - name: INTERFACE_SET_CONTROLLER
     network-interfaces:
       - name: BOND0
         device:
           name: bond0
         bond-data:
           options:
             bond-mode: active-backup
             bond-miimon: 200
           provider: linux
           devices:
             - name: eth3
             - name: eth4
         network-groups:
           - EXTERNAL_API
           - EXTERNAL_VM
           - GUEST
           - MGMT
        
   - name: INTERFACE_SET_COMPUTE
     network-interfaces:
       - name: BOND0
         device:
           name: bond0
           bond-data:
             options:
               bond-mode: active-backup
               bond-miimon: 200
           provider: linux
           devices:
              - name: eth3
              - name: eth4
          network-groups:
              - EXTERNAL_VM
              - GUEST
              - MGMT
        
    - name: INTERFACE_SET_VSA
      network-interfaces:
        - name: BOND0
          device:
             name: bond0
             bond-data:
               options:
                  bond-mode: active-backup
                  bond-miimon: 200
               provider: linux
               devices:
                  - name: eth3
                  - name: eth4
             network-groups:
               - MGMT</codeblock>
    </section>
    <section id="network_groups.yml"><title>network_groups.yml</title>
      <p>The path to this file is:</p>
      <p><codeblock>~/helion/my_cloud/definition/data/network_groups.yml</codeblock></p>
      <p>A network group definition has the following attributes:</p>
      <p>
        <table frame="all" rowsep="1" colsep="1" id="table_jp5_jlt_jt">
          <tgroup cols="2">
            <colspec colname="c1" colnum="1"/>
            <colspec colname="c2" colnum="2"/>
            <thead>
              <row>
                <entry>Key</entry>
                <entry>Value Description</entry>
              </row>
            </thead>
            <tbody>
              <row>
                <entry>name</entry>
                <entry>A user defined name for the network group. The name is used to make
                  references from other configuration files.</entry>
              </row>
              <row>
                <entry>component-endpoints (optional)</entry>
                <entry>The list of service components that will bind to, or need direct access to
                  networks in this network group.</entry>
              </row>
              <row>
                <entry>hostname (optional)</entry>
                <entry>If set to true, the name of the address associated with a network in this
                  group will be used to set the hostname of the server.</entry>
              </row>
              <row>
                <entry>hostname-suffix (optional)</entry>
                <entry>If supplied, this string will be used in the name generation. If not
                  specified, the name of the network group will be used.</entry>
              </row>
              <row>
                <entry>load-balancers (optional)</entry>
                <entry>A list of load balancers to be configured on networks in this network group.
                  Because load balances need a VIP any network group that contains a load balance
                  can only have one network associated with it.</entry>
              </row>
              <row>
                <entry>routes (optional)</entry>
                <entry><p>A list of network groups that networks in this group provide access to via
                    their gateway. This can include “default” to define the default route. </p> A
                  network group with no services attached to it can be used to define routes to
                  external networks.</entry>
              </row>
              <row>
                <entry>tags (optional)</entry>
                <entry>A list of network tags.</entry>
              </row>
            </tbody>
          </tgroup>
        </table>
      </p>
      <p>A load balancer definition has the following attributes:</p>
      <p>
        <table frame="all" rowsep="1" colsep="1" id="table_gv1_wlt_jt">
          <tgroup cols="2">
            <colspec colname="c1" colnum="1"/>
            <colspec colname="c2" colnum="2"/>
            <thead>
              <row>
                <entry>Key</entry>
                <entry>Value Description</entry>
              </row>
            </thead>
            <tbody>
              <row>
                <entry>name</entry>
                <entry>A user defined name for the load balancer.</entry>
              </row>
              <row>
                <entry>provider</entry>
                <entry>The service component that implements the load balancer. Currently only
                  “ip-cluster” (haproxy) is supported. Future releases will provide support for
                  external load balancers.</entry>
              </row>
              <row>
                <entry>roles</entry>
                <entry>The list of endpoint roles that this load balancer provides (see below).
                  Valid roles are “public”, “internal”, and “admin”. To ensure separation of public
                  cannot be combined with internal/admin. See 3.8.1.1 for an example of how the role
                  provides endpoint separation</entry>
              </row>
              <row>
                <entry>components (optional)</entry>
                <entry>The list of service components the load balancer provides an unencrypted VIP
                  for.</entry>
              </row>
              <row>
                <entry>tls-components (optional)</entry>
                <entry>The list of service components the load balancer provides TLS terminated VIP
                  for.</entry>
              </row>
              <row>
                <entry>external-name (optional)</entry>
                <entry>The name to be registered in Keystone for the publicURL. If not specified the
                  IP address of the VIP will be registered.</entry>
              </row>
              <row>
                <entry>cert-file (optional)</entry>
                <entry>The name of the certificate file to be used for public endpoints.</entry>
              </row>
            </tbody>
          </tgroup>
        </table>
      </p>
      <p>Helion OpenStack supports a small number of network tags which may be used to convey
        information between the input model and the service components. A network tag consists
        minimally of a tag name; some network tags have additional attributes, described below:</p>
      <p><b>neutron.networks.vxlan</b></p>
      <p>
        <table frame="all" rowsep="1" colsep="1" id="neutron_networks_vxlan">
          <tgroup cols="2">
            <colspec colname="c1" colnum="1"/>
            <colspec colname="c2" colnum="2"/>
            <thead>
              <row>
                <entry>Tag</entry>
                <entry>Value Description</entry>
              </row>
            </thead>
            <tbody>
              <row>
                <entry>neutron.networks.vxlan</entry>
                <entry>This tag causes Neutron to be configured to use VxLAN as the underlay for
                  tenant networks. The associated network group will carry the VxLAN
                  traffic.</entry>
              </row>
              <row>
                <entry>tenant-vxlan-id-range (optional)</entry>
                <entry>Used to specify the VxLAN identifier range in the format
                  “&lt;min-id&gt;:&lt;max-id&gt;”. The default range is “1001:65535”. Enclose the
                  range in quotation marks.</entry>
              </row>
            </tbody>
          </tgroup>
        </table>
      </p>
      <p>Example using the default ID range:
        <codeblock>
tags:
  - neutron.networks.vxlan</codeblock></p>
      <p>Example using a user-defined ID range:
        <codeblock>
tags:
  - neutron.networks.vxlan:
      tenant-vxlan-id-range: “1:20000”</codeblock></p>
      <p><b>neutron.networks.vlan</b></p>
      <p>
        <table frame="all" rowsep="1" colsep="1" id="neutron_networks_vlan">
          <tgroup cols="2">
            <colspec colname="c1" colnum="1"/>
            <colspec colname="c2" colnum="2"/>
            <thead>
              <row>
                <entry>Tag</entry>
                <entry>Value Description</entry>
              </row>
            </thead>
            <tbody>
              <row>
                <entry>neutron.networks.vlan</entry>
                <entry>
                  <p>This tag causes Neutron to be configured for provider VLAN networks, and
                    optionally be configured to use VLAN as the underlay for tenant networks. The
                    associated network group will carry the VLAN traffic. This tag can be specified
                    on multiple network groups.</p>
                  <p>NOTE: this tag does not cause any Neutron networks to be created, that must be
                    done in Neutron after the cloud is deployed.</p></entry>
              </row>
              <row>
                <entry>provider-physical-network</entry>
                <entry>The provider network name. This is the name to be used in the Neutron API for
                  the <i>provider:physical_network</i> parameter of network objects.</entry>
              </row>
              <row>
                <entry>tenant-vlan-id-range (optional)</entry>
                <entry>This attribute causes Neutron to use VLAN for tenant networks; omit this
                  attribute if you are using provider VLANs only. It specifies the VLAN ID range for
                  tenant networks, in the format “&lt;min-id&gt;:&lt;max-id&gt;”. Enclose the range
                  in quotation marks.</entry>
              </row>
            </tbody>
          </tgroup>
        </table>
      </p>
      <p>Example using a provider vlan only (may be used with tenant VxLAN:
        <codeblock>
tags:
  - neutron.networks.vlan:
      provider-physical-network: physnet1</codeblock></p>
      <p>Example using a tenant and provider VLAN:
        <codeblock>
tags:
  - neutron.networks.vlan:
      provider-physical-network: physnet1
      tenant-vxlan-id-range: “100:200”</codeblock></p>
      <p><b>neutron.networks.flat</b></p>
      <p>
        <table frame="all" rowsep="1" colsep="1" id="neutron_networks_flat">
          <tgroup cols="2">
            <colspec colname="c1" colnum="1"/>
            <colspec colname="c2" colnum="2"/>
            <thead>
              <row>
                <entry>Tag</entry>
                <entry>Value Description</entry>
              </row>
            </thead>
            <tbody>
              <row>
                <entry>neutron.networks.flat</entry>
                <entry>
                  <p>This tag causes Neutron to be configured for provider flat networks. The
                    associated network group will carry the traffic. This tag can be specified on
                    multiple network groups.</p>
                  <p>NOTE: this tag does not cause any Neutron networks to be created, that must be
                    done in Neutron after the cloud is deployed.</p>
                </entry>
              </row>
              <row>
                <entry>provider-physical-network</entry>
                <entry>The provider network name. This is the name to be used in the Neutron API for
                  the <i>provider:physical_network</i> parameter of network objects. When specified
                  on multiple network groups, the name must be unique for each network
                  group.</entry>
              </row>
            </tbody>
          </tgroup>
        </table>
      </p>
      <p>Example using a provider flat network:
        <codeblock>
tags:
  - neutron.networks.flat:
       provider-physical-network: flatnet1</codeblock></p>
      <p><b>neutron.l3_agent.external_network_bridge</b></p>
      <p>
        <table frame="all" rowsep="1" colsep="1" id="neutron_l3_agent">
          <tgroup cols="2">
            <colspec colname="c1" colnum="1"/>
            <colspec colname="c2" colnum="2"/>
            <thead>
              <row>
                <entry>Tag</entry>
                <entry>Value Description</entry>
              </row>
            </thead>
            <tbody>
              <row>
                <entry>neutron.l3_agent.external_network_bridge</entry>
                <entry>
                  <p>This tag causes the Neutron L3 Agent to be configured to use the associated
                    network group as the Neutron external network for floating IP addresses. A CIDR
                      <b>should not</b> be defined for the associated physical network, as that will
                    cause addresses from that network to be configured in the hypervisor. When this
                    tag is used, provider networks cannot be used as external networks.</p>
                  <p>NOTE: this tag does not cause a Neutron external networks to be created, that
                    must be done in Neutron after the cloud is deployed.</p>
                </entry>
              </row>
            </tbody>
          </tgroup>
        </table>
      </p>
      <p>Example using neutron.l3_agent.external_network_bridge:
        <codeblock>
tags:
  - neutron.l3_agent.external_network_bridge</codeblock></p>
      <p><b>vsa.iscsi</b></p>
      <p>
        <table frame="all" rowsep="1" colsep="1" id="vsa_iscsi">
          <tgroup cols="2">
            <colspec colname="c1" colnum="1"/>
            <colspec colname="c2" colnum="2"/>
            <thead>
              <row>
                <entry>Tag</entry>
                <entry>Value Description</entry>
              </row>
            </thead>
            <tbody>
              <row>
                <entry>vsa.iscsi (optional)</entry>
                <entry>
                  <p>This tag is used to identify the network group to which the VSA virtual machine
                    will be attached. This tag is required when using VSA as a block storage back
                    end for Cinder.</p>
                </entry>
              </row>
            </tbody>
          </tgroup>
        </table>
      </p>
      <p>Example using neutron.l3_agent.external_network_bridge:
        <codeblock>
tags:
  - vsa.iscsi</codeblock></p>
      <p><b>nova.compute.iscsi</b></p>
      <p>
        <table frame="all" rowsep="1" colsep="1" id="nova_compute_iscsi">
          <tgroup cols="2">
            <colspec colname="c1" colnum="1"/>
            <colspec colname="c2" colnum="2"/>
            <thead>
              <row>
                <entry>Tag</entry>
                <entry>Value Description</entry>
              </row>
            </thead>
            <tbody>
              <row>
                <entry>nova.compute.iscsi (optional)</entry>
                <entry>
                  <p>This tag is used to identify the network group for nova-compute to use for
                    ISCSI communications with the Cinder block storage back end. This tag is
                    required when using an ISCSI block storage back end for Cinder.</p>
                </entry>
              </row>
            </tbody>
          </tgroup>
        </table>
      </p>
      <p>Example using neutron.l3_agent.external_network_bridge:
        <codeblock>
tags:
  - nova.compute.iscsi</codeblock></p>
      <p>Here is the sample file included with the install:</p>
      <codeblock>
---
  product:
    version: 2
        
  network-groups:
        
    #
    # External API
    #
    # This is the network group that users will use to
    # access the public API endpoints of your cloud
    #
    - name: EXTERNAL_API
      hostname-suffix: extapi
        
      load-balancers:
        - provider: ip-cluster
        name: extlb
        # If external-name is set then public urls in keystone
        # will use this name instead of the IP address
        #external-name: mycloud.org
        components:
          - default
        roles:
          - public
        
     #
     # External VM
     #
     # This is the network group that will be used to provide
     # external access to VMs (via floating IP Addresses)
     #
     - name: EXTERNAL_VM
       tags:
        - neutron.l3_agent.external_network_bridge
        
        
     #
     # GUEST
     #
     # This is the network group that will be used to provide
     # private networks to VMs
     #
     - name: GUEST
        hostname-suffix: guest
        tags:
          - neutron.networks.vxlan
        
        
     #
     # Management
     #
     # This is the network group that will be used to for
     # management traffic within the cloud.
     #
     # The interface used by this group will be presented
     # to Neutron as physnet1, and used by provider VLANS
     #
     # In this example this group is also used for ISCSI
     # traffic between VMs and the VSA block storage
     #
     - name: MGMT
        hostname-suffix: mgmt
        hostname: true
        
        component-endpoints:
          - default
        
        routes:
          - default
        
        load-balancers:
          - provider: ip-cluster
            name: lb
            components:
              - default
            roles:
              - internal
              - admin
        
        tags:
          - neutron.networks.vlan:
             provider-physical-network: physnet1
          - vsa.iscsi
          - nova.compute.iscsi</codeblock>
    </section>
    <section id="networks.yml"><title>networks.yml</title>
      <p>The path to this file is:</p>
      <p><codeblock>~/helion/my_cloud/definition/data/networks.yml</codeblock></p>
      <p>The key values to enter into this document are described in the table below:</p>
      <p>
        <table frame="all" rowsep="1" colsep="1" id="table_vd3_lzt_jt">
          <tgroup cols="2">
            <colspec colname="c1" colnum="1"/>
            <colspec colname="c2" colnum="2"/>
            <thead>
              <row>
                <entry>Key</entry>
                <entry>Value Description</entry>
              </row>
            </thead>
            <tbody>
              <row>
                <entry>name</entry>
                <entry>The name of this network. The network <i>name</i> may be used in a
                  server-group definition to specify a particular network from within a network
                  group to be associated with a set of servers.</entry>
              </row>
              <row>
                <entry>network-group</entry>
                <entry>The name of the associated network group.</entry>
              </row>
              <row>
                <entry>vlanid (optional)</entry>
                <entry>The IEEE 802.1Q VLAN Identifier, a value in the range 1 through 4094. A
                    <i>vlanid</i> must be specified when <i>tagged-vlan</i> is true.</entry>
              </row>
              <row>
                <entry>tagged-vlan (optional)</entry>
                <entry>May be set to “true” or “false”. If true, packets for this network carry the
                    <i>vlanid </i>in the packet header; such packets are referred to as VLAN-tagged
                  frames in IEEE 802.1Q.</entry>
              </row>
              <row>
                <entry>cidr (optional)</entry>
                <entry>The IP subnet associated with this network.</entry>
              </row>
              <row>
                <entry>start-address (optional)</entry>
                <entry>An IP address within the <i>cidr</i> which will be used as the start of the
                  range of IP addresses from which server addresses may be allocated. The default
                  value is the first host address within the <i>cidr </i>(e.g. the .1
                  address).</entry>
              </row>
              <row>
                <entry>end-address (optional)</entry>
                <entry>An IP address within the <i>cidr</i> which will be used as the end of the
                  range of IP addresses from which server addresses may be allocated. The default
                  value is the last host address within the <i>cidr </i>(e.g. the .254 address of a
                  /24).</entry>
              </row>
              <row>
                <entry>gateway-ip (optional)</entry>
                <entry>The IP address of the default gateway.</entry>
              </row>
            </tbody>
          </tgroup>
        </table>
      </p>
      <p>Here is the sample file included with the install:</p>
      <codeblock>
---
  product:
    version: 2
        
  networks:
    #
    # This example uses the following networks
    #
    # Network       CIDR             VLAN
    # -------       ----             ----
    # External API  10.0.1.0/24      101 (tagged)
    # External VM   see note 1       102 (tagged)
    # Guest         10.1.1.0/24      103 (tagged)
    # Mgmt          10.2.1.0/24      100 (untagged)
    #
    # Notes:
    # 1. Defined as part of Neutron configuration
    #
    # Modify these values to match your environment
    #
    - name: NET_EXTERNAL_API
      vlanid: 101
      tagged-vlan: true
      cidr: 10.0.1.0/24
      gateway-ip: 10.0.1.1
      network-group: EXTERNAL_API
     #start-address: 10.0.1.4
        
    - name: NET_EXTERNAL_VM
      vlanid: 102
      tagged-vlan: true
      network-group: EXTERNAL_VM
        
    - name: NET_GUEST
      vlanid: 103
      tagged-vlan: true
      cidr: 10.1.1.0/24
      gateway-ip: 10.1.1.1
      network-group: GUEST
        
    - name: NET_MGMT
      vlanid: 100
      tagged-vlan: false
      cidr: 10.2.1.0/24
      gateway-ip: 10.2.1.1
      network-group: MGMT</codeblock>
    </section>
    <section id="nic_mappings.yml"><title>nic_mappings.yml</title>
      <p>NIC mappings are used to ensure that the network device name used by the operating system
        always maps to the same physical device. A NIC mapping is associated to a server in the
          <codeph>servers.yml</codeph> definition file.</p>
      <p>Your NIC adapter PCI Bus address information that will go into your nic_mappings file was
        also included in the <xref href="bare_install_kvm_cl.dita#preinstall_checklist/nic_mappings"
          type="table">pre-installation checklist</xref> steps.</p>
      <p>The path to this file is:</p>
      <p><codeblock>~/helion/my_cloud/definition/data/nic_mappings.yml</codeblock></p>
      <p>Each entry in the <codeph>nic-mappings</codeph> list has the following attributes:</p>
      <p>
        <table frame="all" rowsep="1" colsep="1" id="nic_mappings">
          <tgroup cols="2">
            <colspec colname="c1" colnum="1"/>
            <colspec colname="c2" colnum="2"/>
            <thead>
              <row>
                <entry>Key</entry>
                <entry>Value Description</entry>
              </row>
            </thead>
            <tbody>
              <row>
                <entry>name</entry>
                <entry>A user-specified name for the mapping. This name may be used in a server
                  definition to apply the mapping to that server.</entry>
              </row>
              <row>
                <entry>physical-ports</entry>
                <entry>A list containing device name to address mapping information.</entry>
              </row>
            </tbody>
          </tgroup>
        </table>
      </p>
      <p>Each entry in the <codeph>physical-ports</codeph> list has the following attributes:</p>
      <p>
        <table frame="all" rowsep="1" colsep="1" id="physical_ports">
          <tgroup cols="2">
            <colspec colname="c1" colnum="1"/>
            <colspec colname="c2" colnum="2"/>
            <thead>
              <row>
                <entry>Key</entry>
                <entry>Value Description</entry>
              </row>
            </thead>
            <tbody>
              <row>
                <entry>logical-name</entry>
                <entry>The network device name that will be associated with the device at the
                  specified <i>bus-address</i>. The logical-name specified here can be used as a
                  device name in network interface model definitions.</entry>
              </row>
              <row>
                <entry>type</entry>
                <entry>The type of port. The current implementation supports only “simple-port”;
                  other port types may be added in a future release.</entry>
              </row>
              <row>
                <entry>bus-address</entry>
                <entry>PCI bus address of the port. Enclose the bus address in quotation marks so
                  yaml does not misinterpret the embedded colon (:) characters.</entry>
              </row>
            </tbody>
          </tgroup>
        </table>
      </p>
      <p>Here is the sample file included with the install:</p>
      <codeblock>
---
  product:
    version: 2
        
  # nic-mappings are used to ensure that the device name used by the
  # operating system always maps to the same physical device.
  # A nic-mapping is associated to a server in the server definition.
  # The logical-name specified here can be used as a device name in
  # the network interface-models definitions.
  #
  # - name               user-defined name for each mapping
  #   physical-ports     list of ports for this mapping
  #     - logical-name   device name to be used by the operating system
  #       type           physical port type
  #       bus-address    bus address of the physical device
  #
  # Notes:
  # - enclose the bus address in quotation marks so yaml does not
  #   misinterpret the embedded colon (:) characters
  # - simple-port is the only currently supported port type

  nic-mappings:
        
    - name: HP_DL360_4PORT
      physical-ports:
        - logical-name: eth1
          type: simple-port
          bus-address: "0000:07:00.0"
        
        - logical-name: eth2
          type: simple-port
          bus-address: "0000:08:00.0"
        
        - logical-name: eth3
          type: simple-port
          bus-address: "0000:09:00.0"
        
        - logical-name: eth4
          type: simple-port
          bus-address: "0000:0a:00.0"
        
    - name: MY_2PORT_SERVER
      physical-ports:
        - logical-name: eth3
          type: simple-port
          bus-address: "0000:04:00.0"
        
        - logical-name: eth4
          type: simple-port
          bus-address: "0000:04:00.1"</codeblock>
      <note>You can remove the NIC information examples that do not pertain to your environment from
        this file.</note>
    </section>

  </conbody>
</concept>
