<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="install_kvm_vsa">
  <title>HP Helion <tm tmtype="reg">OpenStack</tm> 2.0: Installation for Helion Entry Scale Cloud
    with VSA</title>
  <body>
    <section id="important_notes"><title>Important Notes</title>
      <ul>
        <li>If you are looking for information about when to use the GUI installer and when to use
          the CLI, see the <xref href="installation_overview.dita#install_overview">Installation
            Overview</xref>.</li>
        <li>We have put together a <xref href="preinstall_checklist.dita">Pre-Installation
            Checklist</xref> that should help with the recommended pre-installation tasks.</li>
        <li>Ensure the <xref href="hardware.dita">minimum hardware requirements</xref> are met in
          your environment.</li>
        <li>There is no longer a requirement to use a dedicated deployer node. See the note in the
            <xref href="releasenotes.dita">Release Notes</xref> as well as the configuration section
          in the install steps below for details.</li>
        <li>There is no requirement to have a dedicated network for OS-install and system
          deployment. More information can be found on the <xref href="example_configs.dita"
            >Supported Configuration</xref> page.</li>
        <li>If you run into issues during installation, we have put together a list of <xref
            href="installation_troubleshooting.dita">Troubleshooting Steps</xref> you can
          reference.</li>
        <li>Read over the page about the <xref href="input_model.dita">Helion OpenStack 2.0 Input
            Model</xref> to learn about the configuration options for your cloud.</li>
        <li>Make sure <xref href="administration/wipe_disks.dita">all disks on the system(s) are
            wiped</xref> before you begin the install. (For Swift, refer to <xref
            href="objectstorage/allocating_disk_drives.dita#allocating-disk-drives/requirement-disk-device"
            >Swift Requirements for Device Group Drives</xref>)</li>
        <li>The <codeph>/dev/sda</codeph> disk on your systems will be used for the operating
          system.</li>
        <li>Both HP Linux for HP Helion OpenStack and HP Helion OpenStack are part of the ISO you
          will download from the <xref
            href="https://helion.hpwsportal.com/catalog.html#/Category/%7B%22categoryId%22%3A10311%7D/Show"
            format="html" scope="external">Helion Downloads</xref> page. There is only one download;
          it includes both the operating system and the HP Helion OpenStack installer in one
          ISO.</li>
        <li>All machines of a given role should be the same configuration.</li>
        <li>The machine hosting the deployer and all baremetal systems must be connected to a
          management network. Nodes on this management network must be able to reach the iLO
          subsystem of each baremetal system to enable host reboots as part of the install process.
          The HP Helion OpenStack architecture requires that the IPMI network is a separate network
          and that a route exists from the management network to the IPMI network for iLO
          access.</li>
      </ul>
    </section>
    <section id="prereqs">
      <title>Before You Start</title>
      <note type="important">We have put together a <xref href="preinstall_checklist.dita"
          >Pre-Installation Checklist</xref> that should help with the recommended pre-installation
        tasks.</note>
      <p>Prepare your baremetal hardware, as follows, on all nodes:</p>
      <ul>
        <li>Set up the iLO Advanced license in the iLO configuration. Make sure the iLO user has
          admin privileges.</li>
        <li> HP Helion OpenStack 2.0 Beta 2 will detect and use the "BIOS" mode you have selected
          for each node; UEFI or legacy BIOS. UEFI support is new in Beta 2.</li>
        <li>Ensure that the network interface to be used for PXE installation has PXE enabled.</li>
        <li>Ensure that the other network interfaces have PXE disabled.</li>
        <li>Ensure that any logical drives (LUN) for the servers you will be using are created to
          meet the disk requirements outlined in the <xref href="hardware.dita">Minimum Hardware
            Requirements</xref>.</li>
      </ul>
      
      <p>
        <ul id="ul_pfl_bbz_ht">
          <li>HP Helion Cloud must be successfully deployed. It should be up and running.</li>
          <li>Remember to note down the IP address of VSA VM and VSA Cluster from
            <codeph>~/scratch/ansible/last/my_cloud/stage/info/net_info.yml</codeph> file, which
            is generated by configuration
            processor.<!-- (It is recommended that you write down the IP address so that it can be easily accessible).--></li>
        </ul>
      </p>
      <p>You can deploy VSA with adoptive optimization (AO) and without adoptive optimization. The
        process to deploy VSA with AO and without AO remains similar except the change in the disk
        input model. For more detailed information, refer to <xref
          href="#topic_gmc_dgk_jt/deploy-vsa-with-ao-without-ao" format="dita">VSA with AO and
          without AO</xref>.</p>
    </section>
    <section id="install_deployer">
      <title>Set up the Deployer</title>
      <p>You can use a dedicated deployer node or you can run these instructions on your first
        controller node. There is only one difference in the steps and that is notated in the <xref
          href="#install_kvm/configuration">Configure Your Environment</xref> section.</p>
      <ol>
        <li>Download the HP Helion OpenStack 2.0.0 product from the <xref
            href="https://helion.hpwsportal.com/catalog.html#/Category/%7B%22categoryId%22%3A10389%7D/Show"
            format="html" scope="external">Helion Downloads</xref> page after signing in.</li>
        <li>Boot your deployer from the ISO contained in the download. Insert the CD ROM in the
          Virtual Media drive on the iLO.</li>
        <li>Enter "install" to start installation. <note>"install" is all lower case</note></li>
        <li>Select the language. Note that only the English language selection is currently
          supported.</li>
        <li>Select the location.</li>
        <li>Select the keyboard layout.</li>
        <li>Select the primary network interface, if prompted:<ul>
            <li>Assign IP address, netmask, default gateway</li>
          </ul></li>
        <li>Create new account:<ul>
            <li>Enter a username.</li>
            <li>Enter a password.</li>
            <li>Enter time zone if prompted to do so.</li>
          </ul>
        </li>
        <li>Ensure your deployer has a valid DNS nameserver in the <codeph>/etc/resolv.conf</codeph>
          file</li>
        <li>Install and configure NTP for your environment <ul>
            <li>Ensure your NTP server(s) is placed into your <codeph>/etc/ntp.conf</codeph>
              file</li>
          </ul></li>
      </ol>
      <p>At the end of this section you should have a node set up with hLinux on it.</p>
    </section>
    <section id="configure_deployer">
      <title>Configure and Run the Deployer</title>
      <note>It's critical that you don't run as root. Run as the user you just created (or stack if
        you left the default of "stack"), but do not run as root.</note>
      <ol>
        <li>Log in to the node you setup in the previous steps as the user you created during the
          setup phase, and mount the install media at <codeph>/media/cdrom</codeph>. It may be
          necessary to use <codeph>wget</codeph> or another file transfer method to transfer the
          install media to the deployer before completing this step. Here is the command to mount
          the media:
          <codeblock>sudo mount Helion-OpenStack-2.0.0-b.2-Beta2.iso /media/cdrom</codeblock></li>
        <li>Unpack the tarball that is in the <codeph>/media/cdrom/hos2.0.0/</codeph> directory:
          <codeblock>tar zxvf /media/cdrom/hos-2.0.0/hos-2.0.0-b.2-20150920T130131Z.tgz</codeblock></li>
        <li>Run the following included script:
          <codeblock>~/hos-2.0.0-b.2/hos-init.bash</codeblock></li>
        <li><p>You will be prompted to enter an optional SSH passphrase when running
              <codeph>hos-init.bash</codeph>. This passphrase is used to protect the key used by
            Ansible when connecting to its client nodes. If you do not want to use a passphrase then
            just press return at the prompt.</p>
          <p>For automated installation (e.g. CI) it is possible to disable SSH passphrase prompting
            by setting the <codeph>HOS_INIT_AUTO</codeph> environment variable before running
              <codeph>hos-init.bash</codeph>, like this:</p>
          <codeblock>export HOST_INIT_AUTO=y</codeblock></li>
      </ol>
      <p>If you have protected the SSH key with a passphrase then execute the following commands to
        avoid having to enter the passphrase on every attempt by Ansible to connect to its client
        nodes: <codeblock>eval $(ssh-agent)
ssh-add ~/.ssh/id_rsa</codeblock></p>
      <p>At the end of this section you should have a local directory structure, as described
        below:</p>
      <codeblock>
helion/                        Top level directory
helion/examples/               Directory contains the config input files of the example clouds
helion/my_cloud/definition/    Directory contains the config input files
helion/my_cloud/config/        Directory contains .j2 files which are symlinks to the /hos/ansible directory
helion/hos/                    Directory contains files used by the installer
</codeblock>
    </section>
    <section id="configuration">
      <title>Configure Your Environment</title>
      <ol>
        <li>Setup your configuration files, as follows: <ol>
            <li>See the sample set of configuration files in the
                <codeph>~/helion/examples/entry-scale-with-vsa</codeph> directory. The accompanying
              README.md file explains the contents of each of the configuration files.</li>
            <li>Copy the example configuration files into the required setup directory and edit them
              to contain the details of your environment:
              <codeblock>cp -r ~/helion/examples/entry-scale-kvm-vsa/* ~/helion/my_cloud/definition/</codeblock></li>
            <li>Begin inputting your environment information into the configuration files in the
                <codeph>~/helion/my_cloud/definition</codeph> directory. Full details of how to do
              this can be found here: <xref href="input_model.dita">Helion OpenStack 2.0 Input
                Model</xref>.
          <note type="important">If you chose to use your first controller node
            as your deployer, ensure that your <codeph>servers.yml</codeph> file contains the
            <codeph>is-deployer: true</codeph> notation in your controller options. If you are
            using a dedicated deployer node you can omit this. Here is an example snippet of a
            <codeph>servers.yml</codeph> file where a user is using their first controller
            node as their deployer:
            <codeblock># Controllers
  - id: controller1
    ip-addr: 192.168.10.3
    role: CONTROLLER-ROLE
    server-group: RACK1
    nic-mapping: HP-DL360-4PORT
    mac-addr: b2:72:8d:ac:7c:6f
    ilo-ip: 192.168.9.3
    ilo-password: password
    ilo-user: admin
    <b>is-deployer: true</b></codeblock></note></li>
            <li id="encrypt">[Optional] You can use the <codeph>hosencrypt.py</codeph> script to
              encrypt your iLo passwords. This script uses OpenSSL. <ol>
                <li>Change to the Ansible directory:
                  <codeblock>cd ~/helion/hos/ansible</codeblock></li>
                <li>Put the encrytion key into the following environment variable:
                  <codeblock>export HOS_USER_PASSWORD_ENCRYPT_KEY=&#60;encryption key></codeblock></li>
                <li>Run the python script below and follow the instructions. Enter a password that
                  you want to encrypt. <codeblock>hosencrypt.py</codeblock></li>
                <li>Take the string generated and place it in the <codeph>"ilo_password"</codeph>
                  field in your <codeph>~/helion/my_cloud/definition/data/servers.yml</codeph> file,
                  remembering to enclose it in quotes.</li>
                <li>Repeat the above for each server.</li>
              </ol>
              <note>Before you run any playbooks, remember that you need to export the encryption
                key in the following environment variable: <codeph>export
                  HOS_USER_PASSWORD_ENCRYPT_KEY=&#60;encryption key></codeph></note></li>
          </ol></li>
        <li>Commit your configuration to the <xref href="using_git.dita">local git repo</xref>, as
          follows: <codeblock>cd ~/helion/hos/ansible
git add -A
git commit -m "My config or other commit message"
</codeblock>
          <note type="important">This step needs to be repeated any time you make changes to your
            configuration files before you move onto the following steps. See <xref
              href="using_git.dita">Using Git for Configuration Management</xref> for more
            information.</note></li>
      </ol>
      <p>Then you need to run the following commands to complete your configuration. These commands
        also verify your configuration is correct.</p>
      <ol>
        <li>Run the following playbook which confirms that there is iLo connectivity for each of
          your nodes so that they are accessible to be re-imaged in a later step:
          <codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost bm-power-status.yml</codeblock></li>
        <li>Run the configuration processor, as follows:
          <codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
      </ol>
      <p>When you run the Configuration Processor you will be prompted for two passwords. Enter the
        first password to make the Configuration Processor encrypt its sensitive data, which is
        comprised of the random inter-service passwords that it generates and the Ansible group_vars
        and host_vars that it produces for subsequent deploy runs. You will need this key for
        subsequent Ansible deploy runs and subsequent Configuration Processor runs. If you wish to
        change an encryption password that you have already used when running the Configuration
        Processor then enter the new password at the second prompt, otherwise just press carriage
        return.</p>
      <p>For CI purposes you can specify the required passwords on the ansible command line. For
        example, the command below will disable encryption by the configuration processor
        <codeblock>ansible-playbook -i hosts/localhost config-processor-run.yml -e encrypt="" -e rekey=""</codeblock>
      </p>
      <p>If you receive an error during either of these steps then there is an issue with one or
        more of your configuration files. We recommend that you verify that all of the information
        in each of your configuration files is correct for your environment and then commit those
        changes to git using the instructions above.</p>
      <p>Before re-running the Configuration Processor after fixing an error you need to run the
        clean script detailed in the <xref
          href="installation_troubleshooting.dita#troubleshooting_installation/configproc"
          >Installation Troubleshooting</xref> document.</p>
    </section>
    <section id="deploy_cobbler">
      <title>Deploy Cobbler</title>
      <p>This phase of the install process takes the baremetal information that was provided in
          <codeph>servers.yml</codeph> and installs the Cobbler provisioning tool and loads this
        information into Cobbler. This sets each node to <codeph>netboot-enabled: true</codeph>.
        Each node will be automatically marked as <codeph>netboot-enabled: false</codeph> when it
        completes its operating system install successfully. Even if the node tries to PXE boot
        subsequently, Cobbler will not serve it. This is deliberate so that you can't reimage a live
        node by accident.</p>
      <p>The <codeph>cobbler-deploy.yml</codeph> playbook prompts for a password - this is the
        password to be able to log in to the nodes via their consoles after install. The username is
        the same as the user set up in the initial dialogue when installing the deployer from the
        iso, and is the same user that is running the cobbler-deploy play.</p>
      <ol>
        <li>Run the following command:
          <codeblock>export ANSIBLE_HOST_KEY_CHECKING=False</codeblock></li>
        <li>Run the following playbook to deploy Cobbler:
          <codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost cobbler-deploy.yml</codeblock></li>
        <li>The cobbler-deploy stage of the installation now prompts for a password. The entered
          value is used to set a user-defined password for subsequent node installs so that a user
          can gain console/terminal access to the nodes. For example, if a node maintenance is
          required or if the SSH keys are lost. The value is encrypted and placed in the kickstart
          for the node installs.<p>The password prompts look like this:</p>
          <codeblock>Enter the password that will be used to access provisioned nodes:
confirm Enter the password that will be used to access provisioned nodes:</codeblock>
          <p>Alternatively, you can also specify this password in the ansible playbook command like
            this:</p>
          <codeblock>ansible-playbook -i hosts/localhost cobbler-deploy.yml -e hlmuser_password="&lt;password>"</codeblock></li>
      </ol>
    </section>
    <section id="provision_nodes"><title>Provision the Nodes</title>
      <p>This phase of the install process goes through a number of distinct steps: <ol>
          <li>Powers down the nodes to be installed</li>
          <li>Sets the nodes hardware boot order so that the first option is a network boot.</li>
          <li>Powers on the nodes. (The nodes will then boot from the network and be installed using
            infrastructure set up in the previous phase)</li>
          <li>Waits for the nodes to power themselves down (this indicates a success install). This
            can take some time.</li>
          <li>Sets the boot order to hard disk and powers on the nodes.</li>
          <li>Waits for the nodes to be ssh-able and verifies that they have the signature
            expected.</li>
        </ol>
      </p>
      <p>The reimage command is:</p>
      <codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost bm-reimage.yml [-e nodelist=node1,node2,node3]</codeblock>
      <p>If a nodelist is not specified then the set of nodes in cobbler with
          <codeph>netboot-enabled: True</codeph> is selected. The playbook pauses at the start to
        give you a chance to review the set of nodes that it is targeting and to confirm that it's
        correct.</p>
      <p>You can use <codeph>sudo cobbler system report</codeph> which will give you a synopsis of
        the nodes' information including whether the <codeph>netboot-enabled</codeph> flag is
        set.</p>
    </section>
    <section id="deploy"><title>Deploy the Cloud</title>
      <ol>
        <li>Use the playbook below to create a deployment directory:
          <codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
        <li>Run the <codeph>site.yml</codeph> playbook using the following command. The <codeph>-e
            elasticsearch_cluster_name</codeph> switch is used to set a unique name for your
          elasticsearch cluster. This should be set to a value other than
            <codeph>elasticsearch</codeph>. <codeblock>cd ~/scratch/ansible/next/hos/ansible 
ansible-playbook -i hosts/verb_hosts site.yml -e elasticsearch_cluster_name=&#60;name></codeblock>
          <p>If you have used an encryption password when running the Configuration processor use
            the command below and enter the encryption password when prompted:
            <codeblock>ansible-playbook -i hosts/verb_hosts site.yml --ask-vault-pass</codeblock>
          </p>
          <note>The step above runs osconfig to configure the cloud and hlm-deploy, to deploy the
            cloud. Therefore, this step may run for quite some time while all the nodes are
            configured (perhaps 45 minutes or more, depending on the number of nodes, to a degree).
          </note>
        </li>
        <li>Verify that the network is working correctly. Ping each IP (excluding VSA-BLK and VIPs)
          from the /etc/hosts file from one of the controller nodes.</li>
      </ol>
    </section>
    <section>
      <title>Creating a StoreVirtual Cluster and adding it to a new Management Group Cluster using
        CMC</title>
      <p><b> Install (any) X Display Tool to Launch Centralized Management Console (CMC)</b></p>
      <p>You must configure X display tool to launch CMC. User can select <b>any</b> X display tool.
        In this section we are using <b>Xming</b> tool as an example to launch CMC. The following
        example provides the steps to install Xming and launch CMC.</p>
      <p><!--You must configure X display to launch CMC. Therefore, CMC is required to configure the HP StoreVirtual VSA nodes. 
        In this section we are referring to Xming as a X display tool. The display tool can vary from one user to another.--></p>
      <p><!--The follwowing steps provide an example on the configuration of Xming to launch CMC.--></p>
      <p>
        <!--In order to configure the HP StoreVirtual VSA nodes, you must first install the CMC. The following example 
          provides the steps for installaing CMC. The Xming is a tool we are referring in the example below-->
        <ol id="ol_alr_mdl_jt">
          <li>Install <b>Xming</b> on Windows Box from where you can access the deployer node.</li>
          <li>Select <b>Enable X11 forwarding</b> checkbox on the putty session for deployer node. </li>
          <li>SSH to first control plane node.<codeblock>ssh -X</codeblock></li>
          <li>Execute the following command:
            <codeblock>/opt/HP/StoreVirtual/UI/jre/bin/java -jar /opt/HP/StoreVirtual/UI/UI.jar</codeblock>CMC
            will be launch successfully.</li>
        </ol>
      </p>
      <!--<p><note type="important"><?oxy_custom_start type="oxy_content_highlight" color="255,255,0"?>Ensure that CMC is installed successfully and HP StoreVirtual VSA (one or more) is deployed in the same management network where CMC is installed.<?oxy_custom_end?></note></p>-->
      <p>Perform the following steps to create the cluster.<ol id="ol_yln_fhl_jt">
        <li>Open CMC.<p>By default, the CMC is configured to discover the StoreVirtual nodes in
          the subnet in which it is installed. You can manually add the nodes also.</p></li>
        <li>In the CMC UI, click <b>Find</b> and then click <b>Find Systems</b> from the left
          panel. Find Systems dialogue box is displayed.</li>
        <li>You can choose <b>Add</b> or <b>Find</b> option to search the system. Find option
          starts searching for the nodes in the same subnet as that of CMC. Add option displays an
          <b>Enter IP</b> pop-up box to enter the IP of the StoreVirtual system.</li>
        <li>In the CMC UI, click <b>Tasks</b> then point to <b>Management Group</b> and click<b>
          New Management Group</b>.</li>
        <li>In the <b>New Management Group Name</b> box, enter a name for the management group.
          For example: <codeph>hlm003-vsa-mg</codeph></li>
        <li>Click <b>Next</b> to display the Add Administrative User page.</li>
        <li>Enter the required information (username/password) for the Administrative User.<p>For
          example:</p><ul id="ul_cdk_xjl_jt">
            <li>username- stack </li>
            <li>password-stack</li>
          </ul></li>
        <li>Click <b>Next</b> to display the Management Group Time page. </li>
        <li>Add NTP server information and click <b>Next</b> to display the Domain Name Server
          Configuration page.</li>
        <li>Skip the DNS and SMTP sections.</li>
        <li>Click <b>Next</b> to display a Wizard in the Create a Cluster page.</li>
        <li>Select the cluster type as <i>Standard Cluster</i> from the displayed options and
          click <b>Next</b>.</li>
        <li>In the <b>Cluster Name</b> box, enter the name of the cluster and click <b>Next</b>.
          For example: <codeph>hlm003-vsa-cluster</codeph>.</li>
        <li>In the Add VIP and Subnet Mask pop-up box, enter the virtual IP and Subnet Mask of the
          cluster in the respective boxes and click <b>OK</b>. The details are displayed in a
          tabular format in the page. <note>The VIP IP can be found as the cluster IP value in the
            <codeph>~/scratch/ansible/next/my_cloud/stage/info/net_info.yml</codeph>
            file.</note></li>
        <li>Click <b>Next</b> to go to the next page.</li>
        <li>Select the checkbox displayed against <b>Skip Volume Creation</b>.</li>
        <li>Click <b>Next</b>.The Management Group and Cluster is created and displays in the Home
          page of CMC.<p>Wait till the cluster is displayed in the Home Page of CMC.</p></li>
      </ol></p>
    </section>
    <section>
      <title>Configure VSA as a Cinder Backend</title>
      <p>You must modify <codeph>cinder.conf.j2</codeph> to update VSA cluster.</p>
      <p>Perform the following steps to update the VSA cluster:<ol id="ol_dj2_qcz_ht">
        <li>Login to deployer node.</li>
        <li>Edit <codeph>~/helion/my_cloud/config/cinder/cinder.conf.j2</codeph> to add VSA data
          as
          follows:<codeblock># Configure the enabled backends
            enabled_backends=vsa-1</codeblock></li>
        <li>Copy VSA section, uncomment it, and modify the values in the
          <codeph>cinder.conf.j2</codeph> file as shown in the following
          example:<codeblock id="vssa-config">[vsa-1]
            hplefthand_password: &lt;password given during cluster creation>
            hplefthand_clustername: &lt;VSA cluster name>
            hplefthand_api_url: https://&lt;store virtual virtual ip>:8081/lhos
            hplefthand_username: &lt;username given during cluster creation>
            hplefthand_iscsi_chap_enabled: true
            volume_backend_name: &lt;volume backend name>
            volume_driver: cinder.volume.drivers.san.hp.hp_lefthand_iscsi.HPLeftHandISCSIDriver
            hplefthand_debug: false</codeblock><p><!--where,<ul id="ul_swt_2mb_lt"><li>hplefthand_password :  Password given during cluster creation.</li><li>hplefthand_clustername: Name of the VSA cluster.</li><li>hplefthand_api_url: Virtual IP of Store Virtual.</li><li>hplefthand_username: Username given during cluster creation.</li><li>hplefthand_iscsi_chap_enabled : </li><li>volume_backend_name: Name of the backend.</li><li>volume_driver</li><li>hplefthand_debug</li></ul>--></p><note>
              Please enter the parameter values which are specific to your environment. </note></li>
        <li>Commit your configuration to a <xref href="using_git.dita"
          >local
          repository</xref>:<codeblock>cd ~/helion/hos/ansible
            git add -A
            git commit -m "your commit message"</codeblock><note>
              Enter your commit message in quotes </note></li>
        <li>Run the configuration
          processor:<codeblock>cd ~/helion/hos/ansible
            ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
        <li>Run the following command to create a deployment
          directory.<codeblock>cd ~/helion/hos/ansible
            ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
      </ol></p>
    </section>
    <p id="run-ceph-client-package"><b>Run Cinder Reconfigure Packages</b></p>
    <p>Execute the following command to configure VSA. </p>
    <p>
      <codeblock>cd ~/scratch/ansible/next/hos/ansible
        ansible-playbook -i hosts/verb_hosts cinder-reconfigure.yml</codeblock>
    </p>
    <p>You must verify the VSA configuration as backend by creating the volume and volume type. Once
      you are able to create the volume, you can delete the created volume. For more information,
      refer to <xref href="blockstorage/verify_backend_configuration.dita">Verify Backend
        Configuration</xref>.</p>
    <note type="important"> You can create more than one VSA cluster of same or different type by
      specifying the configuration in cloud model. For more details, refer <xref
        href="blockstorage/vsa/vsa_create_multiple_clusters.dita">Modifying Cloud Model to Create
        Multiple Clusters</xref>.</note>
    <section id="deploy-vsa-with-ao-without-ao"><b>VSA with AO and without AO</b><p>VSA is deployed
      with adoptive optimization (AO) and without AO. AO allows built-in storage tiering for VSA.
      While deploying VSA with or without AO you must ensure to use the appropriate disk input
      model.</p><p>VSA with AO will have extra disk section with the marked usage of
        adoptive-optimization as mentioned in the following
        example:<codeblock>Additional disks can be added if available
          device_groups:
            - name: vsa-data
              consumer:
                name: vsa
                usage: data
              devices:
                - name: /dev/sdc
          - name: /dev/sdd
          - name: /dev/sde
          - name: /dev/sdf
          
            - name: vsa-cache
              consumer:
                name: vsa
                usage: <b>adaptive-optimization</b>
              devices:
                - name: /dev/sdb</codeblock></p><p>VSA
            without AO consist of data disks as mentioned in the following
            example:<codeblock>Additional disks can be added if available
              device_groups:
                - name: vsa-data
                  consumer:
                    name: vsa
                    usage: data
                  devices:
                    - name: /dev/sdc
              - name: /dev/sdd
              - name: /dev/sde
              - name: /dev/sdf</codeblock></p><p>It
                is recommended to use SSD disk for AO. The minimum VSA deployment with AO is 2 (apart from
                OS disk) and 1 for VSA without AO. However, the maximum supported disk is 7.
                <!--, VSA will have 1-6 disks as data disks and 1 disk as VSA with AO configuration. --></p><!--<p>VSA deployment can done with AO or without AO. AO stands for adoptive optmization which allows in-built storage tiring for VSA. From the deployment perspective, deployment of VSA with or without AO is not having much difference excepy in disk input model.For example: a VSA with AO will have to have extra disk section with the marked usage of adoptive-optimization mentioned below. Additional disks can be added if availiable device_groups:   - name: vsa-data     consumer:       name: vsa       usage: data     devices:       - name: /dev/sdc - name: /dev/sdd - name: /dev/sde - name: /dev/sdf   - name: vsa-cache     consumer:       name: vsa       usage: adaptive-optimization     devices:       - name: /dev/sdb For VSA without AO, we do need to describe only data disks as mentioned below: Additional disks can be added if availiable device_groups:   - name: vsa-data     consumer:       name: vsa       usage: data     devices:       - name: /dev/sdc - name: /dev/sdd - name: /dev/sde - name: /dev/sdf idealy, it is recommended to use SSD disk for adoptive-optimization purpose. So, a typical VSA will have 1-6 disks as data disks and 1 disks as VSA with AO configuration. From above it is clear that minimum disk for VSA deployment with AO is 2 (apart from OS disk) and 1 for VSA without AO </p>--></section>
    
 
 
 
    <section id="post-installation"><title>Post-Installation Verification and Administration</title>
      <p>We recommend the following post-installation steps:</p>
      <ul>
        <li><xref href="installation_verification.dita">Verify the installation using Tempest's
            embedded tests.</xref></li>
        <li>Go through the <xref href="postinstall_checklist.dita">Post-Installation
            Checklist</xref> for common administrative tasks.</li>
      </ul>
    </section>
  </body>
</topic>
