<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="config_ceph">
  <title>HP Helion <tm tmtype="reg">OpenStack</tm> 2.0: Configuring for Ceph Block Storage
    Backend</title>
  <body><!--Needs Edit-->
    <p>This page describes how to configure your Ceph backend for the Helion Entry Scale Cloud with
      KVM model.</p>
    <section>
      <title>Prerequisite</title>
      <ul id="ul_nhr_blx_st">
        <li>The deployer node must be setup before deploying Ceph. For more details on the
          installation of deployer node, refer to <xref
            href="install_entryscale_kvm.dita#install_kvm">installation guide.</xref></li>
        <li>You can deploy the Ceph monitor service on a dedicated resource. Ensure you modify your
          environment after deploying the deployer node. For more details, refer to <xref
            href="../blockstorage/ceph/deploy_monitor_stand_alone_node.dita#topic_ah5_4yx_qt"
            >Install a monitor service on a dedicated resource node</xref>.</li>
      </ul>
    </section>
    <section>
      <title>Installation Procedure</title>
      <p>Perform the following procedures to configure, install, and the integrate the Ceph Block
        Storage with HP Helion OpenStack 2.0.</p>
    </section>
    <section>
      <p>
        <ol id="ol_xmr_blx_st">
          <li>Login to the deployer node.</li>
          <li>Copy the example configuration files into the required setup directory and edit them
            to contain the details of your environment:
            <codeblock>cp -r ~/helion/examples/entry-scale-kvm-ceph/* ~/helion/my_cloud/definition/</codeblock>Begin
            inputting your environment information into the configuration files in the
              <codeph>~/helion/my_cloud/definition</codeph> directory. Full details of how to do
            this can be found here: <xref href="../input_model.dita">Helion OpenStack 2.0 Input
              Model</xref>. </li>
          <li>Edit the file <codeph>disks_osd.yml</codeph> and enter the details for the additional
            disks meant for OSD data and journal
              filesystems.<codeblock>vi <codeph>disks_osd.yml</codeph></codeblock><p>A sample of
                <codeph>disks_osd.yml</codeph> is as
              follows:</p><codeblock>disk-models:
  - name: OSD-DISKS
    # Disk model to be used for Ceph OSD nodes
    # /dev/sda_root is used as a volume group for /, /var/log and /var/crash
    # sda_root is a templated value to align with whatever partition is really used
    # This value is checked in os config and replaced by the partition actually used
    # on sda e.g. sda1 or sda5

    # Disks to be used by Ceph
    # Additional disks can be added if available
    device-groups:
      - name: ceph-osd-data-only
        devices:
          - name: /dev/sdb
        consumer:
           name: ceph
           attrs:
             usage: data
      - name: ceph-osd-data-and-journal
        devices:
          - name: /dev/sdc
        consumer:
           name: ceph
           attrs:
             usage: data
             journal_disk: /dev/sdd
      - name: ceph-osd-data-and-shared-journal-set-1
        devices:
          - name: /dev/sde
        consumer:
           name: ceph
           attrs:
             usage: data
             journal_disk: /dev/sdg
      - name: ceph-osd-data-and-shared-journal-set-2
        devices:
          - name: /dev/sdf
        consumer:
           name: ceph
           attrs:
             usage: data
             journal_disk: /dev/sdg</codeblock><p>The
              above sample file contains three OSD nodes and two journal disk.</p><p>The disk model
              has the following fields:</p><p>
              <simpletable id="simpletable_iqr_blx_st">
                <strow>
                  <stentry><b>device-groups</b></stentry>
                  <stentry>There can be several device groups. This allows different sets of disks
                    to be used for different purposes.</stentry>
                </strow>
                <strow>
                  <stentry><b>name</b></stentry>
                  <stentry>This is an arbitrary name for the device group. The name must be
                    unique.</stentry>
                </strow>
                <strow>
                  <stentry><b>devices</b></stentry>
                  <stentry>This is a list of devices allocated to the device group. A
                      <codeph>name</codeph> field containing <codeph>/dev/sdb</codeph>,
                      <codeph>/dev/sdc</codeph>, <codeph>/dev/sde</codeph> and
                      <codeph>/dev/sdf</codeph> indicates that the device group is used by
                    Ceph.</stentry>
                </strow>
                <strow>
                  <stentry><b>consumer</b></stentry>
                  <stentry>This specifies the service that uses the device group. A
                      <codeph>name</codeph> field containing <b>ceph</b> indicates that the device
                    group is used by Ceph.</stentry>
                </strow>
                <strow>
                  <stentry><b>attrs</b></stentry>
                  <stentry>This is the list of attributes.</stentry>
                </strow>
                <strow>
                  <stentry><b>usage</b></stentry>
                  <stentry>There can be several use of devices for a particular service. In the
                    above sample, <codeph>usage</codeph> field contains <b>data</b> which indicates
                    that the device is used for data storage.</stentry>
                </strow>
                <strow>
                  <stentry><b>journal_disk</b></stentry>
                  <stentry>It is to used to capture journal data. You can share the journal disk
                    between two devices.</stentry>
                </strow>
              </simpletable>
            </p><p><!--<b>How to edit osd.yml file</b><?oxy_custom_start type="oxy_content_highlight" color="255,255,0"?>??<?oxy_custom_end?>--></p><note>Ensure
              that disks designated to be used for the OSD data and journal storage must be in a
              clean state (i.e. any existing partitions must be deleted). If you do not perform this
              step, Ceph configuration fails with errors.</note></li>
          <li>Editable parameters for Ceph are available in the following locations:<ul
              id="ul_zrr_blx_st">
              <li><codeph>~/helion/my_cloud/config/ceph/settings.yml</codeph><p>In the
                    <codeph>settings.yml</codeph> file, you can edit the following parameters:<ul
                    id="ul_mvr_blx_st">
                    <li><codeph>fsid</codeph></li>
                    <li><codeph>ceph clusters</codeph></li>
                    <li><codeph>osd_settle_time</codeph></li>
                    <li><codeph>OSD_journal_size</codeph></li>
                  </ul></p></li>
              <li>
                <p>Add any additional configuration parameters for Ceph in the same file
                    (<codeph>settings.yml</codeph> file) under the 'extra:' category as
                  follows:<codeblock>extra:
  osd:
    journal_max_write_entries: 200</codeblock></p>
              </li>
              <li>
                <p><codeph>~/helion/my_cloud/config/ceph/user_model.yml</codeph></p>
                <p>The <codeph>user_model.yml</codeph> has the editable values for the different
                  pools created by HP Helion OpenStack.</p>
              </li>
            </ul></li>
          <li> Commit your
              configuration<codeblock>cd ~/helion/hos/ansible
git add -A
git commit -m "&lt;commit message>"</codeblock><note>
              Enter your commit message &lt;commit message></note></li>
          <li>Run the configuration
            processor:<codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
          <li>Run the following command to create a deployment
            directory.<codeblock>cd ~/helion/hos/ansible
<codeph>ansible-playbook -i hosts/localhost ready-deployment.yml</codeph></codeblock></li>
          <li>Run the following ansible
            playbook:<codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts site.yml</codeblock></li>
        </ol>
      </p>
    </section>
    <p>Ceph Monitor service is deployed on the Controller Nodes and OSDs are deployed as separate
      nodes (Resource Nodes).<note>While executing <codeph>site.yml</codeph> playbook with the
          <codeph>â€“-limit</codeph> option, it is also expected to include the controller nodes in
        the list of restricted nodes if the cloud is deployed with a Ceph cluster.</note></p>
    <p><b>Run Ceph Client Packages</b></p>
    <p>Ceph can be used as backend for <xref
        href="../blockstorage/ceph/configuring_use_ceph_cluster.dita#topic_i2z_zc3_pt"
        >cinder-volume</xref>
      <!--and a <xref href="../blockstorage/ceph/configuring_use_ceph_cluster_glance.dita#topic_qch_rmx_st">default glance store</xref>-->.
      The nodes running these services should have a client installed on it. Use
        <codeph>ceph-client-prepare.yml</codeph> to deploy the Ceph client on respective nodes.</p>
    <p>Execute the following playbook so that the rings files are set up properly:</p>
    <p>
      <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts ceph-client-prepare.yml</codeblock>
    </p>
    <p>This will also create Ceph users and Ceph pools on the resource nodes.</p>
  </body>
</topic>
