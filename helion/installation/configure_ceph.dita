<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="config_ceph">
  <title>HP Helion <tm tmtype="reg">OpenStack</tm> 2.0: Configuring for Ceph Block Storage
    Backend</title>
  <body>
    <p>This page describes how to configure your Ceph backend for the Helion Entry-scale with KVM
      Cloud model.</p>
    <section id="notes1"><title>Notes</title>
      <p>The Ceph cluster expects the network group that will be used for management network traffic
        within the cloud to be left to its default value i.e. MANAGEMENT. Altering the name will
        lead to failures in the cloud deployment, therefore it should be avoided.</p>
      <p>You can deploy the Ceph monitor service on a dedicated resource. Ensure you modify your
        environment after installing the lifecycle-manager.</p>
      <p>For more details, refer to <xref
          href="../blockstorage/ceph/deploy_monitor_stand_alone_node.dita">Install a monitor service
          on a dedicated resource node</xref>.</p>
    </section>
    <section id="config_files">
      <title>Configuring Your Ceph Environment Input Files</title>
      <p>
        <ol>
          <li>Log in to the deployer/lifecycle-manager node.</li>
          <li>Copy the example configuration files into the required setup directory and edit them
            to contain the details of your environment: <codeblock>cp -r ~/helion/examples/entry-scale-kvm-ceph/* ~/helion/my_cloud/definition/</codeblock>
            <p>Begin inputting your environment information into the configuration files in the
                <codeph>~/helion/my_cloud/definition</codeph> directory.</p>
            <p>Full details of how to do this can be found here: <xref href="../input_model.dita"
                >Helion OpenStack 2.0 Input Model</xref>.</p></li>
          <li>Edit the <codeph>~/helion/my_cloud/definition/data/disks_osd.yml</codeph> file and
            enter the details for the additional disks meant for OSD data and journal filesystems.
              <p>A sample <codeph>disks_osd.yml</codeph> is as follows:</p>
            <codeblock>disk-models:
  - name: OSD-DISKS
    # Disk model to be used for Ceph OSD nodes
    # /dev/sda_root is used as a volume group for /, /var/log and /var/crash
    # sda_root is a templated value to align with whatever partition is really used
    # This value is checked in os config and replaced by the partition actually used
    # on sda e.g. sda1 or sda5

    # Disks to be used by Ceph
    # Additional disks can be added if available
    device-groups:
      - name: ceph-osd-data-only
        devices:
          - name: /dev/sdb
        consumer:
           name: ceph
           attrs:
             usage: data
      - name: ceph-osd-data-and-journal
        devices:
          - name: /dev/sdc
        consumer:
           name: ceph
           attrs:
             usage: data
             journal_disk: /dev/sdd
      - name: ceph-osd-data-and-shared-journal-set-1
        devices:
          - name: /dev/sde
        consumer:
           name: ceph
           attrs:
             usage: data
             journal_disk: /dev/sdg
      - name: ceph-osd-data-and-shared-journal-set-2
        devices:
          - name: /dev/sdf
        consumer:
           name: ceph
           attrs:
             usage: data
             journal_disk: /dev/sdg</codeblock>
            <p>The above sample file contains three OSD nodes and two journal disks.</p>
            <p>The disk model has the following fields:</p>
            <p>
              <table frame="all" rowsep="1" colsep="1" id="ceph1">
                <tgroup cols="2">
                  <colspec colname="c1" colnum="1"/>
                  <colspec colname="c2" colnum="2"/>
                  <thead>
                    <row>
                      <entry>Value</entry>
                      <entry>Description</entry>
                    </row>
                  </thead>
                  <tbody>
                    <row>
                      <entry><b>device-groups</b></entry>
                      <entry>There can be several device groups. This allows different sets of disks
                        to be used for different purposes.</entry>
                    </row>
                    <row>
                      <entry><b>name</b></entry>
                      <entry>This is an arbitrary name for the device group. The name must be
                        unique.</entry>
                    </row>
                    <row>
                      <entry><b>devices</b></entry>
                      <entry>This is a list of devices allocated to the device group. A
                          <codeph>name</codeph> field containing <codeph>/dev/sdb</codeph>,
                          <codeph>/dev/sdc</codeph>, <codeph>/dev/sde</codeph> and
                          <codeph>/dev/sdf</codeph> indicates that the device group is used by
                        Ceph.</entry>
                    </row>
                    <row>
                      <entry><b>consumer</b></entry>
                      <entry>This specifies the service that uses the device group. A
                          <codeph>name</codeph> field containing <b>ceph</b> indicates that the
                        device group is used by Ceph.</entry>
                    </row>
                    <row>
                      <entry><b>attrs</b></entry>
                      <entry>These are the attributes associated with the consumer.</entry>
                    </row>
                    <row>
                      <entry><b>usage</b></entry>
                      <entry>There can be several uses of devices for a particular service. In the
                        above sample, <codeph>usage</codeph> field contains <b>data</b> which
                        indicates that the device is used for data storage.</entry>
                    </row>
                    <row>
                      <entry><b>journal_disk</b></entry>
                      <entry>Disk to be used for storing the journal data. When running multiple
                        Ceph OSD daemons on a single node, a journal disk can be shared between OSDs
                        of the node.</entry>
                    </row>
                  </tbody>
                </tgroup>
              </table>
            </p>
            <p><note>Ensure that disks designated to be used for the OSD data and journal storage
                must be in a clean state (i.e. any existing partitions must be deleted). If you do
                not perform this step, Ceph configuration fails with errors.</note></p></li>
          <li><p>Editable parameters for Ceph are available in the following locations:</p>
            <ul>
              <li><codeph>~/helion/my_cloud/config/ceph/settings.yml</codeph>
                <p>In the <codeph>settings.yml</codeph> file, you can edit the following
                  parameters:</p>
                <p>
                  <table frame="all" rowsep="1" colsep="1" id="table_gc4_c5t_5t">
                    <tgroup cols="2">
                      <colspec colname="c1" colnum="1"/>
                      <colspec colname="c2" colnum="2"/>
                      <thead>
                        <row>
                          <entry>Value</entry>
                          <entry>Description</entry>
                        </row>
                      </thead>
                      <tbody>
                        <row>
                          <entry><codeph>fsid</codeph></entry>
                          <entry>It is a unique identifier for the Ceph cluster.</entry>
                        </row>
                        <row>
                          <entry><codeph>ceph_cluster</codeph></entry>
                          <entry>
                            <p>Ceph clusters have a cluster name. The default cluster name is ceph,
                              but you may specify a different cluster name.</p>
                          </entry>
                        </row>
                        <row>
                          <entry><codeph>osd_settle_time</codeph></entry>
                          <entry>
                            <p>Time in seconds to wait for after starting/restarting the Ceph OSD
                              services.</p>
                          </entry>
                        </row>
                        <row>
                          <entry><codeph>osd_journal_size</codeph></entry>
                          <entry>
                            <p>The size of the journal in megabytes.</p>
                          </entry>
                        </row>
                      </tbody>
                    </tgroup>
                  </table></p></li>
              <li>
                <p>Add any additional configuration parameters for Ceph in the same file
                    (<codeph>settings.yml</codeph> file) under the 'extra:' category as
                  follows:<codeblock>extra:
  osd:
    journal_max_write_entries: 200</codeblock></p>
              </li>
              <li>
                <p><codeph>~/helion/my_cloud/config/ceph/user_model.yml</codeph></p>
                <p>The <codeph>user_model.yml</codeph> has the editable values for the different
                  pools created by HP Helion OpenStack.</p>
              </li>
            </ul></li>
          <li>Commit your
            configuration<codeblock>cd ~/helion/hos/ansible
git add -A
git commit -m "&lt;commit message>"</codeblock></li>
        </ol></p>
      <p>After your configuration files are setup, continue with the <xref
          href="install_entryscale_kvm.dita#install_kvm/provision">Entry-scale KVM Cloud
          installation steps.</xref></p>
    </section>



    <section id="prereqs"><title>Prerequisites</title>
      <p>In order for Ceph to be used as a <xref
          href="installation_verification.dita#install_verification/volume_verify">backend for
          volumes</xref>, the nodes running these services should have the Ceph client installed on
        them. Use the <codeph>ceph-client-prepare.yml</codeph> playbook to deploy the Ceph client on
        these nodes.</p>
      <p>
        <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts ceph-client-prepare.yml</codeblock>
      </p>
      <p>This will also create Ceph users and Ceph pools on the resource nodes.</p>
    </section>
    <section id="notes"><title>Notes</title>
      <p>While executing the <codeph>site.yml</codeph> playbook with the <codeph>–-limit</codeph>
        option, it is also expected to include the controller nodes in the list of restricted nodes
        if the cloud is deployed with a Ceph cluster.</p>
    </section>
    <section id="configure_backend">
      <title>Configure Ceph as the Backend</title>
      <p>You can use Ceph as either the backend for volumes or volume backups or both. These steps
        will show you how to do this.</p>
      <p>Perform the following procedure on the deployer/lifecycle-manager node to configure Ceph as
        a volume backend:</p>
      <p>
        <ol>
          <li>Log in to the lifecycle-manager node.</li>
          <li>Make the following changes to the
              <codeph>~/helion/my_cloud/config/cinder/cinder.conf.j2</codeph> file: <ol>
              <li>Add your Ceph backend to the <codeph>enabled_backends</codeph> section:
                <codeblock># Configure the enabled backends
                  enabled_backends=ceph1</codeblock></li>
              <li>[OPTIONAL] If you want a use a default volume type, then enter it in the
                  <codeph>[DEFAULT]</codeph> section with the syntax below. You will want to
                remember this value when you create your volume type in the next section.
                <codeblock>[DEFAULT]
                  # Set the default volume type
                  default_volume_type = &lt;your new volume type></codeblock></li>
              <li>Uncomment the <codeph>StoreVirtual (VSA) cluster</codeph> section and fill the
                values as per your cluster information. If you have more than one cluster, you will
                need to add another similar section with its respective values. In the following
                example only one cluster is added. <codeblock>[ceph1]
rbd_secret_uuid = 457eb676-33da-42ec-9a8c-9293d545c337
rbd_user = cinder
rbd_pool = volumes
rbd_ceph_conf = /etc/ceph/ceph.conf
volume_driver = cinder.volume.drivers.rbd.RBDDriver
volume_backend_name = ceph</codeblock>
                <note>The <codeph>rbd_secret_uuid</codeph> entry is same as
                    <codeph>secret_id</codeph> which is available at
                    <codeph>~/helion/my_cloud/config/ceph/user_model.yml</codeph>.</note></li>
                <li>To enable Cinder to backup to Ceph, modify <codeph>cinder.conf.j2</codeph> at
                  <codeph>~/helion/my_cloud/config/cinder</codeph> in the [<b>Default</b>] section with
                  the following values:
                  <codeblock># Ceph backup
                    backup_driver = cinder.backup.drivers.ceph
                    backup_ceph_conf = /etc/ceph/ceph.conf
                    backup_ceph_user = cinder-backup
                    backup_ceph_pool = backups</codeblock><note>You
                      can add a comment (<b># Ceph backup </b>) as shown above so that the Ceph backup
                      content can be easily identified in the <codeph>cinder.conf.j2</codeph>
                      file.</note></li>
            </ol>
          </li>
          <li>Copy <codeph>ceph.client.cinder.keyring</codeph> to the controller nodes: <ol>
              <li>Login to the controller nodes as a root user and run the following
                    command:<codeblock>ceph auth get-or-create client.cinder | tee /etc/ceph/ceph.client.cinder.keyring</codeblock><p><b>OR</b></p></li>
              <li>You can copy the keyring from the deployer/lifecycle-manager node to
                  <codeph>/etc/ceph</codeph> folder on all the controller
                nodes:<codeblock>scp /etc/ceph/ceph.client.cinder.keyring</codeblock></li>
            </ol></li>
          <li>Commit your configuration to the local Git repository to configure Cinder on the
            deployer/lifecycle-manager node:<p>
              <codeblock>cd ~/helion/hos/ansible
                git add -A
                git commit -m "&lt;commit message>"</codeblock>
            </p><note> Enter your commit message &lt;commit message>.</note></li>
          <li>Create a deployment
            directory:<codeblock>cd ~/helion/hos/ansible
              <codeph>ansible-playbook -i hosts/localhost ready-deployment.yml</codeph></codeblock></li>
          <li>Run the Ansible
            playbook:<codeblock>cd ~/scratch/ansible/next/hos/ansible
              ansible-playbook -i hosts/verb_hosts cinder-reconfigure.yml</codeblock></li>
        </ol>
      </p>
    </section>


    <section id="post_install"><title>Verifying your Ceph backend</title>
      <p>After you have configured Ceph as your Block Storage backend, you can verify this all
        completed successfully by creating a new volume.</p>
      <p>See <xref href="installation_verification.dita">Verifying your Installation</xref> for more
        details.</p>
    </section>
  </body>
</topic>
