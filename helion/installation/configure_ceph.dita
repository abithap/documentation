<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="config_ceph">
  <title>HP Helion <tm tmtype="reg">OpenStack</tm> 2.0: Configuring for Ceph Block Storage
    Backend</title>
  <body>
    <!--Needs Edit-->
    <p>This page describes how to configure your Ceph backend for the Helion Entry-scale with KVM
      Cloud model.</p>
    <section>
      <title>Prerequisites</title>
      <ul>
        <li>The Entry-scale KVM with VSA cloud model should be deployed. For more details on the
          installation refer to the <xref href="install_entryscale_kvm.dita#install_kvm">Entry-scale
            with KVM installation</xref> instructions.</li>
      </ul>
      <p><note type="caution">The Ceph cluster expects the network group that will be used for
          management network traffic within the cloud to be left to its default value i.e.
          MANAGEMENT. Altering the name will lead to failures in the cloud deployment, therefore it
          should be avoided.</note></p>
    </section>
    <section id="notes"><title>Notes</title>
      <p>You can deploy the Ceph monitor service on a dedicated resource. Ensure you modify your
        environment after installing the lifecycle-manager.</p>
      <p>For more details, refer to <xref
          href="../blockstorage/ceph/deploy_monitor_stand_alone_node.dita">Install a monitor service
          on a dedicated resource node</xref>.</p>
    </section>
    <section id="config_files">
      <title>Configuring Your Ceph Environment Input Files</title>
      <p>
        <ol>
          <li>Log in to the lifecycle-manager node.</li>
          <li>Copy the example configuration files into the required setup directory and edit them
            to contain the details of your environment: <codeblock>cp -r ~/helion/examples/entry-scale-kvm-ceph/* ~/helion/my_cloud/definition/</codeblock>
            <p>Begin inputting your environment information into the configuration files in the
                <codeph>~/helion/my_cloud/definition</codeph> directory.</p>
            <p>Full details of how to do this can be found here: <xref href="../input_model.dita"
                >Helion OpenStack 2.0 Input Model</xref>.</p></li>
          <li>Edit the <codeph>~/helion/my_cloud/definition/data/disks_osd.yml</codeph> file and
            enter the details for the additional disks meant for OSD data and journal filesystems.
              <p>A sample <codeph>disks_osd.yml</codeph> is as follows:</p>
            <codeblock>disk-models:
  - name: OSD-DISKS
    # Disk model to be used for Ceph OSD nodes
    # /dev/sda_root is used as a volume group for /, /var/log and /var/crash
    # sda_root is a templated value to align with whatever partition is really used
    # This value is checked in os config and replaced by the partition actually used
    # on sda e.g. sda1 or sda5

    # Disks to be used by Ceph
    # Additional disks can be added if available
    device-groups:
      - name: ceph-osd-data-only
        devices:
          - name: /dev/sdb
        consumer:
           name: ceph
           attrs:
             usage: data
      - name: ceph-osd-data-and-journal
        devices:
          - name: /dev/sdc
        consumer:
           name: ceph
           attrs:
             usage: data
             journal_disk: /dev/sdd
      - name: ceph-osd-data-and-shared-journal-set-1
        devices:
          - name: /dev/sde
        consumer:
           name: ceph
           attrs:
             usage: data
             journal_disk: /dev/sdg
      - name: ceph-osd-data-and-shared-journal-set-2
        devices:
          - name: /dev/sdf
        consumer:
           name: ceph
           attrs:
             usage: data
             journal_disk: /dev/sdg</codeblock>
            <p>The above sample file contains three OSD nodes and two journal disks.</p>
            <p>The disk model has the following fields:</p>
            <p>
              <simpletable id="simpletable_iqr_blx_st">
                <strow>
                  <stentry><b>device-groups</b></stentry>
                  <stentry>There can be several device groups. This allows different sets of disks
                    to be used for different purposes.</stentry>
                </strow>
                <strow>
                  <stentry><b>name</b></stentry>
                  <stentry>This is an arbitrary name for the device group. The name must be
                    unique.</stentry>
                </strow>
                <strow>
                  <stentry><b>devices</b></stentry>
                  <stentry>This is a list of devices allocated to the device group. A
                      <codeph>name</codeph> field containing <codeph>/dev/sdb</codeph>,
                      <codeph>/dev/sdc</codeph>, <codeph>/dev/sde</codeph> and
                      <codeph>/dev/sdf</codeph> indicates that the device group is used by
                    Ceph.</stentry>
                </strow>
                <strow>
                  <stentry><b>consumer</b></stentry>
                  <stentry>This specifies the service that uses the device group. A
                      <codeph>name</codeph> field containing <b>ceph</b> indicates that the device
                    group is used by Ceph.</stentry>
                </strow>
                <strow>
                  <stentry><b>attrs</b></stentry>
                  <stentry>This is the list of attributes.</stentry>
                </strow>
                <strow>
                  <stentry><b>usage</b></stentry>
                  <stentry>There can be several use of devices for a particular service. In the
                    above sample, <codeph>usage</codeph> field contains <b>data</b> which indicates
                    that the device is used for data storage.</stentry>
                </strow>
                <strow>
                  <stentry><b>journal_disk</b></stentry>
                  <stentry>Disk to be used for storing the journal data. When running multiple Ceph
                    OSD daemons on a single node, a journal disk can be shared between OSDs of the
                    node.</stentry>
                </strow>
              </simpletable>
            </p>
            <p><note>Ensure that disks designated to be used for the OSD data and journal storage
                must be in a clean state (i.e. any existing partitions must be deleted). If you do
                not perform this step, Ceph configuration fails with errors.</note></p></li>
          <li>Editable parameters for Ceph are available in the following locations: <ul>
              <li><codeph>~/helion/my_cloud/config/ceph/settings.yml</codeph>
                <p>In the <codeph>settings.yml</codeph> file, you can edit the following parameters:
                    <simpletable frame="all" relcolwidth="1.0* 1.0*" id="simpletable_pt2_cf4_5t">
                    <strow>
                      <stentry>Parameters</stentry>
                      <stentry>Descriptions</stentry>
                    </strow>
                    <strow>
                      <stentry><codeph>fsid</codeph></stentry>
                      <stentry>It is a unique identifier for the Ceph cluster.</stentry>
                    </strow>
                    <strow>
                      <stentry><codeph>ceph clusters</codeph></stentry>
                      <stentry>
                        <p>Ceph clusters have a cluster name. The default cluster name is ceph, but
                          you may specify a different cluster name.</p>
                      </stentry>
                    </strow>
                    <strow>
                      <stentry><codeph>osd_settle_time</codeph></stentry>
                      <stentry>
                        <p>Time in seconds to wait for after starting/restarting the Ceph OSD
                          services.</p>
                      </stentry>
                    </strow>
                    <strow>
                      <stentry><codeph>osd_journal_size</codeph></stentry>
                      <stentry>
                        <p>The size of the journal in megabytes.</p>
                      </stentry>
                    </strow>
                  </simpletable></p></li>
              <li>
                <p>Add any additional configuration parameters for Ceph in the same file
                    (<codeph>settings.yml</codeph> file) under the 'extra:' category as
                  follows:<codeblock>extra:
  osd:
    journal_max_write_entries: 200</codeblock></p>
              </li>
              <li>
                <p><codeph>~/helion/my_cloud/config/ceph/user_model.yml</codeph></p>
                <p>The <codeph>user_model.yml</codeph> has the editable values for the different
                  pools created by HP Helion OpenStack.</p>
              </li>
            </ul></li>
          <li>Commit your
              configuration<codeblock>cd ~/helion/hos/ansible
git add -A
git commit -m "&lt;commit message>"</codeblock><note>
              Enter your commit message &lt;commit message></note></li>
        </ol></p>
    </section>
    <section id="install_ceph"><title>Run Ceph Client Packages</title>
      <p>Ceph Monitor service is deployed on the Controller Nodes and OSDs are deployed as separate
        nodes (Resource Nodes).<note>While executing <codeph>site.yml</codeph> playbook with the
            <codeph>â€“-limit</codeph> option, it is also expected to include the controller nodes in
          the list of restricted nodes if the cloud is deployed with a Ceph cluster.</note></p>
      <p><b>Run Ceph Client Packages</b></p>
      <p>Ceph can be used as backend for <xref
          href="../blockstorage/ceph/configuring_use_ceph_cluster.dita#topic_i2z_zc3_pt"
          >cinder-volume</xref> The nodes running these services should have a client installed on
        it. Use <codeph>ceph-client-prepare.yml</codeph> to deploy the Ceph client on respective
        nodes.</p>
      <p>Execute the following playbook so that the rings files are set up properly:</p>
      <p>
        <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts ceph-client-prepare.yml</codeblock>
      </p>
      <p>This will also create Ceph users and Ceph pools on the resource nodes.</p>
    </section>
  </body>
</topic>
