<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="config_vsa">
  <title>HP Helion <tm tmtype="reg">OpenStack</tm> 2.0: Configuring for VSA Block Storage
    Backend</title>
  <body>
    <section id="about">
      <p>This page describes how to configure your VSA backend for the Helion Entry Scale Cloud with
        KVM model.</p>
    </section>
    <section id="prereq"><title>Pre-requisites</title>
      <p>
        <ul>
          <li>Your HP Helion Entry Scale Cloud should be up and running. Installation steps can be
            found <xref href="install_entryscale_kvm.dita">here</xref>.</li>
          <li>You'll need to know the IP address of the VSA VM and VSA Cluster from the
              <codeph>~/scratch/ansible/last/my_cloud/stage/info/net_info.yml</codeph> file, which
            is generated by configuration processor during installation.</li>
        </ul>
      </p>
      <p>You can deploy VSA with adoptive optimization (AO) or without. The process to deploy VSA
        with AO and without AO is similar with the exception of a change in the disk input model.
        For more detailed information, refer to the <xref
          href="#config_vsa/deploy-vsa-with-ao-without-ao" format="dita">VSA with AO and without
          AO</xref> section below.</p>
    </section>
    <section id="configuration">
      <title>Configure Your Environment</title>
      <ol>
        <li>Setup your configuration files, as follows: <ol>
            <li>Copy the example configuration files into the definition directory and edit them to
              contain the details of your environment:
              <codeblock>cp -r ~/helion/examples/entry-scale-kvm-vsa/* ~/helion/my_cloud/definition/</codeblock></li>
            <li>Begin inputting your environment information into the configuration files in the
                <codeph>~/helion/my_cloud/definition</codeph> directory. Full details of how to do
              this can be found here: <xref href="input_model.dita">Helion OpenStack 2.0 Input
                Model</xref>. <note type="important">If you chose to use your first controller node
                as your deployer, ensure that your <codeph>servers.yml</codeph> file contains the
                  <codeph>is-deployer: true</codeph> notation in your controller options. If you are
                using a dedicated deployer node you can omit this. Here is an example snippet of a
                  <codeph>servers.yml</codeph> file where a user is using their first controller
                node as their deployer:
                <codeblock># Controllers
  - id: controller1
    ip-addr: 192.168.10.3
    role: CONTROLLER-ROLE
    server-group: RACK1
    nic-mapping: HP-DL360-4PORT
    mac-addr: b2:72:8d:ac:7c:6f
    ilo-ip: 192.168.9.3
    ilo-password: password
    ilo-user: admin
    <b>is-deployer: true</b></codeblock></note></li>
            <li id="encrypt">[Optional] You can use the <codeph>hosencrypt.py</codeph> script to
              encrypt your iLo passwords. This script uses OpenSSL. <ol>
                <li>Change to the Ansible directory:
                  <codeblock>cd ~/helion/hos/ansible</codeblock></li>
                <li>Put the encrytion key into the following environment variable:
                  <codeblock>export HOS_USER_PASSWORD_ENCRYPT_KEY=&#60;encryption key></codeblock></li>
                <li>Run the python script below and follow the instructions. Enter a password that
                  you want to encrypt. <codeblock>hosencrypt.py</codeblock></li>
                <li>Take the string generated and place it in the <codeph>"ilo_password"</codeph>
                  field in your <codeph>~/helion/my_cloud/definition/data/servers.yml</codeph> file,
                  remembering to enclose it in quotes.</li>
                <li>Repeat the above for each server.</li>
              </ol>
              <note>Before you run any playbooks, remember that you need to export the encryption
                key in the following environment variable: <codeph>export
                  HOS_USER_PASSWORD_ENCRYPT_KEY=&#60;encryption key></codeph></note></li>
          </ol></li>
        <li>Commit your configuration to the <xref href="using_git.dita">local git repo</xref>, as
          follows: <codeblock>cd ~/helion/hos/ansible
git add -A
git commit -m "My config or other commit message"
</codeblock>
          <note type="important">This step needs to be repeated any time you make changes to your
            configuration files before you move onto the following steps. See <xref
              href="using_git.dita">Using Git for Configuration Management</xref> for more
            information.</note></li>
      </ol>
    </section>
    <section>
      <title>Creating a StoreVirtual Cluster and adding it to a new Management Group Cluster using
        CMC</title>
      <p><b> Install (any) X Display Tool to Launch Centralized Management Console (CMC)</b></p>
      <p>You must configure X display tool to launch CMC. User can select <b>any</b> X display tool.
        In this section we are using <b>Xming</b> tool for Windows as an example to launch CMC. The
        following example provides the steps to install Xming and launch CMC.</p>
      <p>
        <ol>
          <li>Install <b>Xming</b> on Windows Box from where you can access the deployer node.</li>
          <li>Select <b>Enable X11 forwarding</b> checkbox on the putty session for deployer node. </li>
          <li>SSH to first control plane node.<codeblock>ssh -X</codeblock></li>
          <li>Execute the following command:
            <codeblock>/opt/HP/StoreVirtual/UI/jre/bin/java -jar /opt/HP/StoreVirtual/UI/UI.jar</codeblock>CMC
            will be launch successfully.</li>
        </ol>
      </p>
      <p>Perform the following steps to create the cluster.<ol id="ol_yln_fhl_jt">
          <li>Open CMC.<p>By default, the CMC is configured to discover the StoreVirtual nodes in
              the subnet in which it is installed. You can manually add the nodes also.</p></li>
          <li>In the CMC UI, click <b>Find</b> and then click <b>Find Systems</b> from the left
            panel. Find Systems dialogue box is displayed.</li>
          <li>You can choose <b>Add</b> or <b>Find</b> option to search the system. Find option
            starts searching for the nodes in the same subnet as that of CMC. Add option displays an
              <b>Enter IP</b> pop-up box to enter the IP of the StoreVirtual system.</li>
          <li>In the CMC UI, click <b>Tasks</b> then point to <b>Management Group</b> and click<b>
              New Management Group</b>.</li>
          <li>In the <b>New Management Group Name</b> box, enter a name for the management group.
            For example: <codeph>hlm003-vsa-mg</codeph></li>
          <li>Click <b>Next</b> to display the Add Administrative User page.</li>
          <li>Enter the required information (username/password) for the Administrative User.<p>For
              example:</p><ul id="ul_cdk_xjl_jt">
              <li>username- stack </li>
              <li>password-stack</li>
            </ul></li>
          <li>Click <b>Next</b> to display the Management Group Time page. </li>
          <li>Add NTP server information and click <b>Next</b> to display the Domain Name System
            Configuration page.</li>
          <li><?oxy_custom_start type="oxy_content_highlight" color="255,255,0"?>Skip the DNS and
            SMTP sections. To do so, <ul id="ul_g42_ngz_qt">
              <li>Cllick <b>Next</b> and . A pop box displays and click <b>Accept
                Incomplete</b>.</li>
            </ul></li>
          <?oxy_custom_end?>
          <li>Click <b>Next</b> to display a Wizard in the Create a Cluster page.</li>
          <li>Select the cluster type as <i>Standard Cluster</i> from the displayed options and
            click <b>Next</b>.</li>
          <li>In the <b>Cluster Name</b> box, enter the name of the cluster and click <b>Next</b>.
            For example: <codeph>hlm003-vsa-cluster</codeph>.</li>
          <li>In the Assign Virtaula IP's and Subnet Masks page is displayed.</li>
          <li>Click Add. Add VIPs and Subnet Masks pop-up box displays, enter the virtual IP and
            Subnet Mask of the cluster in the respective boxes and click <b>OK</b>.
              <!--The details are displayed in a tabular format in the page. --><note>The VIP IP can
              be found as the cluster IP value in the
                <codeph>~/scratch/ansible/next/my_cloud/stage/info/net_info.yml</codeph>
              file.</note></li>
          <li>Click <b>Next</b> to go to the next page.</li>
          <li>Select the checkbox displayed against <b>Skip Volume Creation</b>.</li>
          <li>Click <b>Finish</b>.The Management Group and Cluster is created and displays in the
            Home page of CMC.<p>Wait till the cluster is displayed in the Home Page of CMC.</p></li>
        </ol></p>
    </section>
    <section>
      <title>Configure VSA as a Cinder Backend</title>
      <p>You must modify <codeph>cinder.conf.j2</codeph> to update VSA cluster.</p>
      <p>Perform the following steps to update the VSA cluster: <ol>
          <li>Login to deployer node.</li>
          <li>Edit <codeph>~/helion/my_cloud/config/cinder/cinder.conf.j2</codeph> to add VSA data
            as
            follows:<codeblock># Configure the enabled backends
            enabled_backends=vsa-1</codeblock></li>
          <li>Copy VSA section, uncomment it, and modify the values in the
              <codeph>cinder.conf.j2</codeph> file as shown in the following example:<codeblock id="vssa-config">[vsa-1]
            hplefthand_password: &lt;password given during cluster creation>
            hplefthand_clustername: &lt;VSA cluster name>
            hplefthand_api_url: https://&lt;store virtual virtual ip>:8081/lhos
            hplefthand_username: &lt;username given during cluster creation>
            hplefthand_iscsi_chap_enabled: true
            volume_backend_name: &lt;volume backend name>
            volume_driver: cinder.volume.drivers.san.hp.hp_lefthand_iscsi.HPLeftHandISCSIDriver
            hplefthand_debug: false</codeblock><p>
              <!--where,<ul id="ul_swt_2mb_lt"><li>hplefthand_password :  Password given during cluster creation.</li><li>hplefthand_clustername: Name of the VSA cluster.</li><li>hplefthand_api_url: Virtual IP of Store Virtual.</li><li>hplefthand_username: Username given during cluster creation.</li><li>hplefthand_iscsi_chap_enabled : </li><li>volume_backend_name: Name of the backend.</li><li>volume_driver</li><li>hplefthand_debug</li></ul>-->
              <note>HOS 2.0 is expected to support physical lefthand boxes along with VSA deployed
                on ESX, hyper-v environment</note>
            </p><note> Please enter the parameter values which are specific to your environment.
            </note></li>
          <li>Commit your configuration to a <xref href="using_git.dita">local
              repository</xref>:<codeblock>cd ~/helion/hos/ansible
            git add -A
            git commit -m "your commit message"</codeblock><note>
              Enter your commit message in quotes </note></li>
          <li>Run the configuration
            processor:<codeblock>cd ~/helion/hos/ansible
            ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
          <li>Run the following command to create a deployment
            directory.<codeblock>cd ~/helion/hos/ansible
            ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
        </ol></p>
    </section>
    <p id="run-ceph-client-package"><b>Run Cinder Reconfigure Packages</b></p>
    <p>Execute the following command to configure VSA. </p>
    <p>
      <codeblock>cd ~/scratch/ansible/next/hos/ansible
        ansible-playbook -i hosts/verb_hosts cinder-reconfigure.yml</codeblock>
    </p>
    <p>You must verify the VSA configuration as backend by creating the volume and volume type. Once
      you are able to create the volume, you can delete the created volume. For more information,
      refer to <xref href="blockstorage/verify_backend_configuration.dita">Verify Backend
        Configuration</xref>.</p>
    <note type="important"> You can create more than one VSA cluster of same or different type by
      specifying the configuration in cloud model. For more details, refer <xref
        href="blockstorage/vsa/vsa_create_multiple_clusters.dita">Modifying Cloud Model to Create
        Multiple Clusters</xref>.</note>
    <section id="deploy-vsa-with-ao-without-ao">
      <title>VSA with AO and without AO</title>
      <p>VSA is deployed with adoptive optimization (AO) and without AO. AO allows built-in storage
        tiering for VSA. While deploying VSA with or without AO you must ensure to use the
        appropriate disk input model.</p>
      <p>VSA with AO will have extra disk section with the marked usage of adoptive-optimization as
        mentioned in the following example:
        <codeblock>Additional disks can be added if available
          device_groups:
            - name: vsa-data
              consumer:
                name: vsa
                usage: data
              devices:
                - name: /dev/sdc
          - name: /dev/sdd
          - name: /dev/sde
          - name: /dev/sdf
          
            - name: vsa-cache
              consumer:
                name: vsa
                usage: <b>adaptive-optimization</b>
              devices:
                - name: /dev/sdb</codeblock></p>
      <p>VSA without AO consist of data disks as mentioned in the following example:
        <codeblock>Additional disks can be added if available
              device_groups:
                - name: vsa-data
                  consumer:
                    name: vsa
                    usage: data
                  devices:
                    - name: /dev/sdc
              - name: /dev/sdd
              - name: /dev/sde
              - name: /dev/sdf</codeblock></p>
      <p>It is recommended to use SSD disk for AO. The minimum VSA deployment with AO is 2 (apart
        from OS disk) and 1 for VSA without AO. However, the maximum supported disk is 7.</p>
      <!--, VSA will have 1-6 disks as data disks and 1 disk as VSA with AO configuration. -->
      <!--<p>VSA deployment can done with AO or without AO. AO stands for adoptive optmization which allows in-built storage tiring for VSA. 
        From the deployment perspective, deployment of VSA with or without AO is not having much difference excepy in disk input model.For example: 
        a VSA with AO will have to have extra disk section with the marked usage of adoptive-optimization mentioned below. Additional disks can 
        be added if availiable device_groups:   - name: vsa-data     consumer:       name: vsa       usage: data     devices:       
        - name: /dev/sdc - name: /dev/sdd - name: /dev/sde - name: /dev/sdf   - name: vsa-cache     consumer:       
        name: vsa       usage: adaptive-optimization     devices:       - name: /dev/sdb For VSA without AO, 
        we do need to describe only data disks as mentioned below: Additional disks can be added if availiable device_groups:   
        - name: vsa-data     consumer:       name: vsa       usage: data     devices:       - name: /dev/sdc - name: /dev/sdd 
        - name: /dev/sde - name: /dev/sdf idealy, it is recommended to use SSD disk for adoptive-optimization purpose. 
        So, a typical VSA will have 1-6 disks as data disks and 1 disks as VSA with AO configuration. From above it is clear 
        that minimum disk for VSA deployment with AO is 2 (apart from OS disk) and 1 for VSA without AO </p>-->
    </section>
  </body>
</topic>
