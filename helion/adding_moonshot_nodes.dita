<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="topic_h3s_n4t_mt">
  <title>Adding Moonshot Nodes</title>
  <body>
    <p>
      <note type="important">Moonshots are supported as compute nodes.</note>
    </p>
    <p>HP Moonshot Servers are designed and tailored for specific workloads to deliver optimum
      performance. These support a wide range of complex IT demands right from cloud based
      applications to Social Media, Big Data and Mobility. HP Moonshot Server is the worldâ€™s first
      software defined web server that will accelerate innovation while delivering breakthrough
      efficiency and scale.</p>
    <section>
      <title>Supported Moonshot Servers</title>
    </section>
    <p>The HP Moonshot Servers are now verified hardware to run HP Helion OpenStack. The supported
      Moonshot Servers are :<ul id="ul_m44_rpt_mt">
        <li>HP ProLiant m710 Server Cartridge</li>
        <li>HP ProLiant m300 Server Cartridge</li>
      </ul></p>
    <section><sectiondiv><b>Cartridge Identifier</b></sectiondiv><p>To manage HP Moonshot servers you
        must know the node address (these are also known as the <b>transit_address</b> and the
          <b>target_address</b> respectively) and the cartridge address .<ul id="ul_sk2_vpt_mt">
          <li><b>Node Address</b>
            <b>(or "-t", "target_address"):</b> Moonshot cartridges may have one or more nodes in a
            single cartridge. The first node in the cartridge has <systemoutput>0x72</systemoutput>
            as its node address.<p><b>transit_address</b> and <b>target_address</b> are the values
              that would be specified to the <systemoutput>-T</systemoutput> and
                <systemoutput>-t</systemoutput> flags of ipmitool, respectively, when using dual
              bridging mode.</p><p>If the Moonshot cartridge has only one node, the node address
              will always be <systemoutput>0x72</systemoutput>.</p><p>If it were to have 4 nodes,
              for example, the second, third and fourth nodes will have
                <systemoutput>0x74</systemoutput>, <systemoutput>0x76</systemoutput> and
                <systemoutput>0x78</systemoutput> as their addresses, respectively.</p></li>
        </ul></p><sectiondiv><b>Cartridge Address (or "-T",
        "transit_address")</b></sectiondiv><p>Below is a sample mapping between the cartridge-no and
        cartridge address. The address increases by 2 as the cartridge number increases:<ul
          id="ul_j54_hrt_mt">
          <li>C1N1 = 0x82</li>
          <li>C2N1 = 0x84</li>
          <li>C3N1 = 0x86</li>
          <li>-----</li>
          <li>C10N1 = 0x94</li>
          <li>C11N1 = 0x96</li>
          <li>-----</li>
          <li>C28N1 = 0xB8</li>
          <li>-----</li>
        </ul></p>This command returns the cartridge
      address:<codeblock>ipmitool -I lanplus -H xx.xx.xx.xx -U Administrator -P xxxxx sdr list mcloc</codeblock></section>
    <section>
      <title>Configure Moonshot Nodes</title>
      <p>To configure Moonshot Nodes, you must add the Moonshot node's configuration details in the
        following YAML files:<ul id="ul_vyc_fm1_nt">
          <li><codeph>control_plane.yml</codeph></li>
          <li><codeph>disks_compute.yml</codeph></li>
          <li><codeph>net_interfaces.yml</codeph></li>
          <li><codeph>servers.yml</codeph></li>
          <li><codeph>server_roles.yml</codeph></li>
        </ul>For each Moonshot Nodes you must provide the iLO information which is shared across
        Moonshot and a Moonshot identifier as an additional field.</p>
    </section>
    <section><sectiondiv><b>YAML Files</b></sectiondiv><p>Ensure that you modify the following YAML
        files before provisioning Moonshot nodes:<ul id="ul_qbb_ln1_nt">
          <li><!--<codeph>baremetalConfig.yml</codeph><p>The <codeph>baremetalConfig.yml</codeph> provides a detail about each of the servers in your environment. You must add the iLO information and Moonshot identifier to this file. A sample file of <codeph>baremetalConfig.yml</codeph> is shown below:<codeblock>baremetal_servers:
    - 
      node_name: Compute005
      role: ROLE-COMPUTE-MOONSHOT
      pxe_mac_addr: f0:92:1c:b4:59:30
      pxe_interface: eth0
      pxe_ip_addr: 192.168.60.20
      <b>moonshot: c39n1</b>
      <b>kopt_extras: vga=788 console=ttyS0,9600 earlyprint=serial,ttyS0,9600</b>
      ilo_ip: 10.1.196.27
      ilo_user: Administrator
      ilo_password: password</codeblock></p>In the sample file, <b>moonshot: c39n1</b> is an identifier for Moonshot node.--></li>
          <li><codeph>control_plane.yml</codeph><p>A control plane uses a server with a particular
              server-role. A sample file of <codeph>control_plane.yml</codeph> is shown
              below:<codeblock>---
  product:
    version: 2

resource-nodes:
    - name: <b>moonshot</b>
    resource-prefix: moon
    server-role: <b>ROLE-COMPUTE-MOONSHOT</b>
    service-components:
       - ntp-client
       - nova-kvm
       - nova-compute
       - neutron-l3-agent
       - neutron-metadata-agent
       - neutron-openvswitch-agent
       - neutron-lbaasv2-agent</codeblock></p><p>In
              the sample file, <b>server-role: ROLE-COMPUTE-MOONSHOT</b> identifies the role of the
              Moonshot node.</p></li>
        </ul></p></section>
    <section>
      <ul id="ul_n51_lp1_nt">
        <li><codeph>disks_compute.yml</codeph><note> The minimum size of the root device used in
            input model is 32GB. You can increase the % value so that the size is greater than 32GB.
            This is a generic requirement but is specifically relevant to moonshots since the disk
            is relatively small. </note><p>A disk model defines how local storage is to be
            configured and presented to services. Disk models are identified by a name, which can be
            specified by the user.
            <!--Since, Moonshot has only one disk, you must specify one or more disk before provisioning the Moonshot nodes-->.
            A sample file of <codeph>disks_compute.yml</codeph> is shown
            below:<codeblock>---
  product:
    version: 2

  disk-models:
  - name: <b>DISK_SET_COMPUTE_MOONSHOT</b>
    # Disk model to be used for moonshot compute nodes
    # /dev/sda_root is used as a volume group for /, /var/log and /var/crash
    # sda_root is a templated value to align with whatever partition is really used
    # This value is checked in os config and replaced by the partition actually used
    # on sda e.g. sda1 or sda5
    # /dev/sdb is used as a volume group for /var/lib (for VM storage)
    # Additional discs can be added to either volume group
    volume-groups:
    # The policy is not to consume 100% of the space of each volume group.
    # 5% should be left free for snapshots and to allow for some flexibility.
      - name: hlm-vg
        physical-volumes:
         - /dev/sda_root
        logical-volumes:
          - name: root
            size: 35%
            fstype: ext4
            mount: /
          - name: log
            size: 10%
            mount: /var/log
            fstype: ext4
            mkfs-opts: -O large_file
          - name: crash
            size: 5%
            mount: /var/crash
            fstype: ext4
            mkfs-opts: -O large_file
          - name: compute
            size: 45%
            mount: /var/lib/nova
            fstype: ext4
            mkfs-opts: -O large_file</codeblock></p><p>In
            the sample file, the name of the disk model is
          <b>DISK_SET_COMPUTE_MOONSHOT</b>.</p></li>
        <li><codeph>net_interfaces.yml</codeph><p>Interface model describes how its network
            interfaces are to be configured and used. <!--You need not modify this YAML file.--> A
            sample file of <codeph>net_interfaces.yml</codeph> is shown
            below:<codeblock>---
  product:
    version: 2

  interface-models:
      # These examples uses eth3 and eth4 as a bonded
      # pair for all networks on all three server roles
      #
      # Edit the device names and bond options
      # to match your environment
      #
    
    - name: <b>INTERFACE_SET_COMPUTE_MOONSHOT</b>
      network-interfaces:
        - name: eth1
          device:
              name: eth1
          network-groups:
            - EXTERNAL_VM
            - GUEST
            - MGMT</codeblock></p><p>In
            the sample file, the name of the interface model is
                <b><b>INTERFACE_SET_COMPUTE_MOONSHOT</b></b>.</p></li>
        <li><codeph>servers.yml</codeph><p>Servers list the available servers used for deploying the
            cloud. A sample file of <codeph>servers.yml</codeph> is shown
            below:<codeblock>---
  product:
    version: 2

  servers:
    # NOTE: Addresses of servers need to be
    #       changed to match your environment.
    #
    #       Add additional servers as required
    #
    #       nic-mapping is optional.  The value specified is the name
    #       of an entry in the nic-mappings list from nic_mappings.yml.
    #       To use, uncomment and modify to match your environment.

    
    # Compute Nodes
    - id: compute6
      ip-addr: 192.168.60.21
      role: <b>ROLE-COMPUTE-MOONSHOT</b>
    # nic-mapping: MY_2PORT_SERVER</codeblock></p><p>In
            the sample file, <b>role: ROLE-COMPUTE-MOONSHOT</b> identifies the role of the Moonshot
            node.</p><p>Moonshot server comes with only 2 NICs. One is used for PXE and another is
            used as truck 9. <note>NIC binding is not supported due to limited number of
              NICs.</note></p></li>
        <li><codeph>server_roles.yml</codeph><p>You specify the usage of the server or where you
            want to use a particular server. A sample file of <codeph>server_roles.yml</codeph> is
            shown
            below:<codeblock>---
  product:
    version: 2

  server-roles:
    
    - name: ROLE-COMPUTE-MOONSHOT
      interface-model: INTERFACE_SET_COMPUTE_MOONSHOT
      disk-model: DISK_SET_COMPUTE_MOONSHOT</codeblock></p><p>Once
            you have modified the YAML files as per the moonshot node requirement, you can proceed
            with the installation. Refer to<b> </b><xref
              href="install_entryscale_kvm_vsa.dita#install_kvm">installation guide</xref> for more
            details.</p></li>
      </ul>
      <p> </p>
    </section>
  </body>
</topic>
