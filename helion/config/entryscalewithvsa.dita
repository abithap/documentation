<?xml version="1.0" encoding="UTF-8"?>
<!--This work by HP Helion Openstack is licensed under a Creative Commons Attribution-ShareAlike 3.0 Unported License. See the accompanying LICENSE file for more information.-->
<!DOCTYPE concept PUBLIC "-//OASIS//DTD DITA Concept//EN" "concept.dtd">
<concept id="entry_scale_with_vsa">
  <title>Entry Scale with VSA</title>
  <shortdesc>This page describes the Entry Scale with VSA model and how to use the configuration
    files to achieve your cloud deployment goals.</shortdesc>
  <conbody>
    <section><title>About</title>
      <p>The location of the example files is:</p>
      <p><codeblock>~/helion/examples/entry-scale-with-vsa/</codeblock></p>
      <p>Here are shortcuts to each of the files for full descriptions and syntax:</p>
      <ul>
        <li><xref href="#entry_scale_with_vsa/cloudConfig.yml">cloudConfig.yml</xref></li>
        <li><xref href="#entry_scale_with_vsa/baremetalConfig.yml">baremetalConfig.yml</xref></li>
        <li><xref href="#entry_scale_with_vsa/control_plane.yml">control_plane.yml</xref></li>
        <li><xref href="#entry_scale_with_vsa/disks_compute.yml">disks_compute.yml</xref></li>
        <li><xref href="#entry_scale_with_vsa/disks_controller.yml">disks_controller.yml</xref></li>
        <li><xref href="#entry_scale_with_vsa/disks_vsa.yml">disks_vsa.yml</xref></li>
        <li><xref href="#entry_scale_with_vsa/net_interfaces.yml">net_interfaces.yml</xref></li>
        <li><xref href="#entry_scale_with_vsa/network_groups.yml">network_groups.yml</xref></li>
        <li><xref href="#entry_scale_with_vsa/networks.yml">networks.yml</xref></li>
        <li><xref href="#entry_scale_with_vsa/nic_mappings.yml">nic_mappings.yml</xref></li>
        <li><xref href="#entry_scale_with_vsa/server_roles.yml">server_roles.yml</xref></li>
        <li><xref href="#entry_scale_with_vsa/servers.yml">servers.yml</xref></li>
      </ul>
    </section>
    <section id="cloudConfig.yml">
      <title>cloudConfig.yml</title>
      <p>The path to this file is:</p>
      <p><codeblock>~/helion/my_cloud/definition/cloudConfig.yml</codeblock></p>
      <p>The key values to enter into this document are the values for your local NTP servers and
        DNS namesevers.</p>
      <p>Here is the sample file included with the install:</p>
      <codeblock>
---
  product:
  version: 2
  
  cloud:
  name: helion-entry-scale
  
  # The following values are used when
  # building hostnames
  hostname-data:
  host-prefix: helion
  member-prefix: -m
  
  # Path to the rest of the input model
  data-dir: data
  
  <b># List of ntp servers for your site
  ntp-servers:
  #    - "ntp-server1"
  #    - "ntp-server2"</b>
  
  <b># dns resolving configuration for your site
  # refer to resolv.conf for details on each option
  dns-settings:
  #  nameservers:
  #    - name-server1
  #    - name-server2
  #    - name-server3</b>
  #
  #  domain: sub1.example.net
  #
  #  search:
  #    - sub1.example.net
  #    - sub2.example.net
  #
  #  sortlist:
  #    - 192.168.160.0/255.255.240.0
  #    - 192.168.0.0
  #
  #  # option flags are '&lt;name&gt;:' to enable, remove to unset
  #  # options with values are '&lt;name&gt;:&lt;value&gt;' to set
  #
  #  options:
  #    debug:
  #    ndots: 2
  #    timeout: 30
  #    attempts: 5
  #    rotate:
  #    no-check-names:
  #    inet6:
</codeblock>
      <note>Ensure that you remove the <codeph>#</codeph> tags in front of your nameservers so they
        are recognized.</note>
    </section>
    <section id="baremetalConfig.yml"><title>baremetalConfig.yml</title>
      <p>The path to this file is:</p>
      <p><codeblock>~/helion/my_cloud/definition/data/baremetalConfig.yml</codeblock></p>
      <p>The key values to enter into this document are the details about each of the servers in your environment. Most of these values will be gathered from the iLO console or from your vlan setup information.</p>
      <p>Here is the sample file included with the install:</p>
      <codeblock>
---
  product:
    version: 2
  <b>baremetal_network:
    subnet: 192.168.10.0
    netmask: 255.255.255.0
    server_interface: eth2</b>
   baremetal_servers:
     -
        node_name: controller1
        node_type: ROLE-CONTROLLER
        <b>pxe_mac_addr: b2:72:8d:ac:7c:6f
        pxe_interface: eth2
        pxe_ip_addr: 192.168.10.3
        ilo_ip: 192.168.9.3
        ilo_user: admin
        ilo_password: password</b>
     -
        node_name: controller2
        node_type: ROLE-CONTROLLER
        <b>pxe_mac_addr: 8a:8e:64:55:43:76
        pxe_interface: eth2
        pxe_ip_addr: 192.168.10.4
        ilo_ip: 192.168.9.4
        ilo_user: admin
        ilo_password: password</b>
     -
        node_name: controller3
        node_type: ROLE-CONTROLLER
        <b>pxe_mac_addr: 26:67:3e:49:5a:a7
        pxe_interface: eth2
        pxe_ip_addr: 192.168.10.5
        ilo_ip: 192.168.9.5
        ilo_user: admin
        ilo_password: password</b>
     -
        node_name: compute1
        node_type: ROLE-COMPUTE
        <b>pxe_mac_addr: d6:70:c1:36:43:f7
        pxe_interface: eth2
        pxe_ip_addr: 192.168.10.6
        ilo_ip: 192.168.9.6
        ilo_user: admin
        ilo_password: password</b>
     -
        node_name: compute2
        node_type: ROLE-COMPUTE
        <b>pxe_mac_addr: 8e:8e:62:a6:ce:76
        pxe_interface: eth2
        pxe_ip_addr: 192.168.10.7
        ilo_ip: 192.168.9.7
        ilo_user: admin
        ilo_password: password</b>
     -
        node_name: compute3
        node_type: ROLE-COMPUTE
        <b>pxe_mac_addr: 22:f1:9d:ba:2c:2d
        pxe_interface: eth2
        pxe_ip_addr: 192.168.10.8
        ilo_ip: 192.168.9.8
        ilo_user: admin
        ilo_password: password</b>
     -
        node_name: vsa1
        node_type: ROLE-VSA
        <b>pxe_mac_addr: 8b:f6:9e:ca:3b:78
        pxe_interface: eth2
        pxe_ip_addr: 192.168.10.9
        ilo_ip: 192.168.9.9
        ilo_user: admin
        ilo_password: password</b>
     -
        node_name: vsa2
        node_type: ROLE-VSA
        <b>pxe_mac_addr: 8b:f6:9e:ca:3b:79
        pxe_interface: eth2
        pxe_ip_addr: 192.168.10.10
        ilo_ip: 192.168.9.10
        ilo_user: admin
        ilo_password: password</b>
     -
        node_name: vsa3
        node_type: ROLE-VSA
        <b>pxe_mac_addr: 8b:f6:9e:ca:3b:7a
        pxe_interface: eth2
        pxe_ip_addr: 192.168.10.11
        ilo_ip: 192.168.9.11
        ilo_user: admin
        ilo_password: password</b></codeblock>
    </section>
    <section id="control_plane.yml"><title>control_plane.yml</title>
      <p>The path to this file is:</p>
      <p><codeblock>~/helion/my_cloud/definition/data/control_plane.yml</codeblock></p>
      <p>The key values to enter into this document are ....</p>
      <p>Here is the sample file included with the install:</p>
      <codeblock>
---
  product:
    version: 2
        
  control-planes:
    - name: ccp
      region-name: region1
      common-service-components:
        - logging-producer
        - monasca-agent
        - stunnel
      clusters:
        - id: "1"
          name: c1
          server-role: ROLE-CONTROLLER
          member-count: 3
          service-components:
            - ntp-server
            - swift-ring-builder
            - mysql
            - ip-cluster
            - apache2
            - keystone-api
            - keystone-client
            - rabbitmq
            - glance-api
            - glance-registry
            - glance-client
            - cinder-api
            - cinder-scheduler
            - cinder-volume
            - cinder-backup
            - cinder-client
            - nova-api
            - nova-scheduler
            - nova-conductor
            - nova-console-auth
            - nova-novncproxy
            - nova-client
            - neutron-server
            - neutron-ml2-plugin
            - neutron-l3-agent
            - neutron-dhcp-agent
            - neutron-metadata-agent
            - neutron-openvswitch-agent
            - neutron-client
            - horizon
            - sherpa-api
            - swift-proxy
            - memcached
            - swift-account
            - swift-container
            - swift-object
            - swift-client
            - heat-api
            - heat-api-cfn
            - heat-api-cloudwatch
            - heat-engine
            - heat-client
            - openstack-client
            - ceilometer-api
            - ceilometer-collector
            - ceilometer-agent-central
            - ceilometer-agent-notification
            - ceilometer-expirer
            - ceilometer-common
            - ceilometer-client
            - zookeeper
            - kafka
            - vertica
            - storm
            - monasca-api
            - monasca-persister
            - monasca-notifier
            - monasca-threshold
            - monasca-client
            - logging-server
            - ops-console-web
            - ops-console-monitor
            - cmc-service
            - freezer-api
            - freezer-agent
        
        resource-nodes:
          - name: compute
            resource-prefix: comp
            server-role: ROLE-COMPUTE
            service-components:
              - ntp-client
              - nova-kvm
              - nova-compute
              - neutron-l3-agent
              - neutron-metadata-agent
              - neutron-openvswitch-agent
              - neutron-lbaasv2-agent
              - freezer-agent
        
        - name: vsa
          resource-prefix: vsa
          server-role: ROLE-VSA
          service-components:
            - vsa
            - freezer-agent</codeblock>
    </section>
    <section id="disks_compute.yml"><title>disks_compute.yml</title>
      <p>The path to this file is:</p>
      <p><codeblock>~/helion/my_cloud/definition/data/disks_compute.yml</codeblock></p>
      <p>The key values to enter into this document are ....</p>
      <p>Here is the sample file included with the install:</p>
      <codeblock>
---
  product:
    version: 2
        
  disk-models:
  - name: DISK_SET_COMPUTE
    # Disk model to be used for compute nodes
    # /dev/sda_root is used as a volume group for /, /var/log and /var/crash
    # sda_root is a templated value to align with whatever partition is really used
    # This value is checked in os config and replaced by the partition actually used
    # on sda e.g. sda1 or sda5
    # /dev/sdb is used as a volume group for /var/lib (for VM storage)
    # Additional discs can be added to either volume group
    volume-groups:
    # The policy is not to consume 100% of the space of each volume group.
    # 5% should be left free for snapshots and to allow for some flexibility.
        - name: hlm-vg
          physical-volumes:
            - /dev/sda_root
          logical-volumes:
            - name: root
              size: 35%
              fstype: ext4
              mount: /
            - name: log
              size: 50%
              mount: /var/log
              fstype: ext4
              mkfs-opts: -O large_file
            - name: crash
              size: 10%
              mount: /var/crash
              fstype: ext4
              mkfs-opts: -O large_file
        
        - name: vg-comp
          # this VG is dedicated to Nova Compute to keep VM IOPS off the OS disk
          physical-volumes:
            - /dev/sdb
          logical-volumes:
            - name: compute
              size: 95%
              mount: /var/lib/nova
              fstype: ext4
              mkfs-opts: -O large_file</codeblock>
    </section>
    <section id="disks_controller.yml"><title>disks_controller.yml</title>
      <p>The path to this file is:</p>
      <p><codeblock>~/helion/my_cloud/definition/data/disks_controller.yml</codeblock></p>
      <p>The key values to enter into this document are ....</p>
      <p>Here is the sample file included with the install:</p>
      <codeblock>
---
  product:
    version: 2
        
   disk-models:
   - name: DISK_SET_CONTROLLER
     # /dev/sda_root is used as a Volume Group for /, /var/log, and /var/crash
     # sda_root is a templated value to align with whatever partition is really used
     # This value is checked in os config and replaced by the partition actually used
     # on sda e.g. sda1 or sda5
     volume-groups:
     # The policy is not to consume 100% of the space of each volume group.
     # 5% should be left free for snapshots and to allow for some flexibility.
        - name: hlm-vg
          physical-volumes:
            - /dev/sda_root
          logical-volumes:
            - name: root
              size: 30%
              fstype: ext4
              mount: /
            - name: log
              size: 40%
              mount: /var/log
              fstype: ext4
              mkfs-opts: -O large_file
            - name: crash
              size: 10%
              mount: /var/crash
              fstype: ext4
              mkfs-opts: -O large_file
            - name: elasticsearch
              size: 10%
              mount: /var/lib/elasticsearch
              fstype: ext4
            - name: zookeeper
              size: 5%
              mount: /var/lib/zookeeper
              fstype: ext4
              consumer:
              name: os
        
      # Additional disk group defined for Swift
      # Additional disks can be added if availiable
      device-groups:
        - name: swiftobj
          devices:
            - name: /dev/sdb
            - name: /dev/sdc
          consumer:
            name: swift
            attrs:
              rings:
                - account
                - container
                - object-0</codeblock>
    </section>
    <section id="disks_vsa.yml"><title>disks_vsa.yml</title>
      <p>The path to this file is:</p>
      <p><codeblock>~/helion/my_cloud/definition/data/disks_vsa.yml</codeblock></p>
      <p>The key values to enter into this document are ....</p>
      <p>Here is the sample file included with the install:</p>
      <codeblock>
---
  product:
    version: 2

  disk-models:
  - name: DISK_SET_VSA
    # Disk model to be used for VSA nodes
    # /dev/sda_root is used as a Volume Group for /, /var/log, and /var/crash
    # sda_root is a templated value to align with whatever partition is really used
    # This value is checked in os config and replaced by the partition actually used
    # on sda e.g. sda1 or sda5
    volume-groups:
    # The policy is not to consume 100% of the space of each volume group.
    # 5% should be left free for snapshots and to allow for some flexibility.
      - name: hlm-vg
        physical-volumes:
          - /dev/sda_root
        logical-volumes:
          - name: root
            size: 30%
            fstype: ext4
            mount: /
          - name: log
            size: 45%
            mount: /var/log
            fstype: ext4
            mkfs-opts: -O large_file
          - name: crash
            size: 20%
            mount: /var/crash
            fstype: ext4
            mkfs-opts: -O large_file
         consumer:
           name: os
        
     # Disks to be used by VSA
     # Additional disks can be added if availiable
     device_groups:
       - name: vsa-data
         consumer:
           name: vsa
           usage: data
         devices:
           - name: /dev/sdc
           - name: vsa-cache
         consumer:
           name: vsa
           usage: adaptive-optimization
         devices:
           - name: /dev/sdb</codeblock>
    </section>
    <section id="net_interfaces.yml"><title>net_interfaces.yml</title>
      <p>The path to this file is:</p>
      <p><codeblock>~/helion/my_cloud/definition/data/net_interfaces.yml</codeblock></p>
      <p>The key values to enter into this document are ....</p>
      <p>Here is the sample file included with the install:</p>
      <codeblock>
---
  product:
    version: 2

  interface-models:
     # These examples uses eth3 and eth4 as a bonded
     # pair for all networks on all three server roles
     #
     # Edit the device names and bond options
     # to match your environment
     #
   - name: INTERFACE_SET_CONTROLLER
     network-interfaces:
       - name: BOND0
         device:
           name: bond0
         bond-data:
           options:
             bond-mode: active-backup
             bond-miimon: 200
           provider: linux
           devices:
             - name: eth3
             - name: eth4
         network-groups:
           - EXTERNAL_API
           - EXTERNAL_VM
           - GUEST
           - MGMT
        
   - name: INTERFACE_SET_COMPUTE
     network-interfaces:
       - name: BOND0
         device:
           name: bond0
           bond-data:
             options:
               bond-mode: active-backup
               bond-miimon: 200
           provider: linux
           devices:
              - name: eth3
              - name: eth4
          network-groups:
              - EXTERNAL_VM
              - GUEST
              - MGMT
        
    - name: INTERFACE_SET_VSA
      network-interfaces:
        - name: BOND0
          device:
             name: bond0
             bond-data:
               options:
                  bond-mode: active-backup
                  bond-miimon: 200
               provider: linux
               devices:
                  - name: eth3
                  - name: eth4
             network-groups:
               - MGMT</codeblock>
    </section>
    <section id="network_groups.yml"><title>network_groups.yml</title>
      <p>The path to this file is:</p>
      <p><codeblock>~/helion/my_cloud/definition/data/network_groups.yml</codeblock></p>
      <p>The key values to enter into this document are ....</p>
      <p>Here is the sample file included with the install:</p>
      <codeblock>
---
  product:
    version: 2
        
  network-groups:
        
    #
    # External API
    #
    # This is the network group that users will use to
    # access the public API endpoints of your cloud
    #
    - name: EXTERNAL_API
      hostname-suffix: extapi
        
      load-balancers:
        - provider: ip-cluster
        name: extlb
        # If external-name is set then public urls in keystone
        # will use this name instead of the IP address
        #external-name: mycloud.org
        components:
          - default
        roles:
          - public
        
     #
     # External VM
     #
     # This is the network group that will be used to provide
     # external access to VMs (via floating IP Addresses)
     #
     - name: EXTERNAL_VM
       tags:
        - neutron.l3_agent.external_network_bridge
        
        
     #
     # GUEST
     #
     # This is the network group that will be used to provide
     # private networks to VMs
     #
     - name: GUEST
        hostname-suffix: guest
        tags:
          - neutron.networks.vxlan
        
        
     #
     # Management
     #
     # This is the network group that will be used to for
     # management traffic within the cloud.
     #
     # The interface used by this group will be presented
     # to Neutron as physnet1, and used by provider VLANS
     #
     # In this example this group is also used for ISCSI
     # traffic between VMs and the VSA block storage
     #
     - name: MGMT
        hostname-suffix: mgmt
        hostname: true
        
        component-endpoints:
          - default
        
        routes:
          - default
        
        load-balancers:
          - provider: ip-cluster
            name: lb
            components:
              - default
            roles:
              - internal
              - admin
        
        tags:
          - neutron.networks.vlan:
             provider-physical-network: physnet1
          - vsa.iscsi
          - nova.compute.iscsi</codeblock>
    </section>
    <section id="networks.yml"><title>networks.yml</title>
      <p>The path to this file is:</p>
      <p><codeblock>~/helion/my_cloud/definition/data/networks.yml</codeblock></p>
      <p>The key values to enter into this document are ....</p>
      <p>Here is the sample file included with the install:</p>
      <codeblock>
---
  product:
    version: 2
        
  networks:
    #
    # This example uses the following networks
    #
    # Network       CIDR             VLAN
    # -------       ----             ----
    # External API  10.0.1.0/24      101 (tagged)
    # External VM   see note 1       102 (tagged)
    # Guest         10.1.1.0/24      103 (tagged)
    # Mgmt          10.2.1.0/24      100 (untagged)
    #
    # Notes:
    # 1. Defined as part of Neutron configuration
    #
    # Modify these values to match your environment
    #
    - name: NET_EXTERNAL_API
      vlanid: 101
      tagged-vlan: true
      cidr: 10.0.1.0/24
      gateway-ip: 10.0.1.1
      network-group: EXTERNAL_API
        
    - name: NET_EXTERNAL_VM
      vlanid: 102
      tagged-vlan: true
      network-group: EXTERNAL_VM
        
    - name: NET_GUEST
      vlanid: 103
      tagged-vlan: true
      cidr: 10.1.1.0/24
      gateway-ip: 10.1.1.1
      network-group: GUEST
        
    - name: NET_MGMT
      vlanid: 100
      tagged-vlan: false
      cidr: 10.2.1.0/24
      gateway-ip: 10.2.1.1
      network-group: MGMT</codeblock>
    </section>
    <section id="nic_mappings.yml"><title>nic_mappings.yml</title>
      <p>The path to this file is:</p>
      <p><codeblock>~/helion/my_cloud/definition/data/nic_mappings.yml</codeblock></p>
      <p>The key values to enter into this document are ....</p>
      <p>Here is the sample file included with the install:</p>
      <codeblock>
---
  product:
    version: 2
        
  # nic-mappings are used to ensure that the device name used by the
  # operating system always maps to the same physical device.
  # A nic-mapping is associated to a server in the server definition.
  # The logical-name specified here can be used as a device name in
  # the network interface-models definitions.
  #
  # - name               user-defined name for each mapping
  #   physical-ports     list of ports for this mapping
  #     - logical-name   device name to be used by the operating system
  #       type           physical port type
  #       bus-address    bus address of the physical device
  #
  # Notes:
  # - enclose the bus address in quotation marks so yaml does not
  #   misinterpret the embedded colon (:) characters
  # - simple-port is the only currently supported port type

  nic-mappings:
        
    - name: HP_DL360_4PORT
      physical-ports:
        - logical-name: eth1
          type: simple-port
          bus-address: "0000:07:00.0"
        
        - logical-name: eth2
          type: simple-port
          bus-address: "0000:08:00.0"
        
        - logical-name: eth3
          type: simple-port
          bus-address: "0000:09:00.0"
        
        - logical-name: eth4
          type: simple-port
          bus-address: "0000:0a:00.0"
        
    - name: MY_2PORT_SERVER
      physical-ports:
        - logical-name: eth3
          type: simple-port
          bus-address: "0000:04:00.0"
        
        - logical-name: eth4
          type: simple-port
          bus-address: "0000:04:00.1"</codeblock>
    </section>
    <section id="server_roles.yml"><title>server_roles.yml</title>
      <p>The path to this file is:</p>
      <p><codeblock>~/helion/my_cloud/definition/data/server_roles.yml</codeblock></p>
      <p>The key values to enter into this document are ....</p>
      <p>Here is the sample file included with the install:</p>
      <codeblock>
---
  product:
    version: 2
        
  server-roles:
        
    - name: ROLE-CONTROLLER
      interface-model: INTERFACE_SET_CONTROLLER
      disk-model: DISK_SET_CONTROLLER
        
    - name: ROLE-COMPUTE
      interface-model: INTERFACE_SET_COMPUTE
      disk-model: DISK_SET_COMPUTE
        
    - name: ROLE-VSA
      interface-model: INTERFACE_SET_VSA
      disk-model: DISK_SET_VSA</codeblock>
    </section>
    <section id="servers.yml"><title>servers.yml</title>
      <p>The path to this file is:</p>
      <p><codeblock>~/helion/my_cloud/definition/data/servers.yml</codeblock></p>
      <p>The key values to enter into this document are the IP addresses for each of the servers for
        your environment. They should also be in the <codeph>baremetalConfig.yml</codeph> file as
        the <codeph>pxe_ip_addr</codeph> value so you can copy them from there.</p>
      <note>The entries in this file are examples, if you have more or less nodes you can copy and
        paste to include your entire environment.</note>
      <p>Here is the sample file included with the install:</p>
      <codeblock>
---
  product:
    version: 2
        
  servers:
    # NOTE: Addresses of servers need to be
    #       changed to match your environment.
    #
    #       Add additional servers as required
    #
    #       nic-mapping is optional.  The value specified is the name
    #       of an entry in the nic-mappings list from nic_mappings.yml.
    #       To use, uncomment and modify to match your environment.
        
    # Controllers
    - id: controller1
      <b>ip-addr: 192.168.10.3</b>
      role: ROLE-CONTROLLER
    # nic-mapping: HP_DL360_4PORT
        
    - id: controller2
      <b>ip-addr: 192.168.10.4</b>
      role: ROLE-CONTROLLER
    # nic-mapping: HP_DL360_4PORT
        
    - id: controller3
      <b>ip-addr: 192.168.10.5</b>
      role: ROLE-CONTROLLER
    # nic-mapping: HP_DL360_4PORT
        
    # Compute Nodes
    - id: compute1
      <b>ip-addr: 192.168.10.6</b>
      role: ROLE-COMPUTE
    # nic-mapping: MY_2PORT_SERVER
        
    # VSA Storage Nodes
    - id: vsa1
      <b>ip-addr: 192.168.10.9</b>
      role: ROLE-VSA
    # nic-mapping: MY_2PORT_SERVER  
      
    - id: vsa2
      <b>ip-addr: 192.168.10.10</b>
      role: ROLE-VSA
    # nic-mapping: MY_2PORT_SERVER
      
    - id: vsa2
      <b>ip-addr: 192.168.10.11</b>
      role: ROLE-VSA
    # nic-mapping: MY_2PORT_SERVER</codeblock>
    </section>
  </conbody>
</concept>
