<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="example_configurations">
  <title>HP Helion <tm tmtype="reg">OpenStack</tm> 2.0: Example Configurations</title>
  <body>
    <!--Needs Edit-->
    <p>The HP Helion OpenStack 2.0 system ships with a number of pre-qualified example
      configuration. These are designed to help you to get up and running quickly with a minimum
      number of configuration changes.</p>
    <p>The HP Helion OpenStack input model allows a wide variety of configuration parameters that
      may, at first glance, appear daunting. The example configurations are designed to simplify
      this process by providing pre-built and pre-qualified examples that need only a minimum number
      of modifications to get started.</p>
    <section id="contents">
      <!--
      <sl>
        <sli><xref href="#example_configurations/example_configs"><i>1.0</i> HP Helion OpenStack
            Example Configuration</xref></sli>
        <sli outputclass="listsuba"><xref href="#example_configurations/entryscale_kvm_vsa"
              ><i>1.1</i> Entry-scale KVM with VSA model</xref></sli>
        <sli outputclass="listsuba"><xref href="#example_configurations/entryscale_esx"><i>1.2</i>
            Entry-scale ESX model</xref></sli>
        <sli outputclass="listsuba"><xref href="#example_configurations/entryscale_swift"><i>1.3</i>
            Entry-scale Swift model</xref></sli>
        <sli outputclass="listsuba"><xref href="#example_configurations/entryscale_ceph"><i>1.4</i>
            Entry-scale KVM with Ceph model</xref></sli>
        <sli outputclass="listsuba"><xref href="#example_configurations/midscale_kvm_vsa"><i>1.5</i>
            Mid-scale KVM with VSA model</xref></sli>
        <sli><xref href="#example_configurations/modify_entryscale_kvm_vsa"><i>2.0</i> Modifying the
            Entry-scale KVM with VSA model for your environment</xref></sli>
        <sli outputclass="listsuba"><xref href="#example_configurations/localizing_inputmodel"
              ><i>2.1</i> Localizing the Input Model</xref></sli>
        <sli outputclass="listsubb"><xref href="#example_configurations/networks"><i>2.1.1</i>
            Networks.yml</xref></sli>
        <sli outputclass="listsubb"><xref href="#example_configurations/nicmappings"><i>2.1.2</i>
            Nic_mappings.yml</xref></sli>
        <sli outputclass="listsubb"><xref href="#example_configurations/netinterfaces"><i>2.1.3</i>
            Net_interfaces.yml</xref></sli>
        <sli outputclass="listsubb"><xref href="#example_configurations/networkgroups"><i>2.1.4</i>
            Network_groups.yml</xref></sli>
        <sli outputclass="listsubb"><xref href="#example_configurations/servers"><i>2.1.5</i>
            Servers.yml</xref></sli>
        <sli outputclass="listsuba"><xref href="#example_configurations/customizing_inputmodel"
              ><i>2.2</i> Customizing the Input Model</xref></sli>
        <sli outputclass="listsubb"><xref href="#example_configurations/disks_controller"
              ><i>2.1.1</i> Disks_controller.yml</xref></sli>
        <sli outputclass="listsubc"><xref href="#example_configurations/filesystems"><i>2.2.1.1</i>
            File Systems Storage</xref></sli>
        <sli outputclass="listsubc"><xref href="#example_configurations/swiftstorage"><i>2.2.1.2</i>
            Swift Storage</xref></sli>
        <sli outputclass="listsubb"><xref href="#example_configurations/disks_vsa"><i>2.2.2</i>
            Disks_vsa.yml</xref></sli>
        <sli outputclass="listsubb"><xref href="#example_configurations/disks_compute"><i>2.2.3</i>
            Disks_compute.yml</xref></sli>
      </sl>
      -->
      <ul>
        <li><xref href="#example_configurations/example_configs">HP Helion OpenStack Example
            Configuration</xref>
          <ul>
            <li outputclass="listsuba"><xref href="#example_configurations/entryscale_kvm_vsa"
                  ><i>1.1</i> Entry-scale KVM with VSA model</xref></li>
            <li outputclass="listsuba"><xref href="#example_configurations/entryscale_esx">
                Entry-scale ESX model</xref></li>
            <li outputclass="listsuba"><xref href="#example_configurations/entryscale_swift">
                Entry-scale Swift model</xref></li>
            <li outputclass="listsuba"><xref href="#example_configurations/entryscale_ceph">
                Entry-scale KVM with Ceph model</xref></li>
            <li outputclass="listsuba"><xref href="#example_configurations/midscale_kvm_vsa">
                Mid-scale KVM with VSA model</xref></li>
          </ul></li>
        <li><xref href="#example_configurations/modify_entryscale_kvm_vsa">Modifying the Entry-scale
            KVM with VSA model for your environment</xref>
          <ul>
            <li outputclass="listsuba"><xref href="#example_configurations/localizing_inputmodel"
                >Localizing the Input Model</xref>
              <ul>
                <li outputclass="listsubb"><xref href="#example_configurations/networks">
                    Networks.yml</xref></li>
                <li outputclass="listsubb"><xref href="#example_configurations/nicmappings">
                    Nic_mappings.yml</xref></li>
                <li outputclass="listsubb"><xref href="#example_configurations/netinterfaces">
                    Net_interfaces.yml</xref></li>
                <li outputclass="listsubb"><xref href="#example_configurations/networkgroups">
                    Network_groups.yml</xref></li>
                <li outputclass="listsubb"><xref href="#example_configurations/servers">
                    Servers.yml</xref></li>
              </ul></li>
            <li outputclass="listsuba"><xref href="#example_configurations/customizing_inputmodel"
                >Customizing the Input Model</xref>
              <ul>
                <li outputclass="listsubb"><xref href="#example_configurations/disks_controller"
                    >Disks_controller.yml</xref>
                  <ul>
                    <li outputclass="listsubc"><xref href="#example_configurations/filesystems">
                        File Systems Storage</xref></li>
                    <li outputclass="listsubc"><xref href="#example_configurations/swiftstorage">
                        Swift Storage</xref></li>
                  </ul></li>
                <li outputclass="listsubb"><xref href="#example_configurations/disks_vsa">
                    Disks_vsa.yml</xref></li>
                <li outputclass="listsubb"><xref href="#example_configurations/disks_compute">
                    Disks_compute.yml</xref></li>
              </ul></li>
          </ul></li>
      </ul>
    </section>
    <section id="example_configs"><title><i>1.0</i> HP Helion OpenStack Example
        Configurations</title>
      <p>This section briefly describes the various example configuration and their capabilities. It
        also describes in detail, for the entry-scale-kvm-vsa example, how you may adapt the input
        model to work in your environment.</p>
      <p>HP Helion OpenStack 2.0 ships with two classes of sample cloud models: examples and
        tech-preview. The models in the examples directory have been qualified by our Quality
        Engineering team, while the tech-preview models are more experimental.</p>
      <p>The following pre-qualified examples are shipped with HP Helion OpenStack 2.0:</p>
      <ul>
        <li>Entry-scale KVM with VSA model (entry-scale-kvm-vsa)</li>
        <li>Entry-scale Swift Only model (entry-scale-swift)</li>
        <li>Entry-scale KVM with Ceph model (entry-scale-kvm-ceph)</li>
        <li>Entry-scale ESX model (entry-scale-esx)</li>
      </ul>
      <p>These systems are designed to provide an entry-level solution that can be scaled from a
        small number of nodes to a moderately high node count (approximately 100 compute nodes, for
        example).</p>
      <p>The tech-preview configuration include the following model:</p>
      <ul>
        <li>Mid-scale KVM with VSA model (mid-scale-kvm-vsa): In this model, the cloud control plane
          is subdivided into a number of dedicated service clusters to provide more processing power
          for individual control plane elements. This enables a greater number of resources to be
          supported (compute nodes, Swift object servers). This model also shows how a segmented
          network can be expressed in the HP Helion OpenStack model.</li>
      </ul>
    </section>
    <section id="entryscale_kvm_vsa"><title><i>1.1</i> Entry-scale KVM with VSA model</title>
      <p>This model provides a KVM-based cloud with VSA for volume storage, and has been tested to a
        scale of 100 compute nodes.</p>
      <p>The example is focused on the minimum server count to support a highly-available (HA)
        compute cloud deployment. The first (manually installed) server, often referred to as the
        deployer, is also used as one of the controller nodes. This model consists of a minimum
        server count of seven, with three controllers, three VSA storage servers, and one compute
        server. Swift storage in this example is contained on the controllers.</p>
      <p>Note that the VSA storage requires a minimum of three servers for a HA configuration,
        although the deployment will work with as little as one VSA node.</p>
      <p>This model can also be deployed without the VSA servers and configured to use an external
        storage device, such as a 3PAR array, which would reduce the minimum server count to
        four.</p>
      <!-- MEDIA DIAGRAM GOES HERE -->
      <p>The example requires the following networks:</p>
      <ul>
        <li><b>External API</b> - This is the network that users will use to make requests to the
          cloud.</li>
        <li><b>External VM</b> - This is the network that will be used to provide access to virtual
          machines (via floating IP addresses)</li>
        <li><b>Guest/VxLAN</b> - This is the network that will carry traffic between virtual
          machines on private networks within the cloud</li>
        <li><b>Management</b> - This is the netwrok that will be used for all internal traffic
          between the cloud services, including node provisioning. This network must be on an
          untagged VLAN.</li>
      </ul>
      <p>All of these networks are configured to be presented via a pair of bonded NICs. The example
        also enables provider VLANs to be configured in Neutron on this interface.</p>
      <p>In the diagram, "External Routing" refers to whatever routing you want to provide to allow
        users to access the External API and External VM networks. Note that the EXTERNAL_API
        network must be reachable from the EXTERNAL_VM network if you want virtual machines to be
        able to make API calls to the cloud. "Internal Routing" refers to whatever routing you want
        to provide to allow administrators to access the Management network.</p>
      <p>If you are using HP Helion OpenStack to install the operating system, then an IPMI/iLO
        network connected to the IPMI/iLO ports of all servers and routable from the deployer server
        is also required for BIOS and power management of the nodes during the operating system
        installation process.</p>
      <p>The example uses the following disk configurations:</p>
      <ul>
        <li><b>Controllers</b> - One operating system disk and two disks for Swift storage</li>
        <li><b>VSA</b> - one operating system disk and two disks for VSA storage</li>
        <li><b>Compute</b> - one operating system disk and one disk for virtual machine ephemeral
          storage</li>
      </ul>
      <xref href="#example_configurations/modify_entryscale_kvm_vsa">Modifying the Entry-scale KVM
        with VSA model for your Environment</xref> describes in detail how to modify this example to
      match your environment. </section>
    <section id="entryscale_esx"><title>1.2 Entry-scale ESX model</title>
      <p>This example shows how to integrate HP Helion OpenStack with ESX. The controller
        configuration is essentially the same as in the KVM example, but the resource nodes are
        provided by vCenter. In addition, a number of controller virtual machines are created for
        each vCenter cluster; one ESX Compute virtual machine (which provides the nova-compute proxy
        for vCenter) and one OVSvApp virtual machine per cluster member (which provides network
        access). These virtual machines are created automatically by HP Helion OpenStack as part of
        activating the vCenter cluster, and are therefore not defined in the example.</p>
      <!-- MEDIA DIAGRAM GOES HERE -->
      <p>The physical networking configuration is also largely the same as the KVM example, with the
        exception of the GUEST network which uses tenant VLANs as the Neutron networking model
        rather than VxLAN.</p>
      <p>A separate configuration network (CONF) is required for configuration access from the
        deployer. This network must be reachable from the Management network.</p>
    </section>
    <section id="entryscale_swift"><title>1.3 Entry-scale Swift model</title>
      <p>This example shows how HP Helion OpenStack can be configured to provide a Swift only
        configuration, consisting of three controllers and one or more Swift object servers.</p>
      <!-- MEDIA DIAGRAM GOES HERE -->
      <p>The example requires the following networks:</p>
      <ul>
        <li><b>External API</b> - This is the network that users will use to make requests to the
          cloud</li>
        <li><b>Swift</b> - This is the network that will be used for all data traffic between the
          Swift services</li>
        <li><b>Management</b> - This is the network that will be used for all internal traffic
          between the cloud services, including node provisioning. This network must be on an
          untagged VLAN.</li>
      </ul>
      <p>All of these networks are configured to be presented via a pair of Bonded NICs. The example
        also enables provider VLANs to be configured in Neutron on this interface.</p>
      <p>In the diagram "External Routing" refers to whatever routing you want to provide to allow
        users to access the External API. "Internal Routing" refers to whatever routing you want to
        provide to allow administrators to access the Management network.</p>
      <p>If you are using HP Helion OpenStack to install the operating system then an IPMI/iLO
        network connected to the IPMI/iLO ports of all servers and routable from the deployer is
        also required for BIOS and power management of the node during the operating system
        installation process.</p>
      <p>In the example the controllers use one disk for the operating system and two disks for
        Swift Proxy and Account storage. The Swift object servers use one disk for the operating
        system and four disks for swift storage. These values can be modified to suit your
        environment.</p>
    </section>
    <section id="entryscale_ceph"><title>1.4 Entry-scale KVM with Ceph model</title>
      <p>This example provides a KVM based cloud using Ceph for volume storage. This controller and
        compute configuration are essentially the same as in the KVM example, but the backend Cinder
        storage is provided by Ceph.</p>
      <!-- MEDIA DIAGRAM GOES HERE -->
    </section>
    <section id="midscale_kvm_vsa"><title>1.5 Mid-scale KVM with VSA model (Tech Preview)</title>
      <p>The mid-scale model, which is included as a technology preview in HP Helion OpenStack 2.0,
        illustrates two important aspects of configuring HP Helion OpenStack for increased scale.
        The controller services are distributed across a greater number of controllers and a number
        of the networks are configured as multiple L3 segments (implementing per-rack
        networking).</p>
      <!-- MEDIA DIAGRAM GOES HERE -->
      <p>The distribution of services across controllers is only one possible configuration, and
        other combinations can also be expressed.</p>
    </section>
    <section id="modify_entryscale_kvm_vsa"><title><i>2.0</i> Modifying the Entry-scale KVM with VSA
        model for your Environment</title>
      <p>In this section we will look at the changes that need to be made to the input model in
        order to deploy and run this cloud model in your environment.</p>
      <p>This section is written from the perspective of the <codeph>entry-scale-kvm-vsa</codeph>
        example, although the same principles apply to all of the examples.</p>
      <p>There are two categories of modifications that we will look at:</p>
      <ol>
        <li><b>Localizations</b> - These are the minimum set of changes that you need to make to
          adapt the examples to run in your environment. These are mostly concerned with
          networking.</li>
        <li><b>Customizations</b> - These describe more general changes that you may wish to make to
          your model, e.g. changing disk storage layouts.</li>
      </ol>
      <p>Note that as a convention the examples use upper case for the object names, but these
        strings are only used to define the relationships between objects and have no specific
        significance to the configuration processor. You can change the names to values that are
        relevant to your context providing you do so consistently across the input model.</p>
    </section>
    <section id="localizing_inputmodel"><title><i>2.1</i> Localizing the Input Model</title>
      <p>In this section we look at the minimum set of changes needed to localize the cloud for your
        environment. This assumes you are using other features of the example unchanged.</p>
      <ul>
        <li>Update <codeph>networks.yml</codeph> to specify the network addresses (VLAN IDs and CIDR
          values) for your cloud</li>
        <li>Update <codeph>nic_mappings.yml</codeph> to specify the PCI bus information for your
          servers ethernet devices</li>
        <li>Update <codeph>net_interfaces.yml</codeph> to provide network interface configuration,
          such as bond settings and bond devices</li>
        <li>Update <codeph>network_groups.yml</codeph> to provide the public URL for your cloud and
          to provide security certificates</li>
        <li>Update <codeph>servers.yml</codeph> to provide information on your servers</li>
      </ul>
    </section>
    <section id="networks">
      <title><i>2.1.1</i> Networks.yml</title>
      <p>You will need to allocate site specific CIDRs and VLANs for these networks and update these
        values in the <codeph>networks.yml</codeph> file. The example models define the following
        networks:</p>
      <table frame="all" rowsep="1" colsep="1" id="table_xrn_zzy_st">
        <tgroup cols="4">
          <colspec colname="c1" colnum="1"/>
          <colspec colname="c2" colnum="2"/>
          <colspec colname="c3" colnum="3"/>
          <colspec colname="c4" colnum="4"/>
          <thead>
            <row>
              <entry>Network</entry>
              <entry>CIDR</entry>
              <entry>VLAN ID</entry>
              <entry>Tagged / Untagged</entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry>External API</entry>
              <entry>10.0.1.0/24</entry>
              <entry>101</entry>
              <entry>Tagged</entry>
            </row>
            <row>
              <entry>External VM</entry>
              <entry>Addresses configured by Neutron, leave blank in the file</entry>
              <entry>102</entry>
              <entry>Tagged</entry>
            </row>
            <row>
              <entry>Guest</entry>
              <entry>10.1.1.0/24</entry>
              <entry>103</entry>
              <entry>Tagged</entry>
            </row>
            <row>
              <entry>Management</entry>
              <entry>192.168.10.0/24</entry>
              <entry>100</entry>
              <entry>Un-tagged</entry>
            </row>
          </tbody>
        </tgroup>
      </table>
      <p>You will need to edit this file and provide your local values for these networks.</p>
      <p>The CIDR for the External VM network is configured separately using the Neutron API (see
            <i><xref href="administration/create_extnet.dita">Creating an External
          Network</xref></i>. You will only specify its VLAN ID during the installation process.</p>
      <p>The Management network is shown as untagged. This is required if you are using this network
        to PXE install the operating system on the cloud nodes.</p>
      <p>The example <codeph>networks.yml</codeph> file is shown below, modify the bolded fields to
        reflect your site values.</p>
      <codeblock>networks:
   #
   # This example uses the following networks
   #
   # Network       CIDR             VLAN
   # -------       ----             ----
   # External API  10.0.1.0/24      101 (tagged)
   # External VM   see note 1       102 (tagged)
   # Guest         10.1.1.0/24      103 (tagged)
   # Management    192.168.10.0/24  100 (untagged)
   #	
   # Notes:
   # 1. Defined as part of Neutron configuration
   #
   # Modify these values to match your environment
   #
   - name: EXTERNAL-API-NET
     vlanid: <b>101</b>
     tagged-vlan: true
     cidr: <b>10.0.1.0/24</b>
     gateway-ip: <b>10.0.1.1</b>
     network-group: EXTERNAL-API
        
   - name: EXTERNAL-VM-NET
     vlanid: <b>102</b>
     tagged-vlan: true
     network-group: EXTERNAL-VM
        
   - name: GUEST-NET
     vlanid: <b>103</b>
     tagged-vlan: true
     cidr: <b>10.1.1.0/24</b>
     gateway-ip: <b>10.1.1.1</b>
     network-group: GUEST
        
   - name: MANAGEMENT-NET
     vlanid: 100
     tagged-vlan: false
     cidr: <b>192.168.10.0/24</b>
     gateway-ip: <b>192.168.10.1</b>
     network-group: MANAGEMENT</codeblock>
    </section>
    <section id="nicmappings"><title><i>2.1.2</i> Nic_mappings.yml</title>
      <p>This file maps ethernet port names to specific bus slots. Due to inherent race conditions
        associated with multiple PCI device discovery there is no guarantee that ethernet devices
        will be named as expected by the operating system, and it is possible that different port
        naming will exist on different servers with the same physical configuration.</p>
      <p>In order to provide a deterministic naming pattern, the input model supports an explicit
        mapping from PCI bus address to a user specified name. HP Helion OpenStack uses the prefix
        "hed" (Helion Ethernet Device) to name such devices to avoid any name clashes with the "eth"
        names assigned by the operating system.</p>
      <p>The example <codeph>nic_mappings.yml</codeph> file is shown below.</p>
      <codeblock>nic-mappings:
        
   - name: HP-DL360-4PORT
     physical-ports:
       - logical-name: hed1
         type: simple-port
         bus-address: "0000:07:00.0"
        
        - logical-name: hed2
          type: simple-port
          bus-address: "0000:08:00.0"
        
        - logical-name: hed3
          type: simple-port
          bus-address: "0000:09:00.0"
        
        - logical-name: hed4
          type: simple-port
          bus-address: "0000:0a:00.0"
        
    - name: MY-2PORT-SERVER
      physical-ports:
        - logical-name: hed3
          type: simple-port
          bus-address: "0000:04:00.0"
        
        - logical-name: hed4
          type: simple-port
          bus-address: "0000:04:00.1"</codeblock>
      <p>This defines two sets of NIC mappings, representing two different physical server types.
        The name of each mapping is used as a value in the <codeph>servers.yml</codeph> file to
        associate each server with its required mapping. This enables the use of different server
        models or servers with different network hardware.</p>
      <p>Each mapping lists a set of ports with the following information:</p>
      <ul>
        <li><b>Logical name</b> - we use the form <codeph>hedN</codeph></li>
        <li><b>Type</b> - only simple-port types are supported in HP Helion OpenStack 2.0</li>
        <li><b>Bus-address</b> - the PIC bus address of the port</li>
      </ul>
      <p>The PCI bus address can be found using the <codeph>lspci</codeph> command on one of the
        servers. This command can produce a lot of output, so you can use the following command
        which will limit the output to list ethernet class devices only:</p>
      <codeblock>sudo lspci -D |grep -i eth</codeblock>
      <p>Here is an example output:</p>
      <codeblock>$ sudo lspci -D |grep -i eth
0000:02:00.0 Ethernet controller: Broadcom Corporation NetXtreme BCM5719 Gigabit Ethernet PCIe (rev 01)
0000:02:00.1 Ethernet controller: Broadcom Corporation NetXtreme BCM5719 Gigabit Ethernet PCIe (rev 01)
0000:02:00.2 Ethernet controller: Broadcom Corporation NetXtreme BCM5719 Gigabit Ethernet PCIe (rev 01)
0000:02:00.3 Ethernet controller: Broadcom Corporation NetXtreme BCM5719 Gigabit Ethernet PCIe (rev 01)
0000:04:00.0 Ethernet controller: Intel Corporation 82599ES 10-Gigabit SFI/SFP+ Network Connection (rev 01)
0000:04:00.1 Ethernet controller: Intel Corporation 82599ES 10-Gigabit SFI/SFP+ Network Connection (rev 01)</codeblock>
      <p>To localize this file, replace the mapping name(s) with the names of your choice and
        enumerate the ports as required.</p>
    </section>
    <section id="netinterfaces"><title><i>2.1.3</i> Net_interfaces.yml</title>
      <p>This file is used to fine how the network interfaces are to be configured. The example
        reflects the slightly different configuration of controller, compute nodes, and VSA
        nodes.</p>
      <p>If network bonding is to be used, this file specifies how bonding is to be set up. It also
        specifies which networks are to be associated with each interface.</p>
      <p>In the example we use a bond of interfaces <codeph>hed3</codeph> and <codeph>hed4</codeph>.
        You only need to modify this file if you have mapped your physical ports to different names,
        or if you need to modify the bond options.</p>
      <p>The section of configuration file is shown below, which will create a bonded interface
        using the named <codeph>hed3</codeph> and <codeph>hed4</codeph> NIC mappings described in
        the previous section.</p>
      <codeblock>
    - name: CONTROLLER-INTERFACES
      network-interfaces:
        - name: BOND0
          device:
              name: bond0
          bond-data:
              options:
                  mode: active-backup
                  miimon: 200
                  primary: hed3
          provider: linux
          devices:
              - name: hed3
              - name: hed4
          network-groups:
            - EXTERNAL-API
            - EXTERNAL-VM
            - GUEST
            - MANAGEMENT</codeblock>
      <p>If your system cannot support bonding then you can modify this specification to specify a
        non-bonded interface, for example using device <codeph>hed3</codeph>:</p>
      <codeblock>
   - name: CONTROLLER-INTERFACES
     network-interfaces:
        - name: hed3
          device:
             name: hed3
          network-groups:
             - EXTERNAL-API
             - EXTERNAL-VM
             - GUEST
             - MANAGEMENT</codeblock>
    </section>
    <section id="networkgroups"><title><i>2.1.4</i> Network_groups.yml</title>
      <p>This file defines the networks groups used in your cloud. A network-group defines the
        traffic separation model, and all of the properties that are common to the set of L3
        networks that carry each type of traffic. They define where services and load balancers are
        attached to the network model and the routing within that model.</p>
      <p>In this example, the following network groups are defined:</p>
      <ul>
        <li><b>EXTERNAL-API</b> - this network group is used for external IP traffic to the cloud.
          In addition, it defines: <ul>
            <li>The characteristics of the load balancer to be used for the external API</li>
            <li>The Transport Layer Security (TLS) attributes</li>
          </ul></li>
        <li><b>EXTERNAL-VM</b> - Floating IPs for virtual machines are created on this network
          group. This is identified by the tag value
            <codeph>neutron.l3_agent.external_network_bridge</codeph>.</li>
        <li><b>GUEST</b> - Tenant VxLAN traffic is carried on this network group. This is identified
          by the tag value <codeph>neutron.networks.vxlan</codeph>.</li>
        <li><b>MANAGEMENT</b> - This is the default network group for traffic between service
          components in the cloud. In addition, it defines: <ul>
            <li>An internal load balancer is defined on this network group for managing internal and
              administrative API requests.</li>
          </ul></li>
      </ul>
      <p>Most of the values in this file should be left unmodified if you are using the network
        model defined by the example. More complex modifications are supported but are outside the
        scope of this document.</p>
      <p>However, the values related to the external API network are site specific and need to be
        modified:</p>
      <ul>
        <li>Provide an external URL for the cloud</li>
        <li>Provide the name of the security certificate to use</li>
      </ul>
      <p>The example <codeph>network_groups.yml</codeph> file is shown below, modify the bolded
        fields to reflect your site values.</p>
      <codeblock>
   # External API
   #
   # This is the network group that users will use to
   # access the public API endpoints of your cloud
   #
   - name: EXTERNAL-API
     hostname-suffix: extapi
        
     load-balancers:
       - provider: ip-cluster
         name: extlb
         # If external-name is set then public urls in keystone
         # will use this name instead of the IP address
         # You must either set this to a name that can be resolved
         # in your network 
         # or comment out this line to use IP addresses
         <b>external-name</b>:
        
         <b>tls-components</b>:
            - default
         roles:
            - public
         <b>cert-file: my-public-cert</b></codeblock>
      <p>Here is a description of each of the above bolded sections:</p>
      <p><b>external-name</b></p>
      <p>The external name defines how the public URLs will be registered in Keystone. Users of your
        cloud will need to be able to resolve this URL to access the cloud APIs, and if you are
        using the TLS the name must match the certificate used.</p>
      <p>Because this value is difficult to change after initial deployment, this value is left
        blank in the supplied example which prevents the configuration processor from running until
        a value has been supplied. If you want to register the public URLs as IP addresses instead
        of a name then you can comment out this line.</p>
      <p><b>cert-file</b></p>
      <p>Provide the name of the file located in
          <codeph>~/helion/my_cloud/config/tls/certs/</codeph> that will be used for your cloud
        endpoints. As shown above, this can be either a single certificate for all endpoints or a
        default certificate file and a set of service specific certificate files.</p>
      <p><b>tls-components</b></p>
      <p>If you do not want to use a TLS for the public URLs then change the entry that says
        "tls-components" to "components".</p>
    </section>
    <section id="servers"><title><i>2.1.5</i> Servers.yml</title>
      <p>This file where you provide the details of the physical servers that make up your cloud.
        There are two sections to this file:</p>
      <codeblock>
   baremetal:
      # NOTE: These values need to be changed to match your environment.
      # Define the network range that contains the ip-addr values for
      # the individual servers listed below.
      subnet: <b>192.168.10.0</b>
      netmask: <b>255.255.255.0</b></codeblock>
      <p>The two values in this section are used to configure cobbler for operating system
        installation and must match the network values for the addresses given for the servers.</p>
      <p>The servers section below provides the details of each individual server. For example, here
        are the details for the first controller:</p>
      <codeblock>
   servers:
        
      # Controllers
      - id: controller1
        ip-addr: <b>192.168.10.3</b>
        role: CONTROLLER-ROLE
        server-group: RACK1
        nic-mapping: <b>HP-DL360-4PORT</b>
        mac-addr: <b>b2:72:8d:ac:7c:6f</b>
        ilo-ip: <b>192.168.9.3</b>
        ilo-password: <b>password</b>
        ilo-user: <b>admin</b></codeblock>
      <p>Here is a description of each of the above bolded sections:</p>
      <p><b>id</b></p>
      <p><codeph>id</codeph> is a name you provide to uniquely identify a server. This can be any
        string which is makes sense in your context, such as an asset tag, descriptive name, etc.
        The system will use this value to remember how the server has been allocated.</p>
      <p><b>ip-addr</b></p>
      <p><codeph>ip-addr</codeph> is the IP address that the system will use for SSH connections to
        the server for deployment and configuration changes. This address must be in the IP range of
        one of the networks in the model. In the example, the servers are provided with addresses
        from the MANAGEMENT network.</p>
      <p><b>role</b></p>
      <p><codeph>role</codeph> is a string which refers to an entry in
          <codeph>server_roles.yml</codeph> that tells the system how to configure the disks and
        network interfaces for this server. Roles are also used to define which servers can be used
        for specific purposes. Adding and changing roles is beyond the scope of this walkthrough -
        for more details refer to <xref href="input_model.dita">HPE Helion OpenStack 2.0 Input
          Model</xref>.</p>
      <p><b>server-group</b></p>
      <p><codeph>server-group</codeph> tells the system how this server is physically related to
        networks and other servers. Server groups are used to ensure that servers in a cluster are
        selected from different physical groups. The example provides a set of server groups that
        divide the servers into three sets called "RACK1", "RACK2", and "RACK3". Modifying the
        server group structure is beyond the scope of this walkthrough - for more details refer to
          <xref href="input_model.dita">HPE Helion OpenStack 2.0 Input Model</xref>.</p>
      <p><b>nic-mapping</b></p>
      <p><codeph>nic-mapping</codeph> is the name of a network port mapping definition (see <xref
          href="#example_configurations/nicmappings">Nic_mappings.yml</xref>). You need to set this
        to the mapping that corresponds to this server.</p>
      <p><b>mac-addr</b></p>
      <p><codeph>mac-addr</codeph> is the MAC address of the interface associated with this server
        that will be used for PXE boot.</p>
      <p><b>ilo-ip</b></p>
      <p><codeph>ilo-ip</codeph> is the IP address of the iLO or IPMI port for this server.</p>
      <p><b>ilo-user and ilo-password</b></p>
      <p><codeph>ilo-user</codeph> and <codeph>ilo-password</codeph> are the login details used to
        access the iLO or IPMI port of this server. The iLO password value can be provided as an
        OpenSSL encrypted string - refer to the <xref
          href="installation/install_entryscale_kvm.dita#install_kvm/configuration">installation
          guide</xref> for details on how to generate encrypted passwords.</p>
    </section>
    <section id="customizing_inputmodel"><title><i>2.2</i> Customizing the Input Model</title>
      <p>In this section we look at additional changes that you can make to further adapt the
        example to your environment.</p>
      <ul>
        <li>Update <codeph>disks_controller.yml</codeph> to add additional disk capacity to your
          controllers</li>
        <li>Update <codeph>disks_vsa.yml</codeph> to add additional disk capacity to your VSA
          servers</li>
        <li>Update <codeph>disks_compute.yml</codeph> to add additional disk capacity to your
          compute servers</li>
      </ul>
    </section>
    <section id="disks_controller"><title><i>2.2.1</i> Disks_controller.yml</title>
      <p>The disk configuration of the controllers consists of two sections; a definition of a
        volume group that provides a number of file-systems for various subsystems and device-group
        that provides disk capacity for Swift.</p>
    </section>
    <section id="filesystems"><title><i>2.2.1.1</i> File Systems Storage</title>
      <p> The root volume group (hlm-vg) is divided into a number of logical volumes that provide
        separate file systems for the various services that are co-hosted on the controllers in the
        entry-scale examples. The capacity of each file system is expressed as a percentage of the
        overall volume group capacity. Because not all file system usage scales linearly, two
        different disk configurations are provided:</p>
      <ul>
        <li><b>CONTROLLER-DISKS</b> is based on a 512GB root volume group</li>
        <li><b>CONTROLLER-1TB-DISKS</b> provides a higher percentage of space for the logging
          service</li>
      </ul>
      <p>As supplied, the example uses the smaller disk model. To use the larger disk model you need
        to modify the <codeph>disk-models</codeph> parameter in the
          <codeph>server_roles.yml</codeph> file, as shown below:</p>
      <codeblock>
    server-roles:
        
       - name: CONTROLLER-ROLE
         interface-model: CONTROLLER-INTERFACES
         disk-model: CONTROLLER-1TB-DISKS</codeblock>
      <p>To add additional disks to the root volume group you need to modify the volume group
        definition in whichever disk model you are using. The following exaple shows adding an
        additional disk, <codeph>/dev/sdd</codeph> to the <codeph>disks_controller.yml</codeph>
        file:</p>
      <codeblock>
   disk-models:
      - name: CONTROLLER-DISKS
        
        volume-groups:
         - name: hlm-vg
           physical-volumes:
        
              # NOTE: 'sda_root' is a templated value. This value is checked in
              # os-config and replaced by the partition actually used on sda
              #e.g. sda1 or sda5
              - /dev/sda_root
              <b>- /dev/sdd</b></codeblock>
    </section>
    <section id="swiftstorage"><title><i>2.2.1.2</i> Swift Storage</title>
      <p>Swift storage is configured as a device-group and has a syntax that allows disks to be
        allocated to specific rings. In the examplem two disks are allocated to Swift to be shared
        by the account, container, and object-0 rings.</p>
      <codeblock>
   device-groups:
       - name: swiftobj
         devices:
            - name: /dev/sdb
            - name: /dev/sdc
            # Add any additional disks for swift here
            # -name: /dev/sdd
            # -name: /dev/sde
         consumer:
           name: swift
           attrs:
              rings:
                 - account
                 - container
                       - object-0</codeblock>
      <p>Refer to the <xref href="objectstorage/allocating_disk_drives.dita">Allocating Disk
          Drives</xref> document for details of how to configure additional Swift Storage.</p>
    </section>
    <section id="disks_vsa"><title><i>2.2.2</i> Disks_vsa.yml</title>
      <p>VSA storage is configured as a device-group and has a syntax that allows disks to be
        allocated for data storage or for adaptive optimization (caching). Disks used for adaptive
        optimization should be solid state drives (SSD). The example disk configuration for VSA
        nodes has two disks, one for data and one of adaptive optimization.</p>
      <codeblock>
   device-groups:
       - name: vsa-data
         consumer:
           name: vsa
           usage: data
         devices:
           - name: /dev/sdc
       - name: vsa-cache
         consumer:
           name: vsa
           usage: adaptive-optimization
         devices:
            - name: /dev/sdb</codeblock>
      <p>Additional capacity can be added by adding more disks to the <codeph>vsa-data</codeph>
        device group. Similarly, caching capacity can be increased by adding more high speed storage
        devices to the <codeph>vsa-cache</codeph> device group.</p>
    </section>
    <section id="disks_compute"><title><i>2.2.3</i> Disks_compute.yml</title>
      <p>The example disk configuration for compute nodes consists of two volume groups; one for the
        operating system and one for the ephemeral storage for virtual machines, with one disk
        allocated to each.</p>
      <p>Additional virtual machine ephemeral storage capacity can be configured by adding
        additional disks to the <codeph>vg-comp</codeph> volume group. The following example shows
        the addition of two more disks, <codeph>/dev/sdc</codeph> and <codeph>/dev/sdd</codeph> to
        the <codeph>disks_compute.yml</codeph> file:</p>
      <codeblock>
   - name: vg-comp
        physical-volumes:
          - /dev/sdb
          - /dev/sdc
          - /dev/sdd
        logical-volumes:
          - name: compute
            size: 95%
            mount: /var/lib/nova
            fstype: ext4
                  mkfs-opts: -O large_file</codeblock>
    </section>
  </body>
</topic>
