<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="example_configurations">
  <title>Helion <tm tmtype="reg">OpenStack</tm> 2.0: Example Configurations</title>
  <body>
    <section id="2.0_example_configs"><title><i>2.0</i> HP Helion OpenStack Example
        Configurations</title>
      <p>The HP Helion OpenStack 2.0 system ships with a number of pre-qualified example
        configuration. These are designed to help you to get up and running quickly with a minimum
        number of configuration changes.</p>
      <p>The HP Helion OpenStack input model allows a wide variety of configuration parameters that
        may, at first glance, appear daunting. The example configurations are designed to simplify
        this process by providing pre-built and pre-qualified examples that need only a minimum
        number of modifications to get started.</p>
      <p>This section briefly describes the various example configuration and their capabilities. It
        also describes in detail (for one of the examples, entry-scale-kvm-vsa), how you may adapt
        the input model to work in your environment.</p>
      <p>HP Helion OpenStack 2.0 ships with two classes of sample cloud models: examples and
        tech-preview. The models in the examples directory have been qualified by our Quality
        Engineering team, whilst the tech-preview models are more experimental.</p>
      <p>The following pre-qualified examples are shipped with HP Helion OpenStack 2.0:</p>
      <ul>
        <li>Entry-scale KVM with VSA model (entry-scale-kvm-vsa)</li>
        <li>Entry-scale Swift Only model (entry-scale-swift)</li>
        <li>Entry-scale KVM with Ceph model (entry-scale-kvm-ceph)</li>
        <li>Entry-scale ESX model (entry-scale-esx)</li>
      </ul>
      <p>These systems are designed to provide an entry-level solution that can be scaled from a
        small number of nodes to a moderately high node count (approximately 100 compute nodes, for
        example).</p>
      <p>The tech-preview configuration include the following model:</p>
      <ul>
        <li>Mid-scale KVM with VSA model (mid-scale-kvm-vsa): In this model, the cloud control plane
          is subdivided into a number of dedicated service clusters to provide more processing power
          for individual control plane elements. This enables a greater number of resources to be
          supported (compute nodes, Swift object servers). This model also shows how a segmented
          network can be expressed in the HP Helion OpenStack model.</li>
      </ul>
    </section>
    <section id="2.1_entryscale_kvm_vsa"><title><i>2.1</i> Entry-scale KVM with VSA model</title>
      <p>This model provides a KVM-based cloud with VSA for volume storage, and has been tested to a
        scale of 100 compute nodes.</p>
      <p>The example is focused on the minimum server count to support a highly-available (HA)
        compute cloud deployment. The first (manually installed) server, often referred to as the
        deployer, is also used as one of the controller nodes. This model consists of a minimum
        server count of seven, with three controllers, three VSA storage servers, and one compute
        server. Swift storage in this example is contained on the controllers.</p>
      <p>Note that the VSA storage requires a minimum of three servers for a HA configuration,
        although the deployment will work with as little as one VSA node.</p>
      <p>This model can also be deployed without the VSA servers and configured to use an external
        storage device, such as a 3PAR array, which would reduce the minimum server count to
        four.</p>
      <!-- MEDIA DIAGRAM GOES HERE -->
      <p>The example requires the following networks:</p>
      <ul>
        <li><b>External API</b> - This is the network that users will use to make requests to the
          cloud.</li>
        <li><b>External VM</b> - This is the network that will be used to provide access to virtual
          machines (via floating IP addresses)</li>
        <li><b>Guest/VxLAN</b> - This is the network that will carry traffic between virtual
          machines on private networks within the cloud</li>
        <li><b>Management</b> - This is the netwrok that will be used for all internal traffic
          between the cloud services, including node provisioning. This network must be on an
          untagged VLAN.</li>
      </ul>
      <p>All of these networks are configured to be presented via a pair of bonded NICs. The example
        also enables provider VLANs to be configured in Neutron on this interface.</p>
      <p>In the diagram, "External Routing" refers to whatever routing you want to provide to allow
        users to access the External API and External VM networks. Note that the EXTERNAL_API
        network must be reachable from the EXTERNAL_VM network if you want virtual machines to be
        able to make API calls to the cloud. "Internal Routing" refers to whatever routing you want
        to provide to allow administrators to access the Management network.</p>
      <p>If you are using HP Helion OpenStack to install the operating system, then an IPMI/iLO
        network connected to the IPMI/iLO ports of all servers and routable from the deployer server
        is also required for BIOS and power management of the nodes during the operating system
        installation process.</p>
      <p>The example uses the following disk configurations:</p>
      <ul>
        <li><b>Controllers</b> - One operating system disk and two disks for Swift storage</li>
        <li><b>VSA</b> - one operating system disk and two disks for VSA storage</li>
        <li><b>Compute</b> - one operating system disk and one disk for virtual machine ephemeral
          storage</li>
      </ul>
      <!--link to bottom section here -->
    </section>
    <section id="2.2_entryscale_esx"><title>2.2 Entry-scale ESX model</title>
      <p>This example shows how to integrate HP Helion OpenStack with ESX. The controller
        configuration is essentially the same as in the KVM example, but the resource nodes are
        provided by vCenter. In addition, a number of controller virtual machines are created for
        each vCenter cluster; one ESX Compute virtual machine (which provides the nova-compute proxy
        for vCenter) and one OVSvApp virtual machine per cluster member (which provides network
        access). These virtual machines are created automatically by HP Helion OpenStack as part of
        activating the vCenter cluster, and are therefore not defined in the example.</p>
      <!-- MEDIA DIAGRAM GOES HERE -->
      <p>The physical networking configuration is also largely the same as the KVM example, with the
        exception of the GUEST network which uses tenant VLANs as the Neutron networking model
        rather than VxLAN.</p>
      <p>A separate configuration network (CONF) is required for configuration access from the
        deployer. This network must be reachable from the Management network.</p>
    </section>
    <section id="2.3_entryscale_swift"><title>2.3 Entry-scale Swift model</title>
      <p>This example shows how HP Helion OpenStack can be configured to provide a Swift only
        configuration, consisting of three controllers and one or more Swift object servers.</p>
      <!-- MEDIA DIAGRAM GOES HERE -->
      <p>The example requires the following networks:</p>
      <ul>
        <li><b>External API</b> - This is the network that users will use to make requests to the
          cloud</li>
        <li><b>Swift</b> - This is the network that will be used for all data traffic between the
          Swift services</li>
        <li><b>Management</b> - This is the network that will be used for all internal traffic
          between the cloud services, including node provisioning. This network must be on an
          untagged VLAN.</li>
      </ul>
      <p>All of these networks are configured to be presented via a pair of Bonded NICs. The example
        also enables provider VLANs to be configured in Neutron on this interface.</p>
      <p>In the diagram "External Routing" refers to whatever routing you want to provide to allow
        users to access the External API. "Internal Routing" refers to whatever routing you want to
        provide to allow administrators to access the Management network.</p>
      <p>If you are using HP Helion OpenStack to install the operating system then an IPMI/iLO
        network connected to the IPMI/iLO ports of all servers and routable from the deployer is
        also required for BIOS and power management of the node during the operating system
        installation process.</p>
      <p>In the example the controllers use one disk for the operating system and two disks for
        Swift Proxy and Account storage. The Swift object servers use one disk for the operating
        system and four disks for swift storage. These values can be modified to suit your
        environment.</p>
    </section>
    <section id="2.4_entryscale_ceph"><title>2.4 Entry-scale KVM with Ceph model</title>
      <p>This example provides a KVM based cloud using Ceph for volume storage. This controller and
        compute configuration are essentially the same as in the KVM example, but the backend Cinder
        storage is provided by Ceph.</p>
      <!-- MEDIA DIAGRAM GOES HERE -->
    </section>
    <section id="2.5_mid-scale_kvm_vsa"><title>2.5 Mid-scale KVM with VSA model (Tech
        Preview)</title>
      <p>The mid-scale model, which is included as a technology preview in HP Helion OpenStack 2.0,
        illustrates two important aspects of configuring HP Helion OpenStack for increased scale.
        The controller services are distributed across a greater number of controllers and a number
        of the networks are configured as multiple L3 segments (implementing per-rack
        networking).</p>
      <!-- MEDIA DIAGRAM GOES HERE -->
      <p>The distribution of services across controllers is only one possible configuration, and
        other combinations can also be expressed.</p>
    </section>
    <section id="3.0_modify_entryscale_kvm"><title><i>3.0</i> Modifying the Entry-scale KVM with VSA
        model for your Environment</title>
      <p>In this section we will look at the changes that need to be made to the input model in
        order to deploy and run this cloud model in your environment.</p>
      <p>This section is written from the perspective of the <codeph>entry-scale-kvm-vsa</codeph>
        example, although the same principles apply to all of the examples.</p>
      <p>There are two categories of modifications that we will look at:</p>
      <ol>
        <li><b>Localizations</b> - These are the minimum set of changes that you need to make to
          adapt the examples to run in your environment. These are mostly concerned with
          networking.</li>
        <li><b>Customizations</b> - These describe more general changes that you may wish to make to
          your model, e.g. changing disk storage layouts.</li>
      </ol>
      <p>Note that as a convention the examples use upper case for the object names, but these
        strings are only used to define the relationships between objects and have no specific
        significance to the configuration processor. You can change the names to values that are
        relevant to your context providing you do so consistently across the input model.</p>
    </section>
    <section id="3.1_localizing_inputmodel"><title><i>3.1</i> Localizing the Input Model</title>
      <p>In this section we look at the minimum set of changes needed to localize the cloud for your
        environment. This assumes you are using other features of the example unchanged.</p>
      <ul>
        <li>Update <codeph>networks.yml</codeph> to specify the network addresses (VLAN IDs and CIDR
          values) for your cloud</li>
        <li>Update <codeph>nic_mappings.yml</codeph> to specify the PCI bus information for your
          servers ethernet devices</li>
        <li>Update <codeph>net_interfaces.yml</codeph> to provide network interface configuration,
          such as bond settings and bond devices</li>
        <li>Update <codeph>network_groups.yml</codeph> to provide the public URL for your cloud and
          to provide security certificates</li>
        <li>Update <codeph>servers.yml</codeph> to provide information on your servers</li>
      </ul>
    </section>
    <section id="3.1.1_networks">
      <title><i>3.1.1</i> Networks.yml</title>
      <p>You will need to allocate site specific CIDRs and VLANs for these networks and update these
        values in the <codeph>networks.yml</codeph> file. The example models define the following
        networks:</p>
      <table frame="all" rowsep="1" colsep="1" id="table_xrn_zzy_st">
        <tgroup cols="4">
          <colspec colname="c1" colnum="1"/>
          <colspec colname="c2" colnum="2"/>
          <colspec colname="c3" colnum="3"/>
          <colspec colname="c4" colnum="4"/>
          <thead>
            <row>
              <entry>Network</entry>
              <entry>CIDR</entry>
              <entry>VLAN ID</entry>
              <entry>Tagged / Untagged</entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry>External API</entry>
              <entry>10.0.1.0/24</entry>
              <entry>101</entry>
              <entry>Tagged</entry>
            </row>
            <row>
              <entry>External VM</entry>
              <entry>Addresses configured by Neutron, leave blank in the file</entry>
              <entry>102</entry>
              <entry>Tagged</entry>
            </row>
            <row>
              <entry>Guest</entry>
              <entry>10.1.1.0/24</entry>
              <entry>103</entry>
              <entry>Tagged</entry>
            </row>
            <row>
              <entry>Management</entry>
              <entry>192.168.10.0/24</entry>
              <entry>100</entry>
              <entry>Un-tagged</entry>
            </row>
          </tbody>
        </tgroup>
      </table>
      <p>You will need to edit this file and provide your local values for these networks.</p>
      <p>The CIDR for the External VM network is configured separately using the Neutron API (see
            <i><xref href="administration/create_extnet.dita">Creating an External
          Network</xref></i>. You will only specify its VLAN ID during the installation process.</p>
      <p>The Management network is shown as untagged. This is required if you are using this network
        to PXE install the operating system on the cloud nodes.</p>
      <p>The example <codeph>networks.yml</codeph> file is shown below, modify the bolded fields to
        reflect your site values.</p>
      <codeblock>networks:
   #
   # This example uses the following networks
   #
   # Network       CIDR             VLAN
   # -------       ----             ----
   # External API  10.0.1.0/24      101 (tagged)
   # External VM   see note 1       102 (tagged)
   # Guest         10.1.1.0/24      103 (tagged)
   # Management    192.168.10.0/24  100 (untagged)
   #	
   # Notes:
   # 1. Defined as part of Neutron configuration
   #
   # Modify these values to match your environment
   #
   - name: EXTERNAL-API-NET
     vlanid: <b>101</b>
     tagged-vlan: true
     cidr: <b>10.0.1.0/24</b>
     gateway-ip: <b>10.0.1.1</b>
     network-group: EXTERNAL-API
        
   - name: EXTERNAL-VM-NET
     vlanid: <b>102</b>
     tagged-vlan: true
     network-group: EXTERNAL-VM
        
   - name: GUEST-NET
     vlanid: <b>103</b>
     tagged-vlan: true
     cidr: <b>10.1.1.0/24</b>
     gateway-ip: <b>10.1.1.1</b>
     network-group: GUEST
        
   - name: MANAGEMENT-NET
     vlanid: 100
     tagged-vlan: false
     cidr: <b>192.168.10.0/24</b>
     gateway-ip: <b>192.168.10.1</b>
     network-group: MANAGEMENT</codeblock>
    </section>
    <section id="3.1.2_nicmappings"><title><i>3.1.2</i> Nic_mappings.yml</title>
      <p>This file maps ethernet port names to specific bus slots. Due to inherent race conditions
        associated with multiple PCI device discovery there is no guarantee that ethernet devices
        will be named as expected by the operating system, and it is possible that different port
        naming will exist on different servers with the same physical configuration.</p>
      <p>In order to provide a deterministic naming pattern, the input model supports an explicit
        mapping from PCI bus address to a user specified name. HP Helion OpenStack uses the prefix
        "hed" (Helion Ethernet Device) to name such devices to avoid any name clashes with the "eth"
        names assigned by the operating system.</p>
      <p>The example <codeph>nic_mappings.yml</codeph> file is shown below.</p>
      <codeblock>nic-mappings:
        
   - name: HP-DL360-4PORT
     physical-ports:
       - logical-name: hed1
         type: simple-port
         bus-address: "0000:07:00.0"
        
        - logical-name: hed2
          type: simple-port
          bus-address: "0000:08:00.0"
        
        - logical-name: hed3
          type: simple-port
          bus-address: "0000:09:00.0"
        
        - logical-name: hed4
          type: simple-port
          bus-address: "0000:0a:00.0"
        
    - name: MY-2PORT-SERVER
      physical-ports:
        - logical-name: hed3
          type: simple-port
          bus-address: "0000:04:00.0"
        
        - logical-name: hed4
          type: simple-port
          bus-address: "0000:04:00.1"</codeblock>
      <p>This defines two sets of NIC mappings, representing two different physical server types.  The name of each mapping is used as a value in the <codeph>servers.yml</codeph> file to associate each server with its required mapping.  This enables the use of different server models or servers with different network hardware.</p>
      <p>Each mapping lists a set of ports with the following information:</p>
      <ul>
        <li><b>Logical name</b> - we use the form <codeph>hedN</codeph></li>
        <li><b>Type</b> - only simple-port types are supported in HP Helion OpenStack 2.0</li>
        <li><b>Bus-address</b> - the PIC bus address of the port</li>
      </ul>
      <p>The PCI bus address can be found using the <codeph>lspci</codeph> command on one of the servers.  This command can produce a lot of output, so you can use the following command which will limit the output to list ethernet class devices only:</p>
      <codeblock>sudo lspci -D |grep -i eth</codeblock>
      <p>Here is an example output:</p>
      <codeblock>$ sudo lspci -D |grep -i eth
0000:02:00.0 Ethernet controller: Broadcom Corporation NetXtreme BCM5719 Gigabit Ethernet PCIe (rev 01)
0000:02:00.1 Ethernet controller: Broadcom Corporation NetXtreme BCM5719 Gigabit Ethernet PCIe (rev 01)
0000:02:00.2 Ethernet controller: Broadcom Corporation NetXtreme BCM5719 Gigabit Ethernet PCIe (rev 01)
0000:02:00.3 Ethernet controller: Broadcom Corporation NetXtreme BCM5719 Gigabit Ethernet PCIe (rev 01)
0000:04:00.0 Ethernet controller: Intel Corporation 82599ES 10-Gigabit SFI/SFP+ Network Connection (rev 01)
0000:04:00.1 Ethernet controller: Intel Corporation 82599ES 10-Gigabit SFI/SFP+ Network Connection (rev 01)</codeblock>
      <p>To localize this file, replace the mapping name(s) with the names of your choice and enumerate the ports as required.</p>
    </section>
  </body>
</topic>
